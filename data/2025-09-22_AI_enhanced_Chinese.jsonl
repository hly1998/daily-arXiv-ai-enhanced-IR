{"id": "2509.15380", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15380", "abs": "https://arxiv.org/abs/2509.15380", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios", "comment": null, "summary": "Despite recent advancements in Multilingual Information Retrieval (MLIR), a\nsignificant gap remains between research and practical deployment. Many studies\nassess MLIR performance in isolated settings, limiting their applicability to\nreal-world scenarios. In this work, we leverage the unique characteristics of\nthe Quranic multilingual corpus to examine the optimal strategies to develop an\nad-hoc IR system for the Islamic domain that is designed to satisfy users'\ninformation needs in multiple languages. We prepared eleven retrieval models\nemploying four training approaches: monolingual, cross-lingual,\ntranslate-train-all, and a novel mixed method combining cross-lingual and\nmonolingual techniques. Evaluation on an in-domain dataset demonstrates that\nthe mixed approach achieves promising results across diverse retrieval\nscenarios. Furthermore, we provide a detailed analysis of how different\ntraining configurations affect the embedding space and their implications for\nmultilingual retrieval effectiveness. Finally, we discuss deployment\nconsiderations, emphasizing the cost-efficiency of deploying a single\nversatile, lightweight model for real-world MLIR applications.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u8de8\u8bed\u4e0e\u5355\u8bed\u6280\u672f\u7684\u591a\u8bed\u79cd\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f0a\u65af\u5170\u4fe1\u606f\u9700\u6c42\uff0c\u8bc4\u4ef7\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\uff0c\u5e76\u5bf9\u4e0d\u540c\u8bad\u7ec3\u914d\u7f6e\u5bf9\u5d4c\u5165\u7a7a\u95f4\u7684\u5f71\u54cd\u8fdb\u884c\u5206\u6790\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u90e8\u7f72\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u8fd1\u5e74\u6765\u591a\u8bed\u79cd\u4fe1\u606f\u68c0\u7d22\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u8bb8\u591a\u7814\u7a76\u5728\u5b64\u7acb\u73af\u5883\u4e2d\u8bc4\u4f30\u591a\u8bed\u79cd\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u53e4\u5170\u7ecf\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u7684\u72ec\u7279\u7279\u6027\uff0c\u5f00\u53d1\u4e00\u4e2a\u4e13\u4e3a\u6ee1\u8db3\u591a\u4e2a\u8bed\u8a00\u7684\u4fe1\u606f\u9700\u6c42\u7684\u4f0a\u65af\u5170\u9886\u57df\u7684\u4e34\u65f6\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u4e2d\u51c6\u5907\u4e86\u5341\u4e00\u4e2a\u68c0\u7d22\u6a21\u578b\uff0c\u91c7\u7528\u56db\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff1a\u5355\u8bed\u3001\u8de8\u8bed\u3001\u7ffb\u8bd1-\u8bad\u7ec3-\u6574\u4f53\u4ee5\u53ca\u65b0\u7684\u6df7\u5408\u65b9\u6cd5\uff08\u7ed3\u5408\u8de8\u8bed\u548c\u5355\u8bed\u6280\u672f\uff09\u3002", "result": "\u8bba\u6587\u5728\u4e00\u4e2a\u9886\u57df\u5185\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\u6df7\u5408\u65b9\u6cd5\u5728\u591a\u79cd\u68c0\u7d22\u60c5\u666f\u4e0b\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be6\u7ec6\u5206\u6790\u4e86\u4e0d\u540c\u8bad\u7ec3\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u8bed\u79cd\u68c0\u7d22\u7684\u6709\u6548\u6027\u3002\u8ba8\u8bba\u4e86\u90e8\u7f72\u8003\u91cf\uff0c\u5f3a\u8c03\u4e86\u5355\u4e00\u591a\u529f\u80fd\u8f7b\u91cf\u5316\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2509.15432", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.15432", "abs": "https://arxiv.org/abs/2509.15432", "authors": ["Thong Nguyen", "Yibin Lei", "Jia-Huei Ju", "Andrew Yates"], "title": "SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models", "comment": "Accepted", "summary": "Visual Document Retrieval (VDR) typically operates as text-to-image retrieval\nusing specialized bi-encoders trained to directly embed document images. We\nrevisit a zero-shot generate-and-encode pipeline: a vision-language model first\nproduces a detailed textual description of each document image, which is then\nembedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method\nreaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual\ndocument encoder. It also scales better to large collections and offers broader\nmultilingual coverage. Analysis shows that modern vision-language models\ncapture complex textual and visual cues with sufficient granularity to act as a\nreusable semantic proxy. By offloading modality alignment to pretrained\nvision-language models, our approach removes the need for computationally\nintensive text-image contrastive training and establishes a strong zero-shot\nbaseline for future VDR systems.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u751f\u6210\u548c\u7f16\u7801\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u6863\u56fe\u50cf\u8fdb\u884c\u6587\u672c\u63cf\u8ff0\uff0c\u7136\u540e\u4f7f\u7528\u6807\u51c6\u6587\u672c\u7f16\u7801\u5668\u8fdb\u884c\u5d4c\u5165\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u591a\u5411\u91cf\u89c6\u89c9\u6587\u6863\u7f16\u7801\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u5584Visual Document Retrieval\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6355\u83b7\u590d\u6742\u7684\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u4ee5\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u751f\u6210\u548c\u7f16\u7801\u7ba1\u9053\uff1a\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u6863\u56fe\u50cf\u7684\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\uff0c\u7136\u540e\u901a\u8fc7\u6807\u51c6\u6587\u672c\u7f16\u7801\u5668\u8fdb\u884c\u5d4c\u5165\u3002", "result": "\u5728ViDoRe-v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e8663.4%\u7684nDCG@5\uff0c\u8d85\u8d8a\u4e86\u6700\u5f3a\u7684\u591a\u5411\u91cf\u89c6\u89c9\u6587\u6863\u7f16\u7801\u5668\uff0c\u5728\u5927\u89c4\u6a21\u6587\u6863\u96c6\u548c\u591a\u8bed\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8fd9\u4e00\u65b9\u6cd5\u901a\u8fc7\u5378\u8f7d\u6a21\u6001\u5bf9\u9f50\u4efb\u52a1\uff0c\u6d88\u9664\u4e86\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u7684\u6587\u672c\u56fe\u50cf\u5bf9\u6bd4\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u7cfb\u7edf\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u3002"}}
{"id": "2509.15439", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15439", "abs": "https://arxiv.org/abs/2509.15439", "authors": ["Ekgari Kasawala", "Surej Mouli"], "title": "Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses", "comment": "15 Pages", "summary": "In brain-computer interface (BCI) systems, steady-state visual evoked\npotentials (SSVEP) and P300 responses have achieved widespread implementation\nowing to their superior information transfer rates (ITR) and minimal training\nrequirements. These neurophysiological signals have exhibited robust efficacy\nand versatility in external device control, demonstrating enhanced precision\nand scalability. However, conventional implementations predominantly utilise\nliquid crystal display (LCD)-based visual stimulation paradigms, which present\nlimitations in practical deployment scenarios. This investigation presents the\ndevelopment and evaluation of a novel light-emitting diode (LED)-based dual\nstimulation apparatus designed to enhance SSVEP classification accuracy through\nthe integration of both SSVEP and P300 paradigms. The system employs four\ndistinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,\nbackward, right, and left directional controls, respectively. Oscilloscopic\nverification confirmed the precision of these stimulation frequencies.\nReal-time feature extraction was accomplished through the concurrent analysis\nof maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to\nascertain user intent. Directional control was determined by the frequency\nexhibiting maximal amplitude characteristics. The visual stimulation hardware\ndemonstrated minimal frequency deviation, with error differentials ranging from\n0.15%to 0.20%across all frequencies. The implemented signal processing\nalgorithm successfully discriminated all four stimulus frequencies whilst\ncorrelating them with their respective P300 event markers. Classification\naccuracy was evaluated based on correct task intention recognition. The\nproposed hybrid system achieved a mean classification accuracy of 86.25%,\ncoupled with an average ITR of 42.08 bits per minute (bpm).", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408SSVEP\u548cP300\u7684\u65b0\u578bLED\u523a\u6fc0\u8bbe\u5907\uff0c\u63d0\u9ad8\u4e86\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\u7684\u5206\u7c7b\u51c6\u786e\u6027\u548c\u4fe1\u606f\u4f20\u8f93\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u6db2\u6676\u663e\u793a\uff08LCD\uff09\u89c6\u89c9\u523a\u6fc0\u6a21\u5f0f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u5f00\u53d1\u4e00\u79cd\u65b0\u578bLED\u89c6\u89c9\u523a\u6fc0\u8bbe\u5907\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528LED\u4e3a\u57fa\u7840\u7684\u53cc\u523a\u6fc0\u8bbe\u5907\uff0c\u901a\u8fc7\u7ed3\u5408SSVEP\u548cP300\u6a21\u5f0f\u6765\u63d0\u9ad8SSVEP\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\u4e3a86.25%\uff0c\u5e73\u5747\u4fe1\u606f\u4f20\u8f93\u7387\u4e3a42.08 bpm\u3002", "conclusion": "\u65b0\u578bLED\u89c6\u89c9\u523a\u6fc0\u8bbe\u5907\u5728\u63d0\u9ad8BCI\u7cfb\u7edf\u7684\u5e94\u7528\u6027\u53ca\u7cbe\u5ea6\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.15588", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15588", "abs": "https://arxiv.org/abs/2509.15588", "authors": ["Yu-Cheng Chang", "Guan-Wei Yeo", "Quah Eugene", "Fan-Jie Shih", "Yuan-Ching Kuo", "Tsung-En Yu", "Hung-Chun Hsu", "Ming-Feng Tsai", "Chuan-Ju Wang"], "title": "CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion", "comment": null, "summary": "The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both\ninteractive and offline submission tasks. The former requires systems to\noperate under real-time constraints, making robustness and efficiency as\nimportant as accuracy, while the latter enables controlled evaluation of\npassage ranking and response generation with pre-defined datasets. To address\nthis, we explored query rewriting and retrieval fusion as core strategies. We\nbuilt our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion\n(RRF) strategies to handle different submission tasks. Results show that\nreranking and fusion improve robustness while revealing trade-offs between\neffectiveness and efficiency across both tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u67e5\u8be2\u91cd\u5199\u548c\u68c0\u7d22\u878d\u5408\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u5e76\u63ed\u793a\u4e86\u6548\u7387\u548c\u6548\u679c\u7684\u6743\u8861\u3002", "motivation": "TREC Interactive Knowledge Assistance Track (iKAT)\u9700\u8981\u7cfb\u7edf\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u8fd0\u884c\uff0c\u5f3a\u8c03\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u4e0e\u51c6\u786e\u6027\u540c\u6837\u91cd\u8981\u3002\u540c\u65f6\uff0c\u8fd8\u9700\u8981\u5728\u9884\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u53d7\u63a7\u8bc4\u4f30\uff0c\u4ee5\u63d0\u9ad8\u6bb5\u843d\u6392\u540d\u548c\u54cd\u5e94\u751f\u6210\u51c6\u786e\u5ea6\u3002", "method": "\u91c7\u7528\u4e86Best-of-$N$\u9009\u62e9\u548c\u4e92\u60e0\u6392\u540d\u878d\u5408\uff08RRF\uff09\u7b56\u7565\u6765\u6784\u5efa\u6d41\u6c34\u7ebf\uff0c\u4ee5\u5904\u7406\u4ea4\u4e92\u5f0f\u548c\u79bb\u7ebf\u63d0\u4ea4\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u67e5\u8be2\u91cd\u5199\u548c\u68c0\u7d22\u878d\u5408\u7b56\u7565\uff0c\u5728\u4e0d\u540c\u63d0\u4ea4\u4efb\u52a1\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u91cd\u6392\u5e8f\u548c\u878d\u5408\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u4efb\u52a1\u4e4b\u95f4\u6548\u679c\u548c\u6548\u7387\u7684\u6743\u8861\u3002", "conclusion": "\u91cd\u6392\u5e8f\u548c\u68c0\u7d22\u878d\u5408\u7b56\u7565\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u63ed\u793a\u4e86\u5728\u6548\u679c\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8fd9\u662f\u5728\u5904\u7406\u4ea4\u4e92\u548c\u79bb\u7ebf\u63d0\u4ea4\u4efb\u52a1\u4e2d\u5fc5\u8981\u7684\u91cd\u8981\u7b56\u7565\u3002"}}
{"id": "2509.15658", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15658", "abs": "https://arxiv.org/abs/2509.15658", "authors": ["Jisu Kim", "Jinhee Park", "Changhyun Jeon", "Jungwoo Choi", "Keonwoo Kim", "Minji Hong", "Sehyun Kim"], "title": "Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach", "comment": null, "summary": "Traditional query expansion techniques for addressing vocabulary mismatch\nproblems in information retrieval are context-sensitive and may lead to\nperformance degradation. As an alternative, document expansion research has\ngained attention, but existing methods such as Doc2Query have limitations\nincluding excessive preprocessing costs, increased index size, and reliability\nissues with generated content. To mitigate these problems and seek more\nstructured and efficient alternatives, this study proposes a method that\ndivides documents into chunk units and generates textual data for each chunk to\nsimultaneously improve retrieval efficiency and accuracy. The proposed \"Chunk\nKnowledge Generation Model\" adopts a T5-based multi-task learning structure\nthat simultaneously generates titles and candidate questions from each document\nchunk while extracting keywords from user queries. This approach maximizes\ncomputational efficiency by generating and extracting three types of semantic\ninformation in parallel through a single encoding and two decoding processes.\nThe generated data is utilized as additional information in the retrieval\nsystem. GPT-based evaluation on 305 query-document pairs showed that retrieval\nusing the proposed model achieved 95.41% accuracy at Top@10, demonstrating\nsuperior performance compared to document chunk-level retrieval. This study\ncontributes by proposing an approach that simultaneously generates titles and\ncandidate questions from document chunks for application in retrieval\npipelines, and provides empirical evidence applicable to large-scale\ninformation retrieval systems by demonstrating improved retrieval accuracy\nthrough qualitative evaluation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eT5\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u6863\u5206\u5757\u5e76\u751f\u6210\u6807\u9898\u548c\u95ee\u9898\u6765\u63d0\u9ad8\u4fe1\u606f\u68c0\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u67e5\u8be2\u6269\u5c55\u6280\u672f\u5b58\u5728\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u5bfb\u627e\u66f4\u7ed3\u6784\u5316\u548c\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u201cChunk Knowledge Generation Model\u201d\uff0c\u91c7\u7528T5\u4e3a\u57fa\u7840\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u7ed3\u6784\uff0c\u5bf9\u6587\u6863\u8fdb\u884c\u5206\u5757\u5904\u7406\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5757\u751f\u6210\u6587\u672c\u6570\u636e\u3002\u540c\u65f6\u751f\u6210\u6807\u9898\u548c\u5019\u9009\u95ee\u9898\uff0c\u5e76\u4ece\u7528\u6237\u67e5\u8be2\u4e2d\u63d0\u53d6\u5173\u952e\u8bcd\u3002\u8fd9\u6837\u901a\u8fc7\u5355\u4e00\u7f16\u7801\u548c\u4e24\u4e2a\u89e3\u7801\u8fc7\u7a0b\u5e76\u884c\u751f\u6210\u548c\u63d0\u53d6\u4e09\u79cd\u8bed\u4e49\u4fe1\u606f\uff0c\u6700\u5927\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u901a\u8fc7\u5bf9305\u4e2a\u67e5\u8be2\u6587\u6863\u5bf9\u7684GPT\u6a21\u578b\u8bc4\u4f30\uff0c\u4f7f\u7528\u6240\u63d0\u51fa\u6a21\u578b\u7684\u68c0\u7d22\u5728Top@10\u65f6\u8fbe\u5230\u4e8695.41%\u7684\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u4f18\u4e8e\u6587\u6863\u5757\u7ea7\u522b\u68c0\u7d22\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u6807\u9898\u548c\u5019\u9009\u95ee\u9898\u6765\u6539\u5584\u68c0\u7d22\u7ba1\u9053\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u7684\u7ecf\u9a8c\u8bc1\u636e\uff0c\u663e\u793a\u51fa\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u3002"}}
{"id": "2509.15709", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.15709", "abs": "https://arxiv.org/abs/2509.15709", "authors": ["Zhuangzhuang He", "Zhou Kaiyu", "Haoyue Bai", "Fengbin Zhu", "Yonghui Yang"], "title": "Understanding Embedding Scaling in Collaborative Filtering", "comment": null, "summary": "Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomenon: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.", "AI": {"tldr": "\u7814\u7a76\u63a8\u8350\u6a21\u578b\u7684\u6269\u5c55\uff0c\u5c24\u5176\u662f\u5d4c\u5165\u7ef4\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u53cc\u5cf0\u548c\u5bf9\u6570\u73b0\u8c61\uff0c\u5e76\u5206\u6790\u5176\u6f5c\u5728\u539f\u56e0\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u63a8\u8350\u6a21\u578b\u7684\u6269\u5c55\u53ca\u5d4c\u5165\u7ef4\u5ea6\u5982\u4f55\u5f71\u54cd\u6027\u80fd\uff0c\u89e3\u51b3\u5d4c\u5165\u5c3a\u5bf8\u65e0\u6cd5\u6269\u5c55\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u603b\u7ed310\u4e2a\u4e0d\u540c\u7a00\u758f\u7a0b\u5ea6\u548c\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u75284\u4e2a\u7ecf\u5178\u67b6\u6784\u3002", "result": "\u89c2\u6d4b\u5230\u53cc\u5cf0\u548c\u5bf9\u6570\u73b0\u8c61\uff0c\u5e76\u7406\u89e3\u53cc\u5cf0\u73b0\u8c61\u7684\u6839\u672c\u539f\u56e0\uff0c\u7406\u8bba\u4e0a\u5206\u6790\u6a21\u578b\u7684\u566a\u58f0\u9c81\u68d2\u6027\u4e0e\u5b9e\u9a8c\u76f8\u7b26\u3002", "conclusion": "\u53d1\u73b0\u53cc\u5cf0\u548c\u5bf9\u6570\u73b0\u8c61\uff0c\u89e3\u6790\u5176\u539f\u56e0\u5e76\u8fdb\u884c\u566a\u58f0\u9c81\u68d2\u6027\u5206\u6790\u3002"}}
{"id": "2509.15858", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15858", "abs": "https://arxiv.org/abs/2509.15858", "authors": ["Aysenur Kulunk", "Berk Taskin", "M. Furkan Eseoglu", "H. Bahadir Sahin"], "title": "Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings", "comment": null, "summary": "In large scale e-commerce marketplaces, duplicate product listings frequently\ncause consumer confusion and operational inefficiencies, degrading trust on the\nplatform and increasing costs. Traditional keyword-based search methodologies\nfalter in accurately identifying duplicates due to their reliance on exact\ntextual matches, neglecting semantic similarities inherent in product titles.\nTo address these challenges, we introduce a scalable, multimodal product\ndeduplication designed specifically for the e-commerce domain. Our approach\nemploys a domain-specific text model grounded in BERT architecture in\nconjunction with MaskedAutoEncoders for image representations. Both of these\narchitectures are augmented with dimensionality reduction techniques to produce\ncompact 128-dimensional embeddings without significant information loss.\nComplementing this, we also developed a novel decider model that leverages both\ntext and image vectors. By integrating these feature extraction mechanisms with\nMilvus, an optimized vector database, our system can facilitate efficient and\nhigh-precision similarity searches across extensive product catalogs exceeding\n200 million items with just 100GB of system RAM consumption. Empirical\nevaluations demonstrate that our matching system achieves a macro-average F1\nscore of 0.90, outperforming third-party solutions which attain an F1 score of\n0.83. Our findings show the potential of combining domain-specific adaptations\nwith state-of-the-art machine learning techniques to mitigate duplicate\nlistings in large-scale e-commerce environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e13\u4e3a\u7535\u5b50\u5546\u52a1\u8bbe\u8ba1\u7684\u4ea7\u54c1\u53bb\u91cd\u7cfb\u7edf\uff0c\u5229\u7528BERT\u6784\u67b6\u7684\u6587\u672c\u6a21\u578b\u548cAutoEncoders\u7528\u4e8e\u56fe\u50cf\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u76f8\u4f3c\u6027\u641c\u7d22\uff0cF1\u5f97\u5206\u8fbe\u52300.90\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u5546\u52a1\u5e02\u573a\u4e2d\u56e0\u91cd\u590d\u4ea7\u54c1\u5217\u8868\u5bf9\u6d88\u8d39\u8005\u9020\u6210\u7684\u6df7\u6dc6\u548c\u8fd0\u8425\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528BERT\u67b6\u6784\u7684\u6587\u672c\u6a21\u578b\u4e0eMaskedAutoEncoders\u751f\u6210128\u7ef4\u7d27\u51d1\u5d4c\u5165\uff0c\u7ed3\u5408Milvus\u6570\u636e\u5e93\u8fdb\u884c\u9ad8\u6548\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u5f00\u53d1\u5229\u7528\u6587\u672c\u548c\u56fe\u50cf\u5411\u91cf\u7684\u51b3\u7b56\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u5728200\u4ebf\u4ea7\u54c1\u5217\u8868\u4e0b\u6d88\u8017\u4ec5100GB\u5185\u5b58\uff0c\u5b8f\u5e73\u5747F1\u5f97\u5206\u4e3a0.90\uff0c\u4f18\u4e8e\u7b2c\u4e09\u65b9\u89e3\u51b3\u65b9\u6848\u76840.83\u3002", "conclusion": "\u7ed3\u5408\u57df\u7279\u5f02\u6027\u8c03\u6574\u548c\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u5927\u89c4\u6a21\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u7684\u91cd\u590d\u4ea7\u54c1\u5217\u8868\u3002"}}
