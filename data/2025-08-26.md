<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 18]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Bootstrapping Conditional Retrieval for User-to-Item Recommendations](https://arxiv.org/abs/2508.16793)
*Hongtao Lin,Haoyu Chen,Jaewon Jang,Jiajing Xu*

Main category: cs.IR

TL;DR: 提出了一种结合条件信息的检索模型，增强了用户与条件的特征互动，提升了检索效果，并在Pinterest上实现了活跃用户的增长。


<details>
  <summary>Details</summary>
Motivation: 提高在不同条件下检索到的项目的相关性。

Method: 与标准双塔模型使用相同的训练数据，但在查询中加入条件信息（如话题）。

Result: 该方法能够检索到高度相关的项目，并在Pinterest上线，贡献了+0.26%的活跃用户增长。

Conclusion: 我们提出了一种结合条件信息的检索方法，相较于标准双塔模型，在用户参与度指标上表现更佳。

Abstract: User-to-item retrieval has been an active research area in recommendation
system, and two tower models are widely adopted due to model simplicity and
serving efficiency. In this work, we focus on a variant called
\textit{conditional retrieval}, where we expect retrieved items to be relevant
to a condition (e.g. topic). We propose a method that uses the same training
data as standard two tower models but incorporates item-side information as
conditions in query. This allows us to bootstrap new conditional retrieval use
cases and encourages feature interactions between user and condition.
Experiments show that our method can retrieve highly relevant items and
outperforms standard two tower models with filters on engagement metrics. The
proposed model is deployed to power a topic-based notification feed at
Pinterest and led to +0.26\% weekly active users.

</details>


### [2] [Towards a Real-World Aligned Benchmark for Unlearning in Recommender Systems](https://arxiv.org/abs/2508.17076)
*Pierre Lubitzsch,Olga Ovcharenko,Hao Chen,Maarten de Rijke,Sebastian Schelter*

Main category: cs.IR

TL;DR: 提出了新的推荐系统卸载基准，强调多任务和时序性验证，定制算法性能卓越。


<details>
  <summary>Details</summary>
Motivation: 现有的推荐系统卸载基准不够现实，无法满足真实操作需求，例如CURE4Rec的局限性。

Method: 提出并设计了多任务的卸载基准，采用定制的卸载算法进行实验验证。

Result: 定制卸载算法在顺序推荐模型中的性能优于通用算法，并能在几秒内完成操作。

Conclusion: 提出了具有现实世界操作需求的推荐系统卸载的基准，并通过初步实验验证了定制算法的优越性。

Abstract: Modern recommender systems heavily leverage user interaction data to deliver
personalized experiences. However, relying on personal data presents challenges
in adhering to privacy regulations, such as the GDPR's "right to be forgotten".
Machine unlearning (MU) aims to address these challenges by enabling the
efficient removal of specific training data from models post-training, without
compromising model utility or leaving residual information. However, current
benchmarks for unlearning in recommender systems -- most notably CURE4Rec --
fail to reflect real-world operational demands. They focus narrowly on
collaborative filtering, overlook tasks like session-based and next-basket
recommendation, simulate unrealistically large unlearning requests, and ignore
critical efficiency constraints. In this paper, we propose a set of design
desiderata and research questions to guide the development of a more realistic
benchmark for unlearning in recommender systems, with the goal of gathering
feedback from the research community. Our benchmark proposal spans multiple
recommendation tasks, includes domain-specific unlearning scenarios, and
several unlearning algorithms -- including ones adapted from a recent NeurIPS
unlearning competition. Furthermore, we argue for an unlearning setup that
reflects the sequential, time-sensitive nature of real-world deletion requests.
We also present a preliminary experiment in a next-basket recommendation
setting based on our proposed desiderata and find that unlearning also works
for sequential recommendation models, exposed to many small unlearning
requests. In this case, we observe that a modification of a custom-designed
unlearning algorithm for recommender systems outperforms general unlearning
algorithms significantly, and that unlearning can be executed with a latency of
only several seconds.

</details>


### [3] [Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation](https://arxiv.org/abs/2508.17079)
*Yejin Choi,Jaewoo Park,Janghan Yoon,Saejin Kim,Jaehyun Jeon,Youngjae Yu*

Main category: cs.IR

TL;DR: 提出了一个名为PREMIR的新框架，通过生成跨模态预问题提升多模态检索性能，对未见领域和语言具有强适应性。


<details>
  <summary>Details</summary>
Motivation: 当前的检索方法在面对未见领域或语言时表现不佳，而大多数文档为私人所有或被企业内部限制，这导致了检索难度增加。

Method: PREMIR利用多模态大语言模型的广泛知识在检索前生成跨模态预问题（preQs），通过多种补充性模态扩展匹配范围到token级别。

Result: PREMIR在分布外基准测试中表现出色，包括在封闭域和多语言环境下，超越了所有检索指标上的强基线。

Conclusion: 本文介绍了PREMIR框架，这是一种通过在检索前生成跨模态问题来提升检索精度的方法。

Abstract: Rapid advances in Multimodal Large Language Models (MLLMs) have expanded
information retrieval beyond purely textual inputs, enabling retrieval from
complex real world documents that combine text and visuals. However, most
documents are private either owned by individuals or confined within corporate
silos and current retrievers struggle when faced with unseen domains or
languages. To address this gap, we introduce PREMIR, a simple yet effective
framework that leverages the broad knowledge of an MLLM to generate cross modal
pre questions (preQs) before retrieval. Unlike earlier multimodal retrievers
that compare embeddings in a single vector space, PREMIR leverages preQs from
multiple complementary modalities to expand the scope of matching to the token
level. Experiments show that PREMIR achieves state of the art performance on
out of distribution benchmarks, including closed domain and multilingual
settings, outperforming strong baselines across all retrieval metrics. We
confirm the contribution of each component through in depth ablation studies,
and qualitative analyses of the generated preQs further highlight the model's
robustness in real world settings.

</details>


### [4] [VQL: An End-to-End Context-Aware Vector Quantization Attention for Ultra-Long User Behavior Modeling](https://arxiv.org/abs/2508.17125)
*Kaiyuan Li,Yongxiang Tang,Yanhua Cheng,Yong Bai,Yanxiang Zeng,Chao Wang,Xialong Liu,Peng Jiang*

Main category: cs.IR

TL;DR: VQL框架通过强调上下文感知和低损耗压缩，在超长行为推荐中取得显著效果，同时减少推理延迟，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在大规模推荐系统中，用户的超长行为序列包含丰富的兴趣变化信号。然而，由于延迟和内存限制，直接建模这些序列在生产中是不可行的。现有的解决方案如top-k检索和编码器压缩均未能在低损失压缩、上下文感知和效率之间实现良好的平衡。

Method: 提出了一种上下文感知的向量量化注意力框架（VQL）用于超长行为建模，包括三项创新：(1)仅对注意力键进行量化，而保持值不变，证明了softmax归一化在误差约束在与序列长度无关的一点可直接监督量化质量；(2) 多级量化：注意力头被分组，每组使用自己的小型代码簿，减少了量化误差，同时保持了缓存大小不变；(3)高效的上下文注入：直接整合静态特征，并通过可分离的时间核来建模相对位置，无需扩大代码簿。

Result: 在KuaiRand-1K、KuaiRec和TMALL这三个大规模数据集上的实验表明，VQL始终优于强基线模型，不仅提高了准确性，还降低了推理延迟，确立了超长序列推荐中平衡准确性和效率的新标准。

Conclusion: VQL框架通过一系列创新实现了对超长用户行为序列的有效建模，在准确性和效率上均优于现有方法，为大规模推荐系统提供了更好的解决方案。

Abstract: In large-scale recommender systems, ultra-long user behavior sequences encode
rich signals of evolving interests. Extending sequence length generally
improves accuracy, but directly modeling such sequences in production is
infeasible due to latency and memory constraints. Existing solutions fall into
two categories: (1) top-k retrieval, which truncates the sequence and may
discard most attention mass when L >> k; and (2) encoder-based compression,
which preserves coverage but often over-compresses and fails to incorporate key
context such as temporal gaps or target-aware signals. Neither class achieves a
good balance of low-loss compression, context awareness, and efficiency.
  We propose VQL, a context-aware Vector Quantization Attention framework for
ultra-long behavior modeling, with three innovations. (1) Key-only
quantization: only attention keys are quantized, while values remain intact; we
prove that softmax normalization yields an error bound independent of sequence
length, and a codebook loss directly supervises quantization quality. This also
enables L-free inference via offline caches. (2) Multi-scale quantization:
attention heads are partitioned into groups, each with its own small codebook,
which reduces quantization error while keeping cache size fixed. (3) Efficient
context injection: static features (e.g., item category, modality) are directly
integrated, and relative position is modeled via a separable temporal kernel.
All context is injected without enlarging the codebook, so cached
representations remain query-independent.
  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show
that VQL consistently outperforms strong baselines, achieving higher accuracy
while reducing inference latency, establishing a new state of the art in
balancing accuracy and efficiency for ultra-long sequence recommendation.

</details>


### [5] [Opening the Black Box: Interpretable Remedies for Popularity Bias in Recommender Systems](https://arxiv.org/abs/2508.17297)
*Parviz Ahmadov,Masoud Mansoury*

Main category: cs.IR

TL;DR: 该论文提出了一种基于稀疏自编码器的后处理方法，通过调整神经元激活来缓解推荐系统中的流行度偏差，实验表明该方法在提高公平性方面表现良好，并提供了可解释性。


<details>
  <summary>Details</summary>
Motivation: 流行度偏差问题使得少数热门项目占据了过度的关注，而大多数不那么受欢迎的项目被忽视，导致推荐质量下降和项目曝光不公平。现有的方法在一定程度上解决了这种偏差，但通常缺乏透明度。

Method: 使用稀疏自编码器（SAE）进行训练，以复制预训练模型的行为并提供神经元级别的可解释性，通过引入具有明确偏好的合成用户识别流行度信号，并调整神经元的激活状态。

Result: 实验表明，所提出的方法在两个公共数据集上的顺序推荐模型中显著提高了公平性，同时对准确率的影响很小。

Conclusion: 该论文提出了一种基于稀疏自编码器（SAE）的后处理方法，用来解释和减轻深度推荐模型中的流行度偏差问题。通过调整最偏向的神经元的激活状态，该方法能够在保证高准确率的同时显著提高推荐的公平性，并提供可解释性以及对公平性和准确率之间权衡的细粒度控制。

Abstract: Popularity bias is a well-known challenge in recommender systems, where a
small number of popular items receive disproportionate attention, while the
majority of less popular items are largely overlooked. This imbalance often
results in reduced recommendation quality and unfair exposure of items.
Although existing mitigation techniques address this bias to some extent, they
typically lack transparency in how they operate. In this paper, we propose a
post-hoc method using a Sparse Autoencoder (SAE) to interpret and mitigate
popularity bias in deep recommendation models. The SAE is trained to replicate
a pre-trained model's behavior while enabling neuron-level interpretability. By
introducing synthetic users with clear preferences for either popular or
unpopular items, we identify neurons encoding popularity signals based on their
activation patterns. We then adjust the activations of the most biased neurons
to steer recommendations toward fairer exposure. Experiments on two public
datasets using a sequential recommendation model show that our method
significantly improves fairness with minimal impact on accuracy. Moreover, it
offers interpretability and fine-grained control over the fairness-accuracy
trade-off.

</details>


### [6] [A Universal Framework for Offline Serendipity Evaluation in Recommender Systems via Large Language Models](https://arxiv.org/abs/2508.17571)
*Yu Tokutake,Kazushi Okamoto,Kei Harada,Atsushi Shibata,Koki Karube*

Main category: cs.IR

TL;DR: 提出了一种使用大型语言模型的新评估框架来评估推荐系统中的偶然性表现，发现通常的推荐系统有时优于偶然性导向系统。


<details>
  <summary>Details</summary>
Motivation: 现有的离线指标依赖于模糊的定义或特定数据集和推荐系统，限制了其普适性，需要一个具有广泛适用性的新评估框架。

Method: 使用大型语言模型(LLMs)评估偶然性表现，并通过四种不同的提示策略评估LLMs的偶然性预测准确性，发现链式思维提示策略达到最高准确性。然后在三个常用的真实世界数据集上重新评估偶然性表现。

Result: 使用新评估框架发现链式思维提示策略对偶然性预测的准确性最高。通过评估框架重评偶然性表现，结果表明一般的推荐系统有时比偶然性导向的推荐系统表现更好。

Conclusion: 通过评估框架的结果，研究发现没有一种专注于偶然性的推荐系统能够在所有数据集上始终表现出色，甚至有时一般的推荐系统比偶然性导向的推荐系统表现更好。

Abstract: Serendipity in recommender systems (RSs) has attracted increasing attention
as a concept that enhances user satisfaction by presenting unexpected and
useful items. However, evaluating serendipitous performance remains challenging
because its ground truth is generally unobservable. The existing offline
metrics often depend on ambiguous definitions or are tailored to specific
datasets and RSs, thereby limiting their generalizability. To address this
issue, we propose a universally applicable evaluation framework that leverages
large language models (LLMs) known for their extensive knowledge and reasoning
capabilities, as evaluators. First, to improve the evaluation performance of
the proposed framework, we assessed the serendipity prediction accuracy of LLMs
using four different prompt strategies on a dataset containing user-annotated
serendipitous ground truth and found that the chain-of-thought prompt achieved
the highest accuracy. Next, we re-evaluated the serendipitous performance of
both serendipity-oriented and general RSs using the proposed framework on three
commonly used real-world datasets, without the ground truth. The results
indicated that there was no serendipity-oriented RS that consistently
outperformed across all datasets, and even a general RS sometimes achieved
higher performance than the serendipity-oriented RS.

</details>


### [7] [Preference Trajectory Modeling via Flow Matching for Sequential Recommendation](https://arxiv.org/abs/2508.17618)
*Li Li,Mingyue Cheng,Yuyang Ye,Zhiding Liu,Enhong Chen*

Main category: cs.IR

TL;DR: 提出FlowRec，通过流匹配技术改进推荐效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在顺序推荐中面临对条件高度敏感和推理过程计算昂贵的问题。

Method: 我们提出FlowRec框架，借助流匹配技术，显式建模用户偏好的轨迹，并设计单步对齐损失，提高采样效率和生成质量。

Result: 在四个基准数据集上的大量实验验证了FlowRec相较于目前最先进的基线的优越性。

Conclusion: FlowRec框架显著提升了顺序推荐系统的性能，尤其在用户偏好预测和采样效率上表现出色。

Abstract: Sequential recommendation predicts each user's next item based on their
historical interaction sequence. Recently, diffusion models have attracted
significant attention in this area due to their strong ability to model user
interest distributions. They typically generate target items by denoising
Gaussian noise conditioned on historical interactions. However, these models
face two critical limitations. First, they exhibit high sensitivity to the
condition, making it difficult to recover target items from pure Gaussian
noise. Second, the inference process is computationally expensive, limiting
practical deployment. To address these issues, we propose FlowRec, a simple yet
effective sequential recommendation framework which leverages flow matching to
explicitly model user preference trajectories from current states to future
interests. Flow matching is an emerging generative paradigm, which offers
greater flexibility in initial distributions and enables more efficient
sampling. Based on this, we construct a personalized behavior-based prior
distribution to replace Gaussian noise and learn a vector field to model user
preference trajectories. To better align flow matching with the recommendation
objective, we further design a single-step alignment loss incorporating both
positive and negative samples, improving sampling efficiency and generation
quality. Extensive experiments on four benchmark datasets verify the
superiority of FlowRec over the state-of-the-art baselines.

</details>


### [8] [Demographically-Inspired Query Variants Using an LLM](https://arxiv.org/abs/2508.17644)
*Marwah Alaofi,Nicola Ferro,Paul Thomas,Falk Scholer,Mark Sanderson*

Main category: cs.IR

TL;DR: 利用大语言模型生成查询变体以反映用户多样性并影响信息检索系统评估。


<details>
  <summary>Details</summary>
Motivation: 旨在使现有测试集中的查询多样化，以反映搜索引擎用户的多样性，与之前对“理想”测试集的愿景保持一致。

Method: 使用大语言模型(LLM)生成查询变体，这些查询变体代表不同特性的用户配置文件。

Result: 证明了查询变体对信息检索系统评估的有用性，并且观察到了用户配置文件对系统排名的影响，以及系统性能在不同用户间的变化。

Conclusion: 研究结果表明，利用大语言模型生成的查询变体能够影响系统的排名，并显示用户配置文件在系统有效性方面存在显著差异。

Abstract: This study proposes a method to diversify queries in existing test
collections to reflect some of the diversity of search engine users, aligning
with an earlier vision of an 'ideal' test collection. A Large Language Model
(LLM) is used to create query variants: alternative queries that have the same
meaning as the original. These variants represent user profiles characterised
by different properties, such as language and domain proficiency, which are
known in the IR literature to influence query formulation.
  The LLM's ability to generate query variants that align with user profiles is
empirically validated, and the variants' utility is further explored for IR
system evaluation. Results demonstrate that the variants impact how systems are
ranked and show that user profiles experience significantly different levels of
system effectiveness. This method enables an alternative perspective on system
evaluation where we can observe both the impact of user profiles on system
rankings and how system performance varies across users.

</details>


### [9] [Semantic Search for Information Retrieval](https://arxiv.org/abs/2508.17694)
*Kayla Farivar*

Main category: cs.IR

TL;DR: 本文调查了信息检索系统从传统的词汇技术到现代语义检索器的发展，包括Dense Bi-Encoder、ColBERT、SPLADE和MonoT5等模型。


<details>
  <summary>Details</summary>
Motivation: 探索信息检索系统的发展，特别是从词汇技术到语义检索器的演变，以提高检索效率和效果。

Method: 通过介绍BM25、BERT等基线模型，再到现代语义检索器如Dense Bi-Encoder (DPR)、ColBERT、SPLADE和MonoT5的架构进行比较和分析。

Result: 本文详细介绍了几种现代语义检索器的架构和性能，并对信息检索系统的评估策略和未来研究方向进行了探讨。

Conclusion: 本文总结了当前语义检索器的发展和面临的挑战，并提出了未来研究方向的建议。

Abstract: Information retrieval systems have progressed notably from lexical techniques
such as BM25 and TF-IDF to modern semantic retrievers. This survey provides a
brief overview of the BM25 baseline, then discusses the architecture of modern
state-of-the-art semantic retrievers. Advancing from BERT, we introduce dense
bi-encoders (DPR), late-interaction models (ColBERT), and neural sparse
retrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model. We
conclude with common evaluation tactics, pressing challenges, and propositions
for future directions.

</details>


### [10] [How Do LLM-Generated Texts Impact Term-Based Retrieval Models?](https://arxiv.org/abs/2508.17715)
*Wei Huang,Keping Bi,Yinqiong Cai,Wei Chen,Jiafeng Guo,Xueqi Cheng*

Main category: cs.IR

TL;DR: 研究发现术语检索模型更倾向于文本与查询术语分布相一致的文档，而非呈现来源偏向。


<details>
  <summary>Details</summary>
Motivation: 随着生成式语言模型生成的内容充斥互联网上，信息检索系统需处理人类创作和机器生成文本的混合内容。研究目的在于探讨此类混合内容对传统术语检索模型（例如BM25）的影响。

Method: 通过语言分析探讨LLM生成内容对术语检索模型的影响，研究术语分布特征与检索模型的表现之间的关系。

Result: 发现LLM生成的文本具有更平滑的高频和更陡峭的低频Zipf斜率、较高的术语特异性及更大的文档层级多样性。

Conclusion: 该研究表明，术语检索模型优先考虑与查询术语分布相对应的文档，而非显示固有的来源偏向。

Abstract: As more content generated by large language models (LLMs) floods into the
Internet, information retrieval (IR) systems now face the challenge of
distinguishing and handling a blend of human-authored and machine-generated
texts. Recent studies suggest that neural retrievers may exhibit a preferential
inclination toward LLM-generated content, while classic term-based retrievers
like BM25 tend to favor human-written documents. This paper investigates the
influence of LLM-generated content on term-based retrieval models, which are
valued for their efficiency and robust generalization across domains. Our
linguistic analysis reveals that LLM-generated texts exhibit smoother
high-frequency and steeper low-frequency Zipf slopes, higher term specificity,
and greater document-level diversity. These traits are aligned with LLMs being
trained to optimize reader experience through diverse and precise expressions.
Our study further explores whether term-based retrieval models demonstrate
source bias, concluding that these models prioritize documents whose term
distributions closely correspond to those of the queries, rather than
displaying an inherent source bias. This work provides a foundation for
understanding and addressing potential biases in term-based IR systems managing
mixed-source content.

</details>


### [11] [DiffusionGS: Generative Search with Query Conditioned Diffusion in Kuaishou](https://arxiv.org/abs/2508.17754)
*Qinyao Li,Xiaoyang Zheng,Qihang Zhao,Ke Xu,Zhongbo Sun,Chao Wang,Chenyi Lei,Han Li,Wenwu Ou*

Main category: cs.IR

TL;DR: 提出了DiffusionGS，通过生成模型和扩散机制提高个性化搜索的效率，能够更好地处理用户实时意图与历史行为的结合。离线和在线测试显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以结合用户实时意图与历史行为，这限制了个性化搜索的精度。因此，提出一种新颖的方法来提高用户兴趣捕捉效率。

Method: 该方法主要通过引入生成模型中的扩散机制进行条件去噪处理，利用用户查询作为意图先验，引导注意力分布的优化。

Result: 实验结果表明，DiffusionGS在离线和在线测试中均优于当前先进的方法。

Conclusion: DiffusionGS在个人化搜索排名系统中比现有方法具有优势，能够更好地捕捉用户兴趣的动态变化。

Abstract: Personalized search ranking systems are critical for driving engagement and
revenue in modern e-commerce and short-video platforms. While existing methods
excel at estimating users' broad interests based on the filtered historical
behaviors, they typically under-exploit explicit alignment between a user's
real-time intent (represented by the user query) and their past actions. In
this paper, we propose DiffusionGS, a novel and scalable approach powered by
generative models. Our key insight is that user queries can serve as explicit
intent anchors to facilitate the extraction of users' immediate interests from
long-term, noisy historical behaviors. Specifically, we formulate interest
extraction as a conditional denoising task, where the user's query guides a
conditional diffusion process to produce a robust, user intent-aware
representation from their behavioral sequence. We propose the User-aware
Denoising Layer (UDL) to incorporate user-specific profiles into the
optimization of attention distribution on the user's past actions. By reframing
queries as intent priors and leveraging diffusion-based denoising, our method
provides a powerful mechanism for capturing dynamic user interest shifts.
Extensive offline and online experiments demonstrate the superiority of
DiffusionGS over state-of-the-art methods.

</details>


### [12] [Research on Evaluation Methods for Patent Novelty Search Systems and Empirical Analysis](https://arxiv.org/abs/2508.17782)
*Shu Zhang,LiSha Zhang,Kai Duan,XinKai Sun*

Main category: cs.IR

TL;DR: 提出了一种综合评价方法，利用高质量数据集和多维分析评估专利检索系统性能，并为改进提供证据。


<details>
  <summary>Details</summary>
Motivation: 专利新颖性检索系统对于知识产权保护和创新评估至关重要，其检索准确性直接影响专利质量。

Method: 我们提出了一种综合评价方法，该方法通过从审查员引文和技术一致的家族专利中提取的X型引文构建高质量、可重复的数据集，并使用发明描述作为输入来评估系统。

Result: 实验表明，该方法有效地揭示了不同场景下的性能差异，并为系统改进提供了可操作的证据。

Conclusion: 该框架具有可扩展性和实用性，为专利新颖性检索系统的开发和优化提供了有用的参考。

Abstract: Patent novelty search systems are critical to IP protection and innovation
assessment; their retrieval accuracy directly impacts patent quality. We
propose a comprehensive evaluation methodology that builds high-quality,
reproducible datasets from examiner citations and X-type citations extracted
from technically consistent family patents, and evaluates systems using
invention descriptions as inputs. Using Top-k Detection Rate and Recall as core
metrics, we further conduct multi-dimensional analyses by language, technical
field (IPC), and filing jurisdiction. Experiments show the method effectively
exposes performance differences across scenarios and offers actionable evidence
for system improvement. The framework is scalable and practical, providing a
useful reference for development and optimization of patent novelty search
systems

</details>


### [13] [LexSemBridge: Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation](https://arxiv.org/abs/2508.17858)
*Shaoxiong Zhan,Hai Lin,Hongming Tan,Xiaodong Cai,Hai-Tao Zheng,Xin Su,Zifei Shan,Ruitong Liu,Hong-Gee Kim*

Main category: cs.IR

TL;DR: 论文提出了LexSemBridge框架，通过增强密集查询表达改善精细化检索任务的表现，不改变原有编码器，并验证了其在多种检索任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的检索增强生成（RAG）流水线中的查询日益复杂，现有的密集检索模型在语义匹配上有较好的表现，但在需要精准关键词对齐和区间定位的精细化检索任务中表现欠佳。

Method: 本文提出了LexSemBridge框架，通过精细化、输入感知的向量调制来增强密集查询表达。该框架构建了基于输入标记词的潜在增强向量，并通过元素级互动整合到密集嵌入中。

Result: LexSemBridge在不修改基础编码器的情况下工作，自然扩展到文本和视觉模态。大量实验表明，该方法在语义和精细化检索任务中的有效性和通用性。

Conclusion: LexSemBridge成功增强了密集检索模型在精细化检索任务中的表现，并展示了该框架的普适性和有效性。

Abstract: As queries in retrieval-augmented generation (RAG) pipelines powered by large
language models (LLMs) become increasingly complex and diverse, dense retrieval
models have demonstrated strong performance in semantic matching. Nevertheless,
they often struggle with fine-grained retrieval tasks, where precise keyword
alignment and span-level localization are required, even in cases with high
lexical overlap that would intuitively suggest easier retrieval. To
systematically evaluate this limitation, we introduce two targeted tasks,
keyword retrieval and part-of-passage retrieval, designed to simulate practical
fine-grained scenarios. Motivated by these observations, we propose
LexSemBridge, a unified framework that enhances dense query representations
through fine-grained, input-aware vector modulation. LexSemBridge constructs
latent enhancement vectors from input tokens using three paradigms: Statistical
(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense
embeddings via element-wise interaction. Theoretically, we show that this
modulation preserves the semantic direction while selectively amplifying
discriminative dimensions. LexSemBridge operates as a plug-in without modifying
the backbone encoder and naturally extends to both text and vision modalities.
Extensive experiments across semantic and fine-grained retrieval tasks validate
the effectiveness and generality of our approach. All code and models are
publicly available at https://github.com/Jasaxion/LexSemBridge/

</details>


### [14] [Retrieval Feedback Memory Enhancement Large Model Retrieval Generation Method](https://arxiv.org/abs/2508.17862)
*Leqian Li,Dianxi Shi,Jialu Zhou,Xinyu Wei,Mingyue Yang,Songchang Jin,Shaowu Yang*

Main category: cs.IR

TL;DR: RFM-RAG改进了RAG方法，通过动态证据池进行连续知识管理，提高了多轮查询中的准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在任务中表现出色，但存在参数知识受限和重训练成本高的问题，RAG虽然增加了外部知识检索，但在多轮查询中容易发生信息丢失和冗余检索。

Method: 提出了RFM-RAG方法，通过构建动态证据池实现有状态的连续知识管理，生成描述模型知识缺口的精细查询，检索关键外部知识以迭代更新证据池，并使用R-Feedback Model评估证据完整性。

Result: RFM-RAG在三个公共QA基准测试中表现优于现有方法，提升了整体系统准确性。

Conclusion: RFM-RAG方法能够有效解决RAG方法在多轮查询中信息丢失和冗余检索的问题，并且在公共QA基准测试中表现优异。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
diverse tasks, yet they face inherent limitations such as constrained
parametric knowledge and high retraining costs. Retrieval-Augmented Generation
(RAG) augments the generation process by retrieving externally stored knowledge
absent from the models internal parameters. However, RAG methods face
challenges such as information loss and redundant retrievals during multi-round
queries, accompanying the difficulties in precisely characterizing knowledge
gaps for complex tasks. To address these problems, we propose Retrieval
Feedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms
the stateless retrieval of previous methods into stateful continuous knowledge
management by constructing a dynamic evidence pool. Specifically, our method
generates refined queries describing the models knowledge gaps using relational
triples from questions and evidence from the dynamic evidence pool; Retrieves
critical external knowledge to iteratively update this evidence pool; Employs a
R-Feedback Model to evaluate evidence completeness until convergence. Compared
to traditional RAG methods, our approach enables persistent storage of
retrieved passages and effectively distills key information from passages to
construct clearly new queries. Experiments on three public QA benchmarks
demonstrate that RFM-RAG outperforms previous methods and improves overall
system accuracy.

</details>


### [15] [HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data](https://arxiv.org/abs/2508.18048)
*Jiyoon Myung,Jihyeon Park,Joohyung Han*

Main category: cs.IR

TL;DR: HyST, a hybrid retrieval framework, combines structured filtering with embedding-based retrieval to enhance precision in handling real-world user queries over semi-structured data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for a system that combines structured and unstructured data in user queries within real-world recommendation systems, providing an accurate and scalable solution.

Method: The method involves using HyST, which extracts attribute-level constraints through large language models (LLMs) for structured filtering and processes the remaining unstructured components using semantic embedding search.

Result: The result shows that HyST outperforms traditional baselines in a semi-structured benchmark, emphasizing the value of structured filtering in enhancing retrieval precision.

Conclusion: The paper concludes that HyST, a hybrid retrieval framework, effectively improves retrieval precision in real-world user queries by combining structured filtering with embedding-based retrieval.

Abstract: User queries in real-world recommendation systems often combine structured
constraints (e.g., category, attributes) with unstructured preferences (e.g.,
product descriptions or reviews). We introduce HyST (Hybrid retrieval over
Semi-structured Tabular data), a hybrid retrieval framework that combines
LLM-powered structured filtering with semantic embedding search to support
complex information needs over semi-structured tabular data. HyST extracts
attribute-level constraints from natural language using large language models
(LLMs) and applies them as metadata filters, while processing the remaining
unstructured query components via embedding-based retrieval. Experiments on a
semi-structured benchmark show that HyST consistently outperforms tradtional
baselines, highlighting the importance of structured filtering in improving
retrieval precision, offering a scalable and accurate solution for real-world
user queries.

</details>


### [16] [HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation](https://arxiv.org/abs/2508.18118)
*Junyi Chen,Lu Chi,Siliang Xu,Shiwei Ran,Bingyue Peng,Zehuan Yuan*

Main category: cs.IR

TL;DR: 引入HLLM-Creator，通过用户兴趣建模和个性化内容生成实现在线广告标题生成的优化，提高了生成效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC系统难以生成真正个性化的内容，尤其是在产品具有多种卖点的情况下。为了满足用户不同的需求，个性化内容生成变得极具价值。

Method: 提出了HLLM-Creator，一个用于高效用户兴趣建模和个性化内容生成的层级LLM框架。通过用户聚类和用户-广告匹配预测的修剪策略显著提高生成效率，并设计了基于链式思维推理的数据构建管道保证数据的一致性。

Result: 在测试中，HLLM-Creator实现了对抖音搜索广告的个性化标题生成，并在在线A/B测试中获得了0.476%的Adss提升，证明了其效果和效率。

Conclusion: HLLM-Creator成功提升个性化标题生成的效率和效果，特别是在大型工业场景中。代码和相关数据集已经发布，便于进一步研究和产业应用。

Abstract: AI-generated content technologies are widely used in content creation.
However, current AIGC systems rely heavily on creators' inspiration, rarely
generating truly user-personalized content. In real-world applications such as
online advertising, a single product may have multiple selling points, with
different users focusing on different features. This underscores the
significant value of personalized, user-centric creative generation. Effective
personalized content generation faces two main challenges: (1) accurately
modeling user interests and integrating them into the content generation
process while adhering to factual constraints, and (2) ensuring high efficiency
and scalability to handle the massive user base in industrial scenarios.
Additionally, the scarcity of personalized creative data in practice
complicates model training, making data construction another key hurdle. We
propose HLLM-Creator, a hierarchical LLM framework for efficient user interest
modeling and personalized content generation. During inference, a combination
of user clustering and a user-ad-matching-prediction based pruning strategy is
employed to significantly enhance generation efficiency and reduce
computational overhead, making the approach suitable for large-scale
deployment. Moreover, we design a data construction pipeline based on
chain-of-thought reasoning, which generates high-quality, user-specific
creative titles and ensures factual consistency despite limited personalized
data. This pipeline serves as a critical foundation for the effectiveness of
our model. Extensive experiments on personalized title generation for Douyin
Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a
0.476% increase on Adss, paving the way for more effective and efficient
personalized generation in industrial scenarios. Codes for academic dataset are
available at https://github.com/bytedance/HLLM.

</details>


### [17] [Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations](https://arxiv.org/abs/2508.18132)
*Hung-Chun Hsu,Yuan-Ching Kuo,Chao-Han Huck Yang,Szu-Wei Fu,Hanrong Ye,Hongxu Yin,Yu-Chiang Frank Wang,Ming-Feng Tsai,Chuan-Ju Wang*

Main category: cs.IR

TL;DR: 提出了一种新的框架，通过引入测试时缩放和重新排名机制来提高多轮对话中的产品检索性能，实验显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统产品检索系统在处理复杂的多轮用户交互中存在局限性，现有方法难以在多轮对话中有效建模演变的用户意图。

Method: 提出一种新的框架，将测试时缩放引入多模态产品检索，并在生成检索器的基础上增加一个测试时重新排名机制。

Result: 在多个基准上的实验结果显示平均提高了14.5点的MRR和10.6点的nDCG@1。

Conclusion: 引入测试时缩放和重新排名机制的框架显著提高了多轮对话中的检索准确性，并与用户意图保持一致。

Abstract: The rapid evolution of e-commerce has exposed the limitations of traditional
product retrieval systems in managing complex, multi-turn user interactions.
Recent advances in multimodal generative retrieval -- particularly those
leveraging multimodal large language models (MLLMs) as retrievers -- have shown
promise. However, most existing methods are tailored to single-turn scenarios
and struggle to model the evolving intent and iterative nature of multi-turn
dialogues when applied naively. Concurrently, test-time scaling has emerged as
a powerful paradigm for improving large language model (LLM) performance
through iterative inference-time refinement. Yet, its effectiveness typically
relies on two conditions: (1) a well-defined problem space (e.g., mathematical
reasoning), and (2) the model's ability to self-correct -- conditions that are
rarely met in conversational product search. In this setting, user queries are
often ambiguous and evolving, and MLLMs alone have difficulty grounding
responses in a fixed product corpus. Motivated by these challenges, we propose
a novel framework that introduces test-time scaling into conversational
multimodal product retrieval. Our approach builds on a generative retriever,
further augmented with a test-time reranking (TTR) mechanism that improves
retrieval accuracy and better aligns results with evolving user intent
throughout the dialogue. Experiments across multiple benchmarks show consistent
improvements, with average gains of 14.5 points in MRR and 10.6 points in
nDCG@1.

</details>


### [18] [PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation](https://arxiv.org/abs/2508.18166)
*Bin Tan,Wangyao Ge,Yidi Wang,Xin Liu,Jeff Burtoft,Hao Fan,Hui Wang*

Main category: cs.IR

TL;DR: PCR-CA框架通过并行编码和对比对齐机制改善多类别应用推荐，实验显示明显提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有应用商店推荐系统在处理多类别应用时存在困难，传统分类体系难以捕捉重叠的语义，导致个性化推荐效果不佳。

Method: 我们提出了PCR-CA框架，通过并行代码簿表达和对比对齐机制提高点击率预测。首先从应用文本中提取多模态嵌入，然后采用并行代码簿VQ-AE模块学习离散语义表达。与层次残差量化不同，该设计实现了独立编码多样方面，并使用对比对齐损失增强长尾项目的表示学习。

Result: 实验显示，PCR-CA在大规模数据集上比强基线提升0.76% AUC，长尾应用上提升2.15% AUC。在线A/B测试进一步证明，其在CTR上提升10.52%，CVR上提升16.30%。

Conclusion: PCR-CA框架成功解决了多类别应用推荐中的语义重叠问题，显著提高了推荐系统的预测准确性和用户满意度。

Abstract: Modern app store recommender systems struggle with multiple-category apps, as
traditional taxonomies fail to capture overlapping semantics, leading to
suboptimal personalization. We propose PCR-CA (Parallel Codebook
Representations with Contrastive Alignment), an end-to-end framework for
improved CTR prediction. PCR-CA first extracts compact multimodal embeddings
from app text, then introduces a Parallel Codebook VQ-AE module that learns
discrete semantic representations across multiple codebooks in parallel --
unlike hierarchical residual quantization (RQ-VAE). This design enables
independent encoding of diverse aspects (e.g., gameplay, art style), better
modeling multiple-category semantics. To bridge semantic and collaborative
signals, we employ a contrastive alignment loss at both the user and item
levels, enhancing representation learning for long-tail items. Additionally, a
dual-attention fusion mechanism combines ID-based and semantic features to
capture user interests, especially for long-tail apps. Experiments on a
large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong
baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further
validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement
in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new
framework has now been fully deployed on the Microsoft Store.

</details>
