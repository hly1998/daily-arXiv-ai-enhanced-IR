<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Overview of the TREC 2024 NeuCLIR Track](https://arxiv.org/abs/2509.14355)
*Dawn Lawrie,Sean MacAvaney,James Mayfield,Paul McNamee,Douglas W. Oard,Luca Soldaini,Eugene Yang*

Main category: cs.IR

TL;DR: 研究神经方法对跨语言信息访问的影响，创建了多种语言的新闻和学术文档测试集，完成了多任务实验。


<details>
  <summary>Details</summary>
Motivation: 研究神经方法在跨语言信息访问中的影响。

Method: 包括四种任务类型：新闻的跨语言信息检索（CLIR）、新闻的多语言信息检索（MLIR）、报告生成以及技术文档的CLIR。共进行了274次实验。

Result: 该研究创建了包含中文、波斯语和俄语新闻故事以及中文学术摘要的测试集合。

Conclusion: 可以研究神经方法在跨语言信息检索中的实际效果。

Abstract: The principal goal of the TREC Neural Cross-Language Information Retrieval
(NeuCLIR) track is to study the effect of neural approaches on cross-language
information access. The track has created test collections containing Chinese,
Persian, and Russian news stories and Chinese academic abstracts. NeuCLIR
includes four task types: Cross-Language Information Retrieval (CLIR) from
news, Multilingual Information Retrieval (MLIR) from news, Report Generation
from news, and CLIR from technical documents. A total of 274 runs were
submitted by five participating teams (and as baselines by the track
coordinators) for eight tasks across these four task types. Task descriptions
and the available results are presented.

</details>


### [2] [When Content is Goliath and Algorithm is David: The Style and Semantic Effects of Generative Search Engine](https://arxiv.org/abs/2509.14436)
*Lijia Ma,Juan Qin,Xingchen Xu,Yong Tan*

Main category: cs.IR

TL;DR: 本文研究生成性搜索引擎如何利用大语言模型进行网站引用，并改变搜索引擎优化格局。通过与Google的生成性和传统搜索平台互动收集约一万个网站的数据，分析显示GEs倾向于引用预期性高和语义相似度大的内容，并通过实验证明这种偏好来自LLMs的生成表达模式。还发现对网站内容进行基于LLM的润色会增加AI摘要的信息多样性。最终制造一个生成性搜索引擎并通过随机对照实验评估用户端影响。


<details>
  <summary>Details</summary>
Motivation: 生成性搜索引擎改变搜索引擎优化格局，并通过网站内容优化应用LLMs以影响AI生成的总结信息多样性。

Method: 通过与Google的生成性和传统搜索平台互动收集数据，并进行控制实验使用RAG API来分析生成性搜索引擎的特点。设计生成性搜索引擎并进行随机对照实验以评估用户影响。

Result: GEs倾向引用预期较高和语义相似的内容。基于LLM对网站内容进行润色可增加AI总结的信息多样性。高学历用户因网站润色减少完成时间，低学历用户增加信息密度保持完成时间不变。

Conclusion: 生成性搜索引擎对引用网站的偏好源于LLM的生成表达模式。这种偏好通过基于LLM优化网站内容可以改变，从而增加AI摘要的信息多样性。高学历用户因原始网站润色而减少任务完成时间，低学历用户则提高任务输出的信息密度。

Abstract: Generative search engines (GEs) leverage large language models (LLMs) to
deliver AI-generated summaries with website citations, establishing novel
traffic acquisition channels while fundamentally altering the search engine
optimization landscape. To investigate the distinctive characteristics of GEs,
we collect data through interactions with Google's generative and conventional
search platforms, compiling a dataset of approximately ten thousand websites
across both channels. Our empirical analysis reveals that GEs exhibit
preferences for citing content characterized by significantly higher
predictability for underlying LLMs and greater semantic similarity among
selected sources. Through controlled experiments utilizing retrieval augmented
generation (RAG) APIs, we demonstrate that these citation preferences emerge
from intrinsic LLM tendencies to favor content aligned with their generative
expression patterns. Motivated by applications of LLMs to optimize website
content, we conduct additional experimentation to explore how LLM-based content
polishing by website proprietors alters AI summaries, finding that such
polishing paradoxically enhances information diversity within AI summaries.
Finally, to assess the user-end impact of LLM-induced information increases, we
design a generative search engine and recruit Prolific participants to conduct
a randomized controlled experiment involving an information-seeking and writing
task. We find that higher-educated users exhibit minimal changes in their final
outputs' information diversity but demonstrate significantly reduced task
completion time when original sites undergo polishing. Conversely,
lower-educated users primarily benefit through enhanced information density in
their task outputs while maintaining similar completion times across
experimental groups.

</details>


### [3] [Keywords are not always the key: A metadata field analysis for natural language search on open data portals](https://arxiv.org/abs/2509.14457)
*Lisa-Yao Gan,Arunav Das,Johanna Walker,Elena Simperl*

Main category: cs.IR

TL;DR: 开放数据门户的搜索界面存在局限性，导致用户难以使用自然语言查询找到数据集。研究表明，LLM生成的元数据描述可以改善数据集检索效果。


<details>
  <summary>Details</summary>
Motivation: 开放数据门户的搜索界面通常依赖于基于关键词的机制和有限的元数据字段，这使得用户难以使用自然语言查询找到数据集。尤其是当用户不熟悉领域术语时，不完整或不一致的元数据加剧了这一问题。

Method: 进行受控消融实验，以模拟自然语言查询对各种元数据配置下的检索性能进行评估。比较现有的“描述”元数据字段内容与LLM生成的内容，并探索不同提示策略对质量和检索结果的影响。

Result: 研究发现，数据集描述在对齐用户意图方面起着核心作用，并且LLM生成的描述可以支持有效检索。

Conclusion: 当前元数据实践存在局限，生成模型有潜力提升开放数据门户的数据集可发现性。

Abstract: Open data portals are essential for providing public access to open datasets.
However, their search interfaces typically rely on keyword-based mechanisms and
a narrow set of metadata fields. This design makes it difficult for users to
find datasets using natural language queries. The problem is worsened by
metadata that is often incomplete or inconsistent, especially when users lack
familiarity with domain-specific terminology. In this paper, we examine how
individual metadata fields affect the success of conversational dataset
retrieval and whether LLMs can help bridge the gap between natural queries and
structured metadata. We conduct a controlled ablation study using simulated
natural language queries over real-world datasets to evaluate retrieval
performance under various metadata configurations. We also compare existing
content of the metadata field 'description' with LLM-generated content,
exploring how different prompting strategies influence quality and impact on
search outcomes. Our findings suggest that dataset descriptions play a central
role in aligning with user intent, and that LLM-generated descriptions can
support effective retrieval. These results highlight both the limitations of
current metadata practices and the potential of generative models to improve
dataset discoverability in open data portals.

</details>


### [4] [What Matters in LLM-Based Feature Extractor for Recommender? A Systematic Analysis of Prompts, Models, and Adaptation](https://arxiv.org/abs/2509.14979)
*Kainan Shi,Peilin Zhou,Ge Wang,Han Ding,Fei Wang*

Main category: cs.IR

TL;DR: RecXplore是一种模块化分析框架，可以优化和比较大型语言模型增强的推荐系统设计，无需新技术就能显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的利用大型语言模型（LLMs）来增强序列推荐系统（SRS）的方法在提示、架构和适应策略方面差异很大，使得难以公平地比较设计选择并识别性能的真正驱动因素。

Method: 提出了RecXplore，一个模块化分析框架，分解LLM作为特征提取器的流程为四个模块：数据处理、语义特征提取、特征适应和序列建模。

Result: 实验表明，仅仅结合现有技术的最佳设计就可以在NDCG@5和HR@5评估指标上相较于强项基线提高最多18.7%和12.7%。

Conclusion: 模块化基准测试有助于识别有效的设计模式，促进LLM增强推荐中的标准化研究。

Abstract: Using Large Language Models (LLMs) to generate semantic features has been
demonstrated as a powerful paradigm for enhancing Sequential Recommender
Systems (SRS). This typically involves three stages: processing item text,
extracting features with LLMs, and adapting them for downstream models.
However, existing methods vary widely in prompting, architecture, and
adaptation strategies, making it difficult to fairly compare design choices and
identify what truly drives performance. In this work, we propose RecXplore, a
modular analytical framework that decomposes the LLM-as-feature-extractor
pipeline into four modules: data processing, semantic feature extraction,
feature adaptation, and sequential modeling. Instead of proposing new
techniques, RecXplore revisits and organizes established methods, enabling
systematic exploration of each module in isolation. Experiments on four public
datasets show that simply combining the best designs from existing techniques
without exhaustive search yields up to 18.7% relative improvement in NDCG@5 and
12.7% in HR@5 over strong baselines. These results underscore the utility of
modular benchmarking for identifying effective design patterns and promoting
standardized research in LLM-enhanced recommendation.

</details>
