{"id": "2509.00109", "categories": ["cs.IR", "cs.LG", "H.3.3; I.2.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2509.00109", "abs": "https://arxiv.org/abs/2509.00109", "authors": ["Theodor Stoecker", "Samed Bayer", "Ingo Weber"], "title": "Bias Mitigation for AI-Feedback Loops in Recommender Systems: A Systematic Literature Review and Taxonomy", "comment": "11 pages, 6 figures, 2 tables. Accepted at the FAccTRec '25 Workshop,\n  ACM RecSys 2025 (Prague)", "summary": "Recommender systems continually retrain on user reactions to their own\npredictions, creating AI feedback loops that amplify biases and diminish\nfairness over time. Despite this well-known risk, most bias mitigation\ntechniques are tested only on static splits, so their long-term fairness across\nmultiple retraining rounds remains unclear. We therefore present a systematic\nliterature review of bias mitigation methods that explicitly consider AI\nfeedback loops and are validated in multi-round simulations or live A/B tests.\nScreening 347 papers yields 24 primary studies published between 2019-2025.\nEach study is coded on six dimensions: mitigation technique, biases addressed,\ndynamic testing set-up, evaluation focus, application domain, and ML task,\norganising them into a reusable taxonomy. The taxonomy offers industry\npractitioners a quick checklist for selecting robust methods and gives\nresearchers a clear roadmap to the field's most urgent gaps. Examples include\nthe shortage of shared simulators, varying evaluation metrics, and the fact\nthat most studies report either fairness or performance; only six use both."}
{"id": "2509.00199", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00199", "abs": "https://arxiv.org/abs/2509.00199", "authors": ["Chen Zheng", "Zhenyu Zhao"], "title": "Algorithm Adaptation Bias in Recommendation System Online Experiments", "comment": null, "summary": "Online experiments (A/B tests) are widely regarded as the gold standard for\nevaluating recommender system variants and guiding launch decisions. However, a\nvariety of biases can distort the results of the experiment and mislead\ndecision-making. An underexplored but critical bias is algorithm adaptation\neffect. This bias arises from the flywheel dynamics among production models,\nuser data, and training pipelines: new models are evaluated on user data whose\ndistributions are shaped by the incumbent system or tested only in a small\ntreatment group. As a result, the measured effect of a new product change in\nmodeling and user experience in this constrained experimental setting can\ndiverge substantially from its true impact in full deployment. In practice, the\nexperiment results often favor the production variant with large traffic while\nunderestimating the performance of the test variant with small traffic, which\nleads to missing opportunities to launch a true winning arm or underestimating\nthe impact. This paper aims to raise awareness of algorithm adaptation bias,\nsituate it within the broader landscape of RecSys evaluation biases, and\nmotivate discussion of solutions that span experiment design, measurement, and\nadjustment. We detail the mechanisms of this bias, present empirical evidence\nfrom real-world experiments, and discuss potential methods for a more robust\nonline evaluation."}
{"id": "2509.00389", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00389", "abs": "https://arxiv.org/abs/2509.00389", "authors": ["Xiaoxin Ye", "Chengkai Huang", "Hongtao Huang", "Lina Yao"], "title": "Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain Sequential Recommendation", "comment": null, "summary": "Cross-Domain Sequential Recommendation (CDSR) leverages user behaviors across\ndomains to enhance recommendation quality. However, naive aggregation of\nsequential signals can introduce conflicting domain-specific preferences,\nleading to negative transfer. While Sequential Recommendation (SR) already\nsuffers from noisy behaviors such as misclicks and impulsive actions, CDSR\nfurther amplifies this issue due to domain heterogeneity arising from diverse\nitem types and user intents. The core challenge is disentangling three\nintertwined signals: domain-invariant preferences, domain-specific preferences,\nand noise. Diffusion Models (DMs) offer a generative denoising framework\nwell-suited for disentangling complex user preferences and enhancing robustness\nto noise. Their iterative refinement process enables gradual denoising, making\nthem effective at capturing subtle preference signals. However, existing\napplications in recommendation face notable limitations: sequential DMs often\nconflate shared and domain-specific preferences, while cross-domain\ncollaborative filtering DMs neglect temporal dynamics, limiting their ability\nto model evolving user preferences. To bridge these gaps, we propose\n\\textbf{DPG-Diff}, a novel Disentangled Preference-Guided Diffusion Model, the\nfirst diffusion-based approach tailored for CDSR, to or best knowledge.\nDPG-Diff decomposes user preferences into domain-invariant and domain-specific\ncomponents, which jointly guide the reverse diffusion process. This\ndisentangled guidance enables robust cross-domain knowledge transfer, mitigates\nnegative transfer, and filters sequential noise. Extensive experiments on\nreal-world datasets demonstrate that DPG-Diff consistently outperforms\nstate-of-the-art baselines across multiple metrics."}
{"id": "2509.00520", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.00520", "abs": "https://arxiv.org/abs/2509.00520", "authors": ["Yuzheng Cai", "Yanzhao Zhang", "Dingkun Long", "Mingxin Li", "Pengjun Xie", "Weiguo Zheng"], "title": "ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for Effective and Efficient Text Reranking", "comment": null, "summary": "Text reranking models are a crucial component in modern systems like\nRetrieval-Augmented Generation, tasked with selecting the most relevant\ndocuments prior to generation. However, current Large Language Models (LLMs)\npowered rerankers often face a fundamental trade-off. On one hand, Supervised\nFine-Tuning based pointwise methods that frame relevance as a binary\nclassification task lack the necessary scoring discrimination, particularly for\nthose built on reasoning LLMs. On the other hand, approaches designed for\ncomplex reasoning often employ powerful yet inefficient listwise formulations,\nrendering them impractical for low latency applications. To resolve this\ndilemma, we introduce ERank, a highly effective and efficient pointwise\nreranker built from a reasoning LLM that excels across diverse relevance\nscenarios. We propose a novel two-stage training pipeline that begins with\nSupervised Fine-Tuning (SFT). In this stage, we move beyond binary labels and\ntrain the model generatively to output fine grained integer scores, which\nsignificantly enhances relevance discrimination. The model is then further\nrefined using Reinforcement Learning (RL) with a novel, listwise derived\nreward. This technique instills global ranking awareness into the efficient\npointwise architecture. We evaluate the ERank reranker on the BRIGHT, FollowIR,\nTREC DL, and BEIR benchmarks, demonstrating superior effectiveness and\nrobustness compared to existing approaches. On the reasoning-intensive BRIGHT\nbenchmark, our ERank-4B achieves an nDCG@10 of 38.7, while a larger 32B variant\nreaches a state of the art nDCG@10 of 40.2."}
{"id": "2509.00728", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.00728", "abs": "https://arxiv.org/abs/2509.00728", "authors": ["Pengyue Li", "Sheng Wang", "Hua Dai", "Zhiyu Chen", "Zhifeng Bao", "Brian D. Davison"], "title": "A Survey on Open Dataset Search in the LLM Era: Retrospectives and Perspectives", "comment": null, "summary": "High-quality datasets are typically required for accomplishing data-driven\ntasks, such as training medical diagnosis models, predicting real-time traffic\nconditions, or conducting experiments to validate research hypotheses.\nConsequently, open dataset search, which aims to ensure the efficient and\naccurate fulfillment of users' dataset requirements, has emerged as a critical\nresearch challenge and has attracted widespread interest. Recent studies have\nmade notable progress in enhancing the flexibility and intelligence of open\ndataset search, and large language models (LLMs) have demonstrated strong\npotential in addressing long-standing challenges in this area. Therefore, a\nsystematic and comprehensive review of the open dataset search problem is\nessential, detailing the current state of research and exploring future\ndirections. In this survey, we focus on recent advances in open dataset search\nbeyond traditional approaches that rely on metadata and keywords. From the\nperspective of dataset modalities, we place particular emphasis on\nexample-based dataset search, advanced similarity measurement techniques based\non dataset content, and efficient search acceleration techniques. In addition,\nwe emphasize the mutually beneficial relationship between LLMs and open dataset\nsearch. On the one hand, LLMs help address complex challenges in query\nunderstanding, semantic modeling, and interactive guidance within open dataset\nsearch. In turn, advances in dataset search can support LLMs by enabling more\neffective integration into retrieval-augmented generation (RAG) frameworks and\ndata selection processes, thereby enhancing downstream task performance.\nFinally, we summarize open research problems and outline promising directions\nfor future work. This work aims to offer a structured reference for researchers\nand practitioners in the field of open dataset search."}
{"id": "2509.00909", "categories": ["cs.IR", "I.7.5; I.7.2; I.2.7; H.3.1"], "pdf": "https://arxiv.org/pdf/2509.00909", "abs": "https://arxiv.org/abs/2509.00909", "authors": ["Sabine Wehnert", "Harikrishnan Changaramkulath", "Ernesto William De Luca"], "title": "HiPS: Hierarchical PDF Segmentation of Textbooks", "comment": "10 pages, 7 figures, submitted to the 19th ACM International\n  Conference on Web Search and Data Mining (WSDM)", "summary": "The growing demand for effective tools to parse PDF-formatted texts,\nparticularly structured documents such as textbooks, reveals the limitations of\ncurrent methods developed mainly for research paper segmentation. This work\naddresses the challenge of hierarchical segmentation in complex structured\ndocuments, with a focus on legal textbooks that contain layered knowledge\nessential for interpreting and applying legal norms. We examine a Table of\nContents (TOC)-based technique and approaches that rely on open-source\nstructural parsing tools or Large Language Models (LLMs) operating without\nexplicit TOC input. To enhance parsing accuracy, we incorporate preprocessing\nstrategies such as OCR-based title detection, XML-derived features, and\ncontextual text features. These strategies are evaluated based on their ability\nto identify section titles, allocate hierarchy levels, and determine section\nboundaries. Our findings show that combining LLMs with structure-aware\npreprocessing substantially reduces false positives and improves extraction\nquality. We also find that when the metadata quality of headings in the PDF is\nhigh, TOC-based techniques perform particularly well. All code and data are\npublicly available to support replication. We conclude with a comparative\nevaluation of the methods, outlining their respective strengths and\nlimitations."}
{"id": "2509.00986", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.00986", "abs": "https://arxiv.org/abs/2509.00986", "authors": ["Darko Sasanski", "Riste Stojanov"], "title": "Food Data in the Semantic Web: A Review of Nutritional Resources, Knowledge Graphs, and Emerging Applications", "comment": null, "summary": "This comprehensive review explores food data in the Semantic Web,\nhighlighting key nutritional resources, knowledge graphs, and emerging\napplications in the food domain. It examines prominent food data resources such\nas USDA, FoodOn, FooDB, and Recipe1M+, emphasizing their contributions to\nnutritional data representation. Special focus is given to food entity linking\nand recognition techniques, which enable integration of heterogeneous food data\nsources into cohesive semantic resources. The review further discusses food\nknowledge graphs, their role in semantic interoperability, data enrichment, and\nknowledge extraction, and their applications in personalized nutrition,\ningredient substitution, food-drug and food-disease interactions, and\ninterdisciplinary research. By synthesizing current advancements and\nidentifying challenges, this work provides insights to guide future\ndevelopments in leveraging semantic technologies for the food domain."}
{"id": "2509.01030", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.01030", "abs": "https://arxiv.org/abs/2509.01030", "authors": ["Alexis Horde Vo", "Matt Duckham", "Estrid He", "Rafe Benli"], "title": "Identifying Origins of Place Names via Retrieval Augmented Generation", "comment": null, "summary": "Who is the \"Batman\" behind \"Batman Street\" in Melbourne? Understanding the\nhistorical, cultural, and societal narratives behind place names can reveal the\nrich context that has shaped a community. Although place names serve as\nessential spatial references in gazetteers, they often lack information about\nplace name origins. Enriching these place names in today's gazetteers is a\ntime-consuming, manual process that requires extensive exploration of a vast\narchive of documents and text sources. Recent advances in natural language\nprocessing and language models (LMs) hold the promise of significant automation\nof identifying place name origins due to their powerful capability to exploit\nthe semantics of the stored documents. This chapter presents a retrieval\naugmented generation pipeline designed to search for place name origins over a\nbroad knowledge base, DBpedia. Given a spatial query, our approach first\nextracts sub-graphs that may contain knowledge relevant to the query; then\nranks the extracted sub-graphs to generate the final answer to the query using\nfine-tuned LM-based models (i.e., ColBERTv2 and Llama2). Our results highlight\nthe key challenges facing automated retrieval of place name origins, especially\nthe tendency of language models to under-use the spatial information contained\nin texts as a discriminating factor. Our approach also frames the wider\nimplications for geographic information retrieval using retrieval augmented\ngeneration."}
{"id": "2509.01129", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.01129", "abs": "https://arxiv.org/abs/2509.01129", "authors": ["Shiwen Zhang", "Lingxiang Wang", "Hainan Zhang", "Ziwei Wang", "Sijia Wen", "Zhiming Zheng"], "title": "Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level Code Generation", "comment": null, "summary": "In competitive programming task, problem statements are often embedded within\nelaborate narrative backgrounds, requiring deep understanding of the underlying\nsolutions to successfully complete the tasks. Current code generation models\nprimarily focus on token-level semantic modeling, highly susceptible to\ndistractions from irrelevant narrative statements. Inspired by RAG, retrieving\nreference code with similar solutions may help enhance model performance on\ndifficult problems. However, existing retrieval models also emphasize\nsurface-level semantic similarity, neglecting the deeper solution-level logical\nsimilarities that are critical in competitive programming. Therefore, designing\nranking models capable of accurately identifying and retrieving problems and\ncorresponding codes remains an urgent research problem in competitive code\ngeneration. In this paper, we propose SolveRank, a solution-aware ranking model\nempowered by synthetic data for competitive programming tasks. Specifically, we\nleverage the DeepSeek-R1 model to generate logically equivalent but differently\nphrased new problems, verified by GPT-4o for solution consistency. Then, we\ntrain SolveRank with these as positive samples and BM25/random-retrieved\nproblems as negatives. During inference, SolveRank retrieves relevant problems\nand corresponding code from the corpus to assist a downstream code generator.\nExperiments on the xCodeEval dataset demonstrate that SolveRank outperforms\nSOTA ranking methods in precision and recall metrics, and boosts code\ngeneration performance for difficult problems."}
{"id": "2509.01184", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.01184", "abs": "https://arxiv.org/abs/2509.01184", "authors": ["Yutian Xiao", "Shukuan Wang", "Binhao Wang", "Zhao Zhang", "Yanze Zhang", "Shanqi Liu", "Chao Feng", "Xiang Li", "Fuzhen Zhuang"], "title": "MARS: Modality-Aligned Retrieval for Sequence Augmented CTR Prediction", "comment": null, "summary": "Click-through rate (CTR) prediction serves as a cornerstone of recommender\nsystems. Despite the strong performance of current CTR models based on user\nbehavior modeling, they are still severely limited by interaction sparsity,\nespecially in low-active user scenarios. To address this issue, data\naugmentation of user behavior is a promising research direction. However,\nexisting data augmentation methods heavily rely on collaborative signals while\noverlooking the rich multimodal features of items, leading to insufficient\nmodeling of low-active users.\n  To alleviate this problem, we propose a novel framework \\textbf{MARS}\n(\\textbf{M}odality-\\textbf{A}ligned \\textbf{R}etrieval for \\textbf{S}equence\nAugmented CTR Prediction). MARS utilizes a Stein kernel-based approach to align\ntext and image features into a unified and unbiased semantic space to construct\nmultimodal user embeddings. Subsequently, each low-active user's behavior\nsequence is augmented by retrieving, filtering, and concentrating the most\nsimilar behavior sequence of high-active users via multimodal user embeddings.\nValidated by extensive offline experiments and online A/B tests, our framework\nMARS consistently outperforms state-of-the-art baselines and achieves\nsubstantial growth on core business metrics within\nKuaishou~\\footnote{https://www.kuaishou.com/}. Consequently, MARS has been\nsuccessfully deployed, serving the main traffic for hundreds of millions of\nusers. To ensure reproducibility, we provide anonymous access to the\nimplementation code~\\footnote{https://github.com/wangshukuan/MARS}."}
{"id": "2509.01306", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01306", "abs": "https://arxiv.org/abs/2509.01306", "authors": ["Jiawei Cao", "Jie Ouyang", "Zhaomeng Zhou", "Mingyue Cheng", "Yupeng Li", "Jiaxian Yan", "Qi Liu"], "title": "Re3: Learning to Balance Relevance & Recency for Temporal Information Retrieval", "comment": null, "summary": "Temporal Information Retrieval (TIR) is a critical yet unresolved task for\nmodern search systems, retrieving documents that not only satisfy a query's\ninformation need but also adhere to its temporal constraints. This task is\nshaped by two challenges: Relevance, ensuring alignment with the query's\nexplicit temporal requirements, and Recency, selecting the freshest document\namong multiple versions. Existing methods often address the two challenges in\nisolation, relying on brittle heuristics that fail in scenarios where temporal\nrequirements and staleness resistance are intertwined. To address this gap, we\nintroduce Re2Bench, a benchmark specifically designed to disentangle and\nevaluate Relevance, Recency, and their hybrid combination. Building on this\nfoundation, we propose Re3, a unified and lightweight framework that\ndynamically balances semantic and temporal information through a query-aware\ngating mechanism. On Re2Bench, Re3 achieves state-of-the-art results, leading\nin R@1 across all three subsets. Ablation studies with backbone sensitivity\ntests confirm robustness, showing strong generalization across diverse encoders\nand real-world settings. This work provides both a generalizable solution and a\nprincipled evaluation suite, advancing the development of temporally aware\nretrieval systems. Re3 and Re2Bench are available online:\nhttps://anonymous.4open.science/r/Re3-0C5A"}
{"id": "2509.01536", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.01536", "abs": "https://arxiv.org/abs/2509.01536", "authors": ["Ebrahim Norouzi", "Nicole Jung", "Anna M. Jacyszyn", "JÃ¶rg Waitelonis", "Harald Sack"], "title": "AI4DiTraRe: Building the BFO-Compliant Chemotion Knowledge Graph", "comment": null, "summary": "Chemistry is an example of a discipline where the advancements of technology\nhave led to multi-level and often tangled and tricky processes ongoing in the\nlab. The repeatedly complex workflows are combined with information from\nchemical structures, which are essential to understand the scientific process.\nAn important tool for many chemists is Chemotion, which consists of an\nelectronic lab notebook and a repository. This paper introduces a semantic\npipeline for constructing the BFO-compliant Chemotion Knowledge Graph,\nproviding an integrated, ontology-driven representation of chemical research\ndata. The Chemotion-KG has been developed to adhere to the FAIR (Findable,\nAccessible, Interoperable, Reusable) principles and to support AI-driven\ndiscovery and reasoning in chemistry. Experimental metadata were harvested from\nthe Chemotion API in JSON-LD format, converted into RDF, and subsequently\ntransformed into a Basic Formal Ontology-aligned graph through SPARQL CONSTRUCT\nqueries. The source code and datasets are publicly available via GitHub. The\nChemotion Knowledge Graph is hosted by FIZ Karlsruhe Information Service\nEngineering. Outcomes presented in this work were achieved within the Leibniz\nScience Campus ``Digital Transformation of Research'' (DiTraRe) and are part of\nan ongoing interdisciplinary collaboration."}
{"id": "2509.01549", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01549", "abs": "https://arxiv.org/abs/2509.01549", "authors": ["Viacheslav Yusupov", "Maxim Rakhuba", "Evgeny Frolov"], "title": "Ultra Fast Warm Start Solution for Graph Recommendations", "comment": "Accepted to CIKM 2025", "summary": "In this work, we present a fast and effective Linear approach for updating\nrecommendations in a scalable graph-based recommender system UltraGCN. Solving\nthis task is extremely important to maintain the relevance of the\nrecommendations under the conditions of a large amount of new data and changing\nuser preferences. To address this issue, we adapt the simple yet effective\nlow-rank approximation approach to the graph-based model. Our method delivers\ninstantaneous recommendations that are up to 30 times faster than conventional\nmethods, with gains in recommendation quality, and demonstrates high\nscalability even on the large catalogue datasets."}
{"id": "2509.01551", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.01551", "abs": "https://arxiv.org/abs/2509.01551", "authors": ["Jing Long", "Sirui Huang", "Huan Huo", "Tong Chen", "Hongzhi Yin", "Guandong Xu"], "title": "Cloud-Device Collaborative Agents for Sequential Recommendation", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled agent-based\nrecommendation systems with strong semantic understanding and flexible\nreasoning capabilities. While LLM-based agents deployed in the cloud offer\npowerful personalization, they often suffer from privacy concerns, limited\naccess to real-time signals, and scalability bottlenecks. Conversely, on-device\nagents ensure privacy and responsiveness but lack the computational power for\nglobal modeling and large-scale retrieval. To bridge these complementary\nlimitations, we propose CDA4Rec, a novel Cloud-Device collaborative framework\nfor sequential Recommendation, powered by dual agents: a cloud-side LLM and a\ndevice-side small language model (SLM). CDA4Rec tackles the core challenge of\ncloud-device coordination by decomposing the recommendation task into modular\nsub-tasks including semantic modeling, candidate retrieval, structured user\nmodeling, and final ranking, which are allocated to cloud or device based on\ncomputational demands and privacy sensitivity. A strategy planning mechanism\nleverages the cloud agent's reasoning ability to generate personalized\nexecution plans, enabling context-aware task assignment and partial parallel\nexecution across agents. This design ensures real-time responsiveness, improved\nefficiency, and fine-grained personalization, even under diverse user states\nand behavioral sparsity. Extensive experiments across multiple real-world\ndatasets demonstrate that CDA4Rec consistently outperforms competitive\nbaselines in both accuracy and efficiency, validating its effectiveness in\nheterogeneous and resource-constrained environments."}
{"id": "2509.01566", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01566", "abs": "https://arxiv.org/abs/2509.01566", "authors": ["Yujing Wang", "Yiren Chen", "Huoran Li", "Chunxu Xu", "Yuchong Luo", "Xianghui Mao", "Cong Li", "Lun Du", "Chunyang Ma", "Qiqi Jiang", "Yin Wang", "Fan Gao", "Wenting Mo", "Pei Wen", "Shantanu Kumar", "Taejin Park", "Yiwei Song", "Vijay Rajaram", "Tao Cheng", "Sonu Durgia", "Pranam Kolari"], "title": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets", "comment": "7 pages, 3 figures", "summary": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate."}
{"id": "2509.02017", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02017", "abs": "https://arxiv.org/abs/2509.02017", "authors": ["Yuhao Wang", "Junwei Pan", "Xinhang Li", "Maolin Wang", "Yuan Wang", "Yue Liu", "Dapeng Liu", "Jie Jiang", "Xiangyu Zhao"], "title": "Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs", "comment": "CIKM 2025 Full Research Paper", "summary": "Sequential recommendation (SR) aims to capture users' dynamic interests and\nsequential patterns based on their historical interactions. Recently, the\npowerful capabilities of large language models (LLMs) have driven their\nadoption in SR. However, we identify two critical challenges in existing\nLLM-based SR methods: 1) embedding collapse when incorporating pre-trained\ncollaborative embeddings and 2) catastrophic forgetting of quantized embeddings\nwhen utilizing semantic IDs. These issues dampen the model scalability and lead\nto suboptimal recommendation performance. Therefore, based on LLMs like\nLlama3-8B-instruct, we introduce a novel SR framework named MME-SID, which\nintegrates multimodal embeddings and quantized embeddings to mitigate embedding\ncollapse. Additionally, we propose a Multimodal Residual Quantized Variational\nAutoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstruction\nloss and contrastive learning for alignment, which effectively preserve\nintra-modal distance information and capture inter-modal correlations,\nrespectively. To further alleviate catastrophic forgetting, we initialize the\nmodel with the trained multimodal code embeddings. Finally, we fine-tune the\nLLM efficiently using LoRA in a multimodal frequency-aware fusion manner.\nExtensive experiments on three public datasets validate the superior\nperformance of MME-SID thanks to its capability to mitigate embedding collapse\nand catastrophic forgetting. The implementation code and datasets are publicly\navailable for reproduction:\nhttps://github.com/Applied-Machine-Learning-Lab/MME-SID."}
{"id": "2509.02220", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02220", "abs": "https://arxiv.org/abs/2509.02220", "authors": ["Markus Reiter-Haas", "Elisabeth Lex"], "title": "Towards Multi-Aspect Diversification of News Recommendations Using Neuro-Symbolic AI for Individual and Societal Benefit", "comment": "Accepted at INRA 2025: 13th International Workshop on News\n  Recommendation and Analytics in Conjunction with ACM RecSys 2025", "summary": "News recommendations are complex, with diversity playing a vital role. So\nfar, existing literature predominantly focuses on specific aspects of news\ndiversity, such as viewpoints. In this paper, we introduce multi-aspect\ndiversification in four distinct recommendation modes and outline the nuanced\nchallenges in diversifying lists, sequences, summaries, and interactions. Our\nproposed research direction combines symbolic and subsymbolic artificial\nintelligence, leveraging both knowledge graphs and rule learning. We plan to\nevaluate our models using user studies to not only capture behavior but also\ntheir perceived experience. Our vision to balance news consumption points to\nother positive effects for users (e.g., increased serendipity) and society\n(e.g., decreased polarization)."}
{"id": "2509.02227", "categories": ["cs.IR", "cs.AI", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2509.02227", "abs": "https://arxiv.org/abs/2509.02227", "authors": ["Qing Dai", "Rasmus Ischebeck", "Maruisz Sapinski", "Adam Grycner"], "title": "Application Of Large Language Models For The Extraction Of Information From Particle Accelerator Technical Documentation", "comment": null, "summary": "The large set of technical documentation of legacy accelerator systems,\ncoupled with the retirement of experienced personnel, underscores the urgent\nneed for efficient methods to preserve and transfer specialized knowledge. This\npaper explores the application of large language models (LLMs), to automate and\nenhance the extraction of information from particle accelerator technical\ndocuments. By exploiting LLMs, we aim to address the challenges of knowledge\nretention, enabling the retrieval of domain expertise embedded in legacy\ndocumentation. We present initial results of adapting LLMs to this specialized\ndomain. Our evaluation demonstrates the effectiveness of LLMs in extracting,\nsummarizing, and organizing knowledge, significantly reducing the risk of\nlosing valuable insights as personnel retire. Furthermore, we discuss the\nlimitations of current LLMs, such as interpretability and handling of rare\ndomain-specific terms, and propose strategies for improvement. This work\nhighlights the potential of LLMs to play a pivotal role in preserving\ninstitutional knowledge and ensuring continuity in highly specialized fields."}
{"id": "2509.02266", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.02266", "abs": "https://arxiv.org/abs/2509.02266", "authors": ["Sourabh Dattawad", "Agnese Daffara", "Tanise Ceron"], "title": "Leveraging Media Frames to Improve Normative Diversity in News Recommendations", "comment": "Accepted at 13th International Workshop on News Recommendation and\n  Analytics in Conjunction with ACM RecSys 2025", "summary": "Click-based news recommender systems suggest users content that aligns with\ntheir existing history, limiting the diversity of articles they encounter.\nRecent advances in aspect-based diversification -- adding features such as\nsentiments or news categories (e.g. world, politics) -- have made progress\ntoward diversifying recommendations in terms of perspectives. However, these\napproaches often overlook the role of news framing, which shapes how stories\nare told by emphasizing specific angles or interpretations. In this paper, we\ntreat media frames as a controllable aspect within the recommendation pipeline.\nBy selecting articles based on a diversity of frames, our approach emphasizes\nvaried narrative angles and broadens the interpretive space recommended to\nusers. In addition to introducing frame-based diversification method, our work\nis the first to assess the impact of a news recommender system that integrates\nframe diversity using normative diversity metrics: representation, calibration,\nand activation. Our experiments based on media frame diversification show an\nimprovement in exposure to previously unclicked frames up to 50%. This is\nimportant because repeated exposure to the same frames can reinforce existing\nbiases or narrow interpretations, whereas introducing novel frames broadens\nusers' understanding of issues and perspectives. The method also enhances\ndiversification across categorical and sentiment levels, thereby demonstrating\nthat framing acts as a strong control lever for enhancing normative diversity."}
{"id": "2509.02377", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.02377", "abs": "https://arxiv.org/abs/2509.02377", "authors": ["Jinseok Kim", "Sukmin Cho", "Soyeong Jeong", "Sangyeop Kim", "Sungzoon Cho"], "title": "Upcycling Candidate Tokens of Large Language Models for Query Expansion", "comment": "CIKM 2025", "summary": "Query Expansion (QE) improves retrieval performance by enriching queries with\nrelated terms. Recently, Large Language Models (LLMs) have been used for QE,\nbut existing methods face a trade-off: generating diverse terms boosts\nperformance but increases computational cost. To address this challenge, we\npropose Candidate Token Query Expansion (CTQE), which extracts diverse and\nrelevant terms from a single LLM decoding pass by leveraging unselected\ncandidate tokens. These tokens, though not part of the final output, are\nconditioned on the full query and capture useful information. By aggregating\nthem, CTQE achieves both relevance and diversity without extra inference,\nreducing overhead and latency. Experiments show that CTQE delivers strong\nretrieval performance with significantly lower cost, outperforming or\ncomparable to more expensive methods. Code is available at:\nhttps://github.com/bluejeans8/CTQE"}
{"id": "2509.02558", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.02558", "abs": "https://arxiv.org/abs/2509.02558", "authors": ["Yijun Ge", "Sahel Sharifymoghaddam", "Jimmy Lin"], "title": "Lighting the Way for BRIGHT: Reproducible Baselines with Anserini, Pyserini, and RankLLM", "comment": "15 pages, 1 figure, 9 tables", "summary": "The BRIGHT benchmark is a dataset consisting of reasoning-intensive queries\nover diverse domains. We explore retrieval results on BRIGHT using a range of\nretrieval techniques, including sparse, dense, and fusion methods, and\nestablish reproducible baselines. We then apply listwise reranking with large\nlanguage models (LLMs) to further investigate the impact of reranking on\nreasoning-intensive queries. These baselines are integrated into popular\nretrieval and reranking toolkits Anserini, Pyserini, and RankLLM, with\ntwo-click reproducibility that makes them easy to build upon and convenient for\nfurther development. While attempting to reproduce the results reported in the\noriginal BRIGHT paper, we find that the provided BM25 scores differ notably\nfrom those that we obtain using Anserini and Pyserini. We discover that this\ndifference is due to BRIGHT's implementation of BM25, which applies BM25 on the\nquery rather than using the standard bag-of-words approach, as in Anserini, to\nconstruct query vectors. This difference has become increasingly relevant due\nto the rise of longer queries, with BRIGHT's lengthy reasoning-intensive\nqueries being a prime example, and further accentuated by the increasing usage\nof retrieval-augmented generation, where LLM prompts can grow to be much longer\nthan ''traditional'' search engine queries. Our observation signifies that it\nmay be time to reconsider BM25 approaches going forward in order to better\naccommodate emerging applications. To facilitate this, we integrate query-side\nBM25 into both Anserini and Pyserini."}
