{"id": "2510.01196", "categories": ["cs.IR", "cs.LG", "H.3.3; I.2.0"], "pdf": "https://arxiv.org/pdf/2510.01196", "abs": "https://arxiv.org/abs/2510.01196", "authors": ["Ivo Silva", "Pedro Nogueira", "Guilherme Bonaldo"], "title": "Location Matters: Leveraging Multi-Resolution Geo-Embeddings for Housing Search", "comment": "Accepted to RecSys 2025 (industry track)", "summary": "QuintoAndar Group is Latin America's largest housing platform,\nrevolutionizing property rentals and sales. Headquartered in Brazil, it\nsimplifies the housing process by eliminating paperwork and enhancing\naccessibility for tenants, buyers, and landlords. With thousands of houses\navailable for each city, users struggle to find the ideal home. In this\ncontext, location plays a pivotal role, as it significantly influences property\nvalue, access to amenities, and life quality. A great location can make even a\nmodest home highly desirable. Therefore, incorporating location into\nrecommendations is essential for their effectiveness. We propose a geo-aware\nembedding framework to address sparsity and spatial nuances in housing\nrecommendations on digital rental platforms. Our approach integrates an\nhierarchical H3 grid at multiple levels into a two-tower neural architecture.\nWe compare our method with a traditional matrix factorization baseline and a\nsingle-resolution variant using interaction data from our platform. Embedding\nspecific evaluation reveals richer and more balanced embedding representations,\nwhile offline ranking simulations demonstrate a substantial uplift in\nrecommendation quality.", "AI": {"tldr": "QuintoAndar Group\u901a\u8fc7\u5730\u7406\u611f\u77e5\u5d4c\u5165\u6846\u67b6\u6539\u5584\u4e86\u6570\u5b57\u79df\u8d41\u5e73\u53f0\u7684\u623f\u5c4b\u63a8\u8350\u8d28\u91cf\uff0c\u5f3a\u8c03\u4e86\u4f4d\u7f6e\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u4f4f\u623f\u4f4d\u7f6e\u5bf9\u4e8e\u623f\u4ea7\u4ef7\u503c\u3001\u4fbf\u5229\u8bbe\u65bd\u7684\u83b7\u53d6\u548c\u751f\u6d3b\u8d28\u91cf\u6709\u91cd\u5927\u5f71\u54cd\u3002\u4e3a\u4e86\u63d0\u9ad8\u63a8\u8350\u7684\u6709\u6548\u6027\uff0c\u9700\u8981\u5c06\u5730\u7406\u4f4d\u7f6e\u8003\u8651\u8fdb\u53bb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5730\u7406\u611f\u77e5\u5d4c\u5165\u6846\u67b6\uff0c\u5c06\u5206\u5c42H3\u7f51\u683c\u5d4c\u5165\u5230\u53cc\u5854\u795e\u7ecf\u7ed3\u6784\u4e2d\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u77e9\u9635\u5206\u89e3\u57fa\u7ebf\u548c\u5355\u5206\u8fa8\u7387\u53d8\u4f53\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5d4c\u5165\u6027\u7279\u5b9a\u8bc4\u4ef7\u63ed\u793a\u4e86\u66f4\u4e30\u5bcc\u548c\u5e73\u8861\u7684\u5d4c\u5165\u8868\u73b0\uff0c\u79bb\u7ebf\u6392\u540d\u6a21\u62df\u663e\u793a\u63a8\u8350\u8d28\u91cf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u5730\u7406\u611f\u77e5\u5d4c\u5165\u6846\u67b6\uff0c\u53ef\u4ee5\u6539\u5584\u6570\u5b57\u79df\u8d41\u5e73\u53f0\u7684\u623f\u5c4b\u63a8\u8350\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u758f\u6027\u548c\u7a7a\u95f4\u7279\u6027\u6311\u6218\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.01197", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01197", "abs": "https://arxiv.org/abs/2510.01197", "authors": ["Gadir Suleymanli", "Alexander Rogiers", "Lucas Lageweg", "Jefrey Lijffijt"], "title": "Are LLMs ready to help non-expert users to make charts of official statistics data?", "comment": null, "summary": "In this time when biased information, deep fakes, and propaganda proliferate,\nthe accessibility of reliable data sources is more important than ever.\nNational statistical institutes provide curated data that contain quantitative\ninformation on a wide range of topics. However, that information is typically\nspread across many tables and the plain numbers may be arduous to process.\nHence, this open data may be practically inaccessible. We ask the question \"Are\ncurrent Generative AI models capable of facilitating the identification of the\nright data and the fully-automatic creation of charts to provide information in\nvisual form, corresponding to user queries?\". We present a structured\nevaluation of recent large language models' (LLMs) capabilities to generate\ncharts from complex data in response to user queries. Working with diverse\npublic data from Statistics Netherlands, we assessed multiple LLMs on their\nability to identify relevant data tables, perform necessary manipulations, and\ngenerate appropriate visualizations autonomously. We propose a new evaluation\nframework spanning three dimensions: data retrieval & pre-processing, code\nquality, and visual representation. Results indicate that locating and\nprocessing the correct data represents the most significant challenge.\nAdditionally, LLMs rarely implement visualization best practices without\nexplicit guidance. When supplemented with information about effective chart\ndesign, models showed marked improvement in representation scores. Furthermore,\nan agentic approach with iterative self-evaluation led to excellent performance\nacross all evaluation dimensions. These findings suggest that LLMs'\neffectiveness for automated chart generation can be enhanced through\nappropriate scaffolding and feedback mechanisms, and that systems can already\nreach the necessary accuracy across the three evaluation dimensions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f53\u524d\u751f\u6210\u5f0fAI\u6a21\u578b\u662f\u5426\u80fd\u81ea\u52a8\u751f\u6210\u7528\u6237\u67e5\u8be2\u6240\u9700\u7684\u56fe\u8868\uff0c\u8bc4\u4f30\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u6570\u636e\u548c\u751f\u6210\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u7ecf\u8fc7\u9002\u5f53\u7684\u6307\u5bfc\u53ef\u663e\u8457\u63d0\u5347\u8868\u73b0\u3002", "motivation": "\u5728\u5f53\u4eca\u4fe1\u606f\u504f\u89c1\u3001\u865a\u5047\u4fe1\u606f\u548c\u5ba3\u4f20\u6cdb\u6ee5\u7684\u65f6\u4ee3\uff0c\u53ef\u9760\u6570\u636e\u6e90\u7684\u53ef\u8bbf\u95ee\u6027\u53d8\u5f97\u6bd4\u4ee5\u5f80\u4efb\u4f55\u65f6\u5019\u90fd\u91cd\u8981\u3002\u56fd\u5bb6\u7edf\u8ba1\u673a\u6784\u63d0\u4f9b\u4e86\u5305\u542b\u5e7f\u6cdb\u4e3b\u9898\u7684\u5b9a\u91cf\u4fe1\u606f\u7684\u7cbe\u9009\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u901a\u5e38\u5206\u6563\u5728\u8bb8\u591a\u8868\u683c\u4e2d\uff0c\u96be\u4ee5\u5904\u7406\u3002\u5f00\u653e\u6570\u636e\u53ef\u80fd\u5b9e\u9645\u4e0a\u4e0d\u53ef\u8bbf\u95ee\u3002", "method": "\u5bf9\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u7ed3\u6784\u5316\u8bc4\u4f30\uff0c\u4ee5\u8bc4\u4f30\u5176\u6839\u636e\u7528\u6237\u67e5\u8be2\u4ece\u590d\u6742\u6570\u636e\u4e2d\u751f\u6210\u56fe\u8868\u7684\u80fd\u529b\u3002\u91c7\u7528\u6765\u81ea\u8377\u5170\u7edf\u8ba1\u5c40\u7684\u591a\u6837\u5316\u516c\u5171\u6570\u636e\uff0c\u8bc4\u4f30\u591a\u4e2aLLM\u5728\u8bc6\u522b\u76f8\u5173\u6570\u636e\u8868\u3001\u6267\u884c\u5fc5\u8981\u64cd\u4f5c\u548c\u81ea\u4e3b\u751f\u6210\u9002\u5f53\u7684\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5b9a\u4f4d\u548c\u5904\u7406\u6b63\u786e\u6570\u636e\u662f\u6700\u4e3b\u8981\u7684\u6311\u6218\u3002\u6b64\u5916\uff0cLLMs\u5728\u6ca1\u6709\u660e\u786e\u6307\u5bfc\u65f6\u5f88\u5c11\u80fd\u5b9e\u65bd\u53ef\u89c6\u5316\u6700\u4f73\u5b9e\u8df5\u3002\u4e00\u65e6\u63d0\u4f9b\u5173\u4e8e\u6709\u6548\u56fe\u8868\u8bbe\u8ba1\u7684\u4fe1\u606f\uff0c\u6a21\u578b\u5728\u8868\u793a\u8bc4\u5206\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u5584\u3002\u8fed\u4ee3\u81ea\u8bc4\u7684\u4ee3\u7406\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LLMs\u81ea\u52a8\u751f\u6210\u56fe\u8868\u7684\u6709\u6548\u6027\u53ef\u4ee5\u901a\u8fc7\u9002\u5f53\u7684\u652f\u6491\u7ed3\u6784\u548c\u53cd\u9988\u673a\u5236\u5f97\u5230\u589e\u5f3a\uff0c\u540c\u65f6\u7cfb\u7edf\u5728\u4e09\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u5df2\u7ecf\u53ef\u4ee5\u8fbe\u5230\u5fc5\u8981\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.01198", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01198", "abs": "https://arxiv.org/abs/2510.01198", "authors": ["Matan Mandelbrod", "Biwei Jiang", "Giald Wagner", "Tal Franji", "Guy Feigenblat"], "title": "Optimal signals assignment for eBay View Item page", "comment": "Accepted at the CONSEQUENCES 2025 workshop, co-located with ACM\n  RecSys 2025", "summary": "Signals are short textual or visual snippets displayed on the eBay View-Item\n(VI) page, providing additional, contextual information for users about the\nviewed item. The aim in displaying the signals is to facilitate intelligent\npurchase and to incentivise engagement. In this paper, we present two\napproaches for developing statistical models that optimally populate the VI\npage with signals. Both approaches were A/B tested, and yielded significant\nincrease in business metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u4f18\u5316eBay\u9875\u9762\u4fe1\u53f7\u663e\u793a\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7A/B\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5bf9\u4e1a\u52a1\u63d0\u5347\u6709\u79ef\u6781\u4f5c\u7528\u3002", "motivation": "eBay\u7684VI\u9875\u9762\u901a\u8fc7\u663e\u793a\u4fe1\u53f7\u6765\u63d0\u4f9b\u989d\u5916\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u4fc3\u8fdb\u667a\u80fd\u8d2d\u4e70\u5e76\u6fc0\u52b1\u7528\u6237\u4e92\u52a8\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u5f00\u53d1\u7edf\u8ba1\u6a21\u578b\uff0c\u4ee5\u4f18\u5316VI\u9875\u9762\u7684\u4fe1\u53f7\u663e\u793a\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86A/B\u6d4b\u8bd5\u3002", "result": "A/B\u6d4b\u8bd5\u663e\u793a\uff0c\u4e24\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4e1a\u52a1\u6307\u6807\u3002", "conclusion": "\u5c55\u793a\u4fe1\u53f7\u53ef\u4ee5\u63d0\u5347\u7528\u6237\u7684\u8d2d\u4e70\u4f53\u9a8c\uff0c\u5e76\u5bf9\u4e1a\u52a1\u6307\u6807\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2510.01523", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01523", "abs": "https://arxiv.org/abs/2510.01523", "authors": ["Shreeranjani Srirangamsridharan", "Ali Abavisani", "Reza Yousefi Maragheh", "Ramin Giahi", "Kai Zhao", "Jason Cho", "Sushant Kumar"], "title": "MetaSynth: Multi-Agent Metadata Generation from Implicit Feedback in Black-Box Systems", "comment": "NeurIPS Workshop LAW", "summary": "Meta titles and descriptions strongly shape engagement in search and\nrecommendation platforms, yet optimizing them remains challenging. Search\nengine ranking models are black box environments, explicit labels are\nunavailable, and feedback such as click-through rate (CTR) arrives only\npost-deployment. Existing template, LLM, and retrieval-augmented approaches\neither lack diversity, hallucinate attributes, or ignore whether candidate\nphrasing has historically succeeded in ranking. This leaves a gap in directly\nleveraging implicit signals from observable outcomes. We introduce MetaSynth, a\nmulti-agent retrieval-augmented generation framework that learns from implicit\nsearch feedback. MetaSynth builds an exemplar library from top-ranked results,\ngenerates candidate snippets conditioned on both product content and exemplars,\nand iteratively refines outputs via evaluator-generator loops that enforce\nrelevance, promotional strength, and compliance. On both proprietary e-commerce\ndata and the Amazon Reviews corpus, MetaSynth outperforms strong baselines\nacross NDCG, MRR, and rank metrics. Large-scale A/B tests further demonstrate\n10.26% CTR and 7.51% clicks. Beyond metadata, this work contributes a general\nparadigm for optimizing content in black-box systems using implicit signals.", "AI": {"tldr": "MetaSynth\u901a\u8fc7\u9690\u6027\u4fe1\u53f7\u4f18\u5316\u6392\u540d\u8868\u73b0\uff0c\u5728\u7535\u5b50\u5546\u52a1\u548c\u4e9a\u9a6c\u900a\u8bc4\u8bba\u6570\u636e\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8CTR\u548c\u70b9\u51fb\u7387\u3002", "motivation": "\u5f53\u524d\u7684\u5143\u6570\u636e\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u591a\u6837\u6027\u3001\u53ef\u80fd\u751f\u6210\u865a\u5047\u5c5e\u6027\u6216\u5ffd\u7565\u5386\u53f2\u6210\u529f\u7684\u5019\u9009\u77ed\u8bed\uff0c\u7f3a\u4e4f\u76f4\u63a5\u5229\u7528\u89c2\u5bdf\u7ed3\u679c\u7684\u9690\u6027\u4fe1\u53f7\uff0cMetaSynth\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MetaSynth\u91c7\u7528\u591a\u4ee3\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u9690\u6027\u641c\u7d22\u53cd\u9988\u8fdb\u884c\u5b66\u4e60\u3002\u901a\u8fc7\u4ece\u9ad8\u6392\u540d\u7ed3\u679c\u4e2d\u6784\u5efa\u793a\u4f8b\u5e93\uff0c\u751f\u6210\u53d7\u4ea7\u54c1\u5185\u5bb9\u548c\u793a\u4f8b\u7ea6\u675f\u7684\u5019\u9009\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u8bc4\u4f30-\u751f\u6210\u5668\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u8f93\u51fa\u3002", "result": "MetaSynth\u5728\u4e13\u6709\u7535\u5b50\u5546\u52a1\u6570\u636e\u548c\u4e9a\u9a6c\u900a\u8bc4\u8bba\u8bed\u6599\u5e93\u4e2d\uff0c\u5728NDCG\u3001MRR\u548c\u6392\u540d\u6307\u6807\u65b9\u9762\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff1b\u5927\u89c4\u6a21A/B\u6d4b\u8bd5\u663e\u793a\u5176\u63d0\u5347\u4e8610.26%\u7684CTR\u548c7.51%\u7684\u70b9\u51fb\u7387\u3002", "conclusion": "MetaSynth\u4e0d\u4ec5\u5728\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f18\u5316\u9ed1\u7bb1\u7cfb\u7edf\u5185\u5bb9\u7684\u901a\u7528\u8303\u5f0f\u3002"}}
{"id": "2510.01553", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01553", "abs": "https://arxiv.org/abs/2510.01553", "authors": ["Zhuofan Shi", "Zijie Guo", "Xinjian Ma", "Gang Huang", "Yun Ma", "Xiang Jing"], "title": "IoDResearch: Deep Research on Private Heterogeneous Data via the Internet of Data", "comment": "8 pages,4 figures", "summary": "The rapid growth of multi-source, heterogeneous, and multimodal scientific\ndata has increasingly exposed the limitations of traditional data management.\nMost existing DeepResearch (DR) efforts focus primarily on web search while\noverlooking local private data. Consequently, these frameworks exhibit low\nretrieval efficiency for private data and fail to comply with the FAIR\nprinciples, ultimately resulting in inefficiency and limited reusability. To\nthis end, we propose IoDResearch (Internet of Data Research), a private\ndata-centric Deep Research framework that operationalizes the Internet of Data\nparadigm. IoDResearch encapsulates heterogeneous resources as FAIR-compliant\ndigital objects, and further refines them into atomic knowledge units and\nknowledge graphs, forming a heterogeneous graph index for multi-granularity\nretrieval. On top of this representation, a multi-agent system supports both\nreliable question answering and structured scientific report generation.\nFurthermore, we establish the IoD DeepResearch Benchmark to systematically\nevaluate both data representation and Deep Research capabilities in IoD\nscenarios. Experimental results on retrieval, QA, and report-writing tasks show\nthat IoDResearch consistently surpasses representative RAG and Deep Research\nbaselines. Overall, IoDResearch demonstrates the feasibility of\nprivate-data-centric Deep Research under the IoD paradigm, paving the way\ntoward more trustworthy, reusable, and automated scientific discovery.", "AI": {"tldr": "\u63d0\u51faIoDResearch\u6846\u67b6\uff0c\u89e3\u51b3\u4f20\u7edf\u6570\u636e\u7ba1\u7406\u5728\u5904\u7406\u79c1\u4eba\u6570\u636e\u65f6\u7684\u6548\u7387\u548cFAIR\u539f\u5219\u9075\u5faa\u95ee\u9898\uff0c\u5e76\u5728\u68c0\u7d22\u3001\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u7ba1\u7406\u9762\u4e34\u591a\u6e90\u3001\u5f02\u6784\u548c\u591a\u6a21\u6001\u79d1\u5b66\u6570\u636e\u7684\u9650\u5236\uff0c\u73b0\u6709\u7684\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u5728\u68c0\u7d22\u6548\u7387\u548c\u9075\u5faaFAIR\u539f\u5219\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u79c1\u4eba\u6570\u636e\u65f6\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63d0\u51faIoDResearch\u6846\u67b6\uff0c\u5c06\u5f02\u6784\u8d44\u6e90\u5c01\u88c5\u4e3a\u7b26\u5408FAIR\u539f\u5219\u7684\u6570\u5b57\u5bf9\u8c61\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ec6\u5316\u4e3a\u539f\u5b50\u77e5\u8bc6\u5355\u4f4d\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u5f62\u6210\u7528\u4e8e\u591a\u7c92\u5ea6\u68c0\u7d22\u7684\u5f02\u6784\u56fe\u7d22\u5f15\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\u652f\u6301\u53ef\u9760\u7684\u95ee\u7b54\u548c\u7ed3\u6784\u5316\u79d1\u5b66\u62a5\u544a\u751f\u6210\u3002", "result": "IoDResearch\u6846\u67b6\u5728\u68c0\u7d22\u3001\u95ee\u7b54\u548c\u62a5\u544a\u4e66\u5199\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ee3\u8868\u6027RAG\u548c\u6df1\u5ea6\u7814\u7a76\u57fa\u7ebf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728IoD\u60c5\u5883\u4e0b\u7684\u79c1\u5bc6\u6570\u636e\u4e13\u6ce8\u6df1\u5ea6\u7814\u7a76\u662f\u53ef\u884c\u7684\uff0c\u4fc3\u8fdb\u4e86\u66f4\u53ef\u4fe1\u3001\u53ef\u91cd\u7528\u548c\u81ea\u52a8\u5316\u7684\u79d1\u5b66\u53d1\u73b0\u3002", "conclusion": "IoDResearch\u6846\u67b6\u5c55\u793a\u4e86\u5728IoD\u6a21\u5f0f\u4e0b\u8fdb\u884c\u79c1\u4eba\u6570\u636e\u4e13\u6ce8\u6df1\u5ea6\u7814\u7a76\u7684\u53ef\u884c\u6027\uff0c\u63a8\u52a8\u4e86\u66f4\u53ef\u4fe1\u3001\u53ef\u91cd\u7528\u548c\u81ea\u52a8\u5316\u7684\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2510.01574", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01574", "abs": "https://arxiv.org/abs/2510.01574", "authors": ["Adithya Rajan", "Xiaoyu Liu", "Prateek Verma", "Vibhu Arora"], "title": "Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query Autocomplete", "comment": "Accepted to the Proceedings of the ACM SIGIR Asia Pacific Conference\n  on Information Retrieval (SIGIR-AP 2025), December 7-10, 2025, Xi'an, China", "summary": "We introduce a data-centric approach for mitigating presentation bias in\nreal-time neural query autocomplete systems through the use of synthetic\nprefixes. These prefixes are generated from complete user queries collected\nduring regular search sessions where autocomplete was not active. This allows\nus to enrich the training data for learning to rank models with more diverse\nand less biased examples. This method addresses the inherent bias in engagement\nsignals collected from live query autocomplete interactions, where model\nsuggestions influence user behavior. Our neural ranker is optimized for\nreal-time deployment under strict latency constraints and incorporates a rich\nset of features, including query popularity, seasonality, fuzzy match scores,\nand contextual signals such as department affinity, device type, and vertical\nalignment with previous user queries. To support efficient training, we\nintroduce a task-specific simplification of the listwise loss, reducing\ncomputational complexity from $O(n^2)$ to $O(n)$ by leveraging the query\nautocomplete structure of having only one ground-truth selection per prefix.\nDeployed in a large-scale e-commerce setting, our system demonstrates\nstatistically significant improvements in user engagement, as measured by mean\nreciprocal rank and related metrics. Our findings show that synthetic prefixes\nnot only improve generalization but also provide a scalable path toward bias\nmitigation in other low-latency ranking tasks, including related searches and\nquery recommendations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u5408\u6210\u524d\u7f00\u7684\u6570\u636e\u4e2d\u5fc3\u65b9\u6cd5\u6765\u6539\u5584\u5b9e\u65f6\u67e5\u8be2\u81ea\u52a8\u5b8c\u6210\u7cfb\u7edf\u4e2d\u7684\u504f\u5dee\uff0c\u5e76\u5728\u7535\u5b50\u5546\u52a1\u73af\u5883\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u67e5\u8be2\u81ea\u52a8\u5b8c\u6210\u4e92\u52a8\u4e2d\u7684\u56fa\u6709\u504f\u5dee\uff0c\u4f18\u5316\u795e\u7ecf\u6392\u5e8f\u5668\u4ee5\u5728\u4e25\u683c\u7684\u5ef6\u8fdf\u9650\u5236\u4e0b\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u751f\u6210\u6765\u81ea\u5b8c\u6574\u7528\u6237\u67e5\u8be2\u7684\u5408\u6210\u524d\u7f00\u6765\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\u3002\u4e3a\u9ad8\u6548\u8bad\u7ec3\uff0c\u6211\u4eec\u7b80\u5316\u4e86\u5217\u8868\u635f\u5931\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u7535\u5b50\u5546\u52a1\u73af\u5883\u4e2d\u90e8\u7f72\u540e\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u5728\u7528\u6237\u53c2\u4e0e\u5ea6\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6570\u636e\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u524d\u7f00\u6765\u51cf\u8f7b\u5b9e\u65f6\u795e\u7ecf\u67e5\u8be2\u81ea\u52a8\u5b8c\u6210\u7cfb\u7edf\u4e2d\u7684\u5c55\u793a\u504f\u5dee\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u51cf\u5c11\u504f\u89c1\uff0c\u4ece\u800c\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2510.01606", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01606", "abs": "https://arxiv.org/abs/2510.01606", "authors": ["Bo Ma", "LuYao Liu", "Simon Lau", "Chandler Yuan", "and XueY Cui", "Rosie Zhang"], "title": "Bridging Collaborative Filtering and Large Language Models with Dynamic Alignment, Multimodal Fusion and Evidence-grounded Explanations", "comment": null, "summary": "Recent research has explored using Large Language Models for recommendation\ntasks by transforming user interaction histories and item metadata into text\nprompts, then having the LLM produce rankings or recommendations. A promising\napproach involves connecting collaborative filtering knowledge to LLM\nrepresentations through compact adapter networks, which avoids expensive\nfine-tuning while preserving the strengths of both components. Yet several\nchallenges persist in practice: collaborative filtering models often use static\nsnapshots that miss rapidly changing user preferences; many real-world items\ncontain rich visual and audio content beyond textual descriptions; and current\nsystems struggle to provide trustworthy explanations backed by concrete\nevidence. Our work introduces \\model{}, a framework that tackles these\nlimitations through three key innovations. We develop an online adaptation\nmechanism that continuously incorporates new user interactions through\nlightweight modules, avoiding the need to retrain large models. We create a\nunified representation that seamlessly combines collaborative signals with\nvisual and audio features, handling cases where some modalities may be\nunavailable. Finally, we design an explanation system that grounds\nrecommendations in specific collaborative patterns and item attributes,\nproducing natural language rationales users can verify. Our approach maintains\nthe efficiency of frozen base models while adding minimal computational\noverhead, making it practical for real-world deployment.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u534f\u4f5c\u8fc7\u6ee4\u77e5\u8bc6\u89e3\u51b3\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u4f7f\u7528\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u65e0\u6cd5\u5e94\u5bf9\u5feb\u901f\u53d8\u5316\u7684\u7528\u6237\u504f\u597d\uff0c\u7f3a\u4e4f\u5bf9\u5bcc\u5a92\u4f53\u5185\u5bb9\u7684\u5904\u7406\u80fd\u529b\uff0c\u4ee5\u53ca\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5728\u7ebf\u9002\u5e94\u673a\u5236\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u4e0d\u65ad\u6574\u5408\u65b0\u7684\u7528\u6237\u4ea4\u4e92\uff1b\u521b\u5efa\u4e86\u4e00\u79cd\u7edf\u4e00\u8868\u793a\uff0c\u7ed3\u5408\u534f\u4f5c\u4fe1\u53f7\u4e0e\u89c6\u89c9\u548c\u97f3\u9891\u7279\u5f81\uff1b\u8bbe\u8ba1\u4e86\u4e00\u79cd\u89e3\u91ca\u7cfb\u7edf\uff0c\u63d0\u4f9b\u5177\u4f53\u7684\u534f\u4f5c\u6a21\u5f0f\u548c\u9879\u76ee\u5c5e\u6027\u4f5c\u4e3a\u63a8\u8350\u7684\u57fa\u7840\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u4fdd\u6301\u5927\u6a21\u578b\u7684\u6548\u7387\uff0c\u589e\u52a0\u8f83\u5c11\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\\model{}\uff0c\u901a\u8fc7\u4e09\u9879\u5173\u952e\u521b\u65b0\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.01622", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01622", "abs": "https://arxiv.org/abs/2510.01622", "authors": ["Bo Ma", "Hang Li", "ZeHua Hu", "XiaoFan Gui", "LuYao Liu", "Simon Lau"], "title": "LLM4Rec: Large Language Models for Multimodal Generative Recommendation with Causal Debiasing", "comment": null, "summary": "Contemporary generative recommendation systems face significant challenges in\nhandling multimodal data, eliminating algorithmic biases, and providing\ntransparent decision-making processes. This paper introduces an enhanced\ngenerative recommendation framework that addresses these limitations through\nfive key innovations: multimodal fusion architecture, retrieval-augmented\ngeneration mechanisms, causal inference-based debiasing, explainable\nrecommendation generation, and real-time adaptive learning capabilities. Our\nframework leverages advanced large language models as the backbone while\nincorporating specialized modules for cross-modal understanding, contextual\nknowledge integration, bias mitigation, explanation synthesis, and continuous\nmodel adaptation. Extensive experiments on three benchmark datasets\n(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent\nimprovements in recommendation accuracy, fairness, and diversity compared to\nexisting approaches. The proposed framework achieves up to 2.3% improvement in\nNDCG@10 and 1.4% enhancement in diversity metrics while maintaining\ncomputational efficiency through optimized inference strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u751f\u6210\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u673a\u5236\u3001\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u53bb\u504f\u4ee5\u53ca\u53ef\u89e3\u91ca\u63a8\u8350\u751f\u6210\u548c\u5b9e\u65f6\u81ea\u9002\u5e94\u5b66\u4e60\u80fd\u529b\u89e3\u51b3\u4e86\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u3001\u6d88\u9664\u7b97\u6cd5\u504f\u5dee\u548c\u63d0\u4f9b\u900f\u660e\u51b3\u7b56\u8fc7\u7a0b\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u673a\u5236\u3001\u56e0\u679c\u63a8\u7406\u53bb\u504f\u3001\u89e3\u91ca\u6027\u63a8\u8350\u751f\u6210\u548c\u5b9e\u65f6\u81ea\u9002\u5e94\u5b66\u4e60\u80fd\u529b\u8fdb\u884c\u589e\u5f3a\uff0c\u5e76\u5229\u7528\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u7ed3\u5408\u6a21\u5757\u5316\u7684\u8de8\u6a21\u6001\u7406\u89e3\u3001\u4e0a\u4e0b\u6587\u77e5\u8bc6\u6574\u5408\u3001\u504f\u5dee\u51cf\u7f13\u3001\u89e3\u91ca\u5408\u6210\u53ca\u6301\u7eed\u6a21\u578b\u9002\u5e94\u7b49\u529f\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08MovieLens-25M\u3001Amazon-Electronics\u3001Yelp-2023\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5728\u63a8\u8350\u51c6\u786e\u6027\u3001\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u5747\u6709\u6301\u7eed\u6539\u5584\uff0c\u5728NDCG@10\u6307\u6807\u4e0a\u63d0\u9ad8\u4e86\u6700\u9ad82.3%\uff0c\u591a\u6837\u6027\u6307\u6807\u4e0a\u63d0\u9ad8\u4e861.4%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3001\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u7b56\u7565\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.01698", "categories": ["cs.IR", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01698", "abs": "https://arxiv.org/abs/2510.01698", "authors": ["Seungheon Doh", "Keunwoo Choi", "Juhan Nam"], "title": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling", "comment": "Accepted for publication at The Workshop on AI for Music, Neural\n  Information Processing Systems (NeurIPS-AI4Music)", "summary": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u4e50\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u8c03\u7528\u591a\u79cd\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u97f3\u4e50\u63a8\u8350\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u5df2\u6709\u8f83\u5927\u8fdb\u5c55\uff0c\u4f46\u63a8\u8350\u884c\u4e3a\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5143\u6570\u636e\u6216\u5c5e\u6027\u8fc7\u6ee4\u65b9\u9762\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\u3002\u4e3a\u4e86\u63d0\u5347\u8fd9\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u7cfb\u7edf\u3002", "method": "\u8be5\u7cfb\u7edf\u5c06LLM\u4f5c\u4e3a\u7aef\u5230\u7aef\u63a8\u8350\u7cfb\u7edf\uff0c\u80fd\u591f\u89e3\u6790\u7528\u6237\u610f\u56fe\uff0c\u89c4\u5212\u5de5\u5177\u8c03\u7528\uff0c\u5e76\u534f\u8c03\u5e03\u5c14\u8fc7\u6ee4(SQL)\u3001\u7a00\u758f\u68c0\u7d22(BM25)\u3001\u7a20\u5bc6\u68c0\u7d22(\u5d4c\u5165\u76f8\u4f3c\u6027)\u548c\u751f\u6210\u68c0\u7d22(\u8bed\u4e49ID)\u8fd9\u51e0\u79cd\u4e13\u95e8\u7ec4\u4ef6\u3002\u901a\u8fc7\u5de5\u5177\u89c4\u5212\uff0c\u7cfb\u7edf\u9884\u6d4b\u4f7f\u7528\u7684\u5de5\u5177\u7c7b\u578b\u3001\u6267\u884c\u987a\u5e8f\u53ca\u53c2\u6570\u4ee5\u4fbf\u627e\u5230\u7b26\u5408\u7528\u6237\u504f\u597d\u7684\u97f3\u4e50\u3002", "result": "\u901a\u8fc7\u8be5\u7cfb\u7edf\u7684\u5de5\u5177\u8c03\u7528\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6839\u636e\u7528\u6237\u67e5\u8be2\u9009\u62e9\u9002\u5f53\u68c0\u7d22\u65b9\u6cd5\uff0c\u8fdb\u800c\u5728\u4e0d\u540c\u63a8\u8350\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u4f1a\u8bdd\u97f3\u4e50\u63a8\u8350\u7cfb\u7edf\u8303\u5f0f\uff0c\u901a\u8fc7\u6709\u673a\u7ed3\u5408\u591a\u79cd\u6570\u636e\u5e93\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u6837\u7684\u6570\u636e\u6a21\u5f0f\uff0c\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.01871", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01871", "abs": "https://arxiv.org/abs/2510.01871", "authors": ["Oscar Villemaud", "Suryanarayana Sankagiri", "Matthias Grossglauser"], "title": "Ranking Items from Discrete Ratings: The Cost of Unknown User Thresholds", "comment": "12 pages, 4 figures", "summary": "Ranking items is a central task in many information retrieval and recommender\nsystems. User input for the ranking task often comes in the form of ratings on\na coarse discrete scale. We ask whether it is possible to recover a\nfine-grained item ranking from such coarse-grained ratings. We model items as\nhaving scores and users as having thresholds; a user rates an item positively\nif the item's score exceeds the user's threshold. Although all users agree on\nthe total item order, estimating that order is challenging when both the scores\nand the thresholds are latent. Under our model, any ranking method naturally\npartitions the $n$ items into bins; the bins are ordered, but the items inside\neach bin are still unordered. Users arrive sequentially, and every new user can\nbe queried to refine the current ranking. We prove that achieving a\nnear-perfect ranking, measured by Spearman distance, requires $\\Theta(n^2)$\nusers (and therefore $\\Omega(n^2)$ queries). This is significantly worse than\nthe $O(n\\log n)$ queries needed to rank from comparisons; the gap reflects the\nadditional queries needed to identify the users who have the appropriate\nthresholds. Our bound also quantifies the impact of a mismatch between score\nand threshold distributions via a quadratic divergence factor. To show the\ntightness of our results, we provide a ranking algorithm whose query complexity\nmatches our bound up to a logarithmic factor. Our work reveals a tension in\nonline ranking: diversity in thresholds is necessary to merge coarse ratings\nfrom many users into a fine-grained ranking, but this diversity has a cost if\nthe thresholds are a priori unknown.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u4ece\u7c97\u7c92\u5ea6\u8bc4\u5206\u4e2d\u6062\u590d\u7ec6\u7c92\u5ea6\u9879\u76ee\u6392\u540d\u3002\u6211\u4eec\u5c06\u9879\u76ee\u5efa\u6a21\u4e3a\u5206\u6570\uff0c\u5c06\u7528\u6237\u5efa\u6a21\u4e3a\u9608\u503c\uff0c\u5e76\u7814\u7a76\u7528\u6237\u5bf9\u9879\u76ee\u8bc4\u5206\u7684\u673a\u5236\u3002\u6211\u4eec\u8bc1\u660e\u5b9e\u73b0\u63a5\u8fd1\u5b8c\u7f8e\u6392\u540d\u9700\u8981 $\\Theta(n^2)$ \u7528\u6237\uff0c\u8fd9\u6bd4\u901a\u8fc7\u6bd4\u8f83\u83b7\u5f97\u6392\u540d\u6240\u9700\u7684 $O(n\\log n)$ \u67e5\u8be2\u8981\u7cdf\u7cd5\u5f97\u591a\u3002\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5728\u7ebf\u6392\u540d\u4e2d\u7684\u9608\u503c\u591a\u6837\u6027\u4e0e\u6392\u540d\u7cbe\u7ec6\u5ea6\u4e4b\u95f4\u7684\u7d27\u5f20\u5173\u7cfb\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u4ece\u7528\u6237\u7684\u7c97\u7c92\u5ea6\u8bc4\u5206\u4e2d\u6062\u590d\u9879\u76ee\u7684\u7ec6\u7c92\u5ea6\u6392\u540d\uff0c\u4ece\u800c\u63d0\u9ad8\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u7684\u6392\u540d\u51c6\u786e\u6027\u3002", "method": "\u6211\u4eec\u6a21\u578b\u5316\u9879\u76ee\u548c\u7528\u6237\u884c\u4e3a\uff0c\u5229\u7528\u7528\u6237\u5230\u6765\u7684\u987a\u5e8f\uff0c\u901a\u8fc7\u8be2\u95ee\u65b0\u7528\u6237\u6765\u7ec6\u5316\u5f53\u524d\u6392\u540d\u3002\u6211\u4eec\u8bc1\u660e\u9700\u8981 $\\Theta(n^2)$ \u7684\u7528\u6237\u6765\u5b9e\u73b0\u51e0\u4e4e\u5b8c\u7f8e\u7684\u6392\u540d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u7406\u8bba\u754c\u9650\u4e00\u81f4\u7684\u6392\u540d\u7b97\u6cd5\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u7ed3\u679c\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4ece\u7c97\u7c92\u5ea6\u8bc4\u5206\u6062\u590d\u7ec6\u7c92\u5ea6\u6392\u540d\u9700\u8981 $\\Theta(n^2)$ \u7684\u7528\u6237\uff0c\u4e14\u63d0\u4f9b\u4e86\u4e00\u4e2a\u67e5\u8be2\u590d\u6742\u5ea6\u5339\u914d\u7406\u8bba\u754c\u9650\u7684\u7b97\u6cd5\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5728\u9608\u503c\u5206\u5e03\u4e0d\u5339\u914d\u65f6\uff0c\u901a\u8fc7\u4e8c\u6b21\u5dee\u5f02\u56e0\u5b50\u91cf\u5316\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5728\u7ebf\u6392\u540d\u4e2d\u7684\u9608\u503c\u591a\u6837\u6027\u4e0e\u6392\u540d\u7cbe\u7ec6\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002\u4e3a\u4e86\u5c06\u7c97\u7c92\u5ea6\u8bc4\u5206\u6574\u5408\u4e3a\u7ec6\u7c92\u5ea6\u6392\u540d\uff0c\u9700\u8981\u9608\u503c\u7684\u591a\u6837\u6027\uff0c\u4f46\u5982\u679c\u9608\u503c\u672a\u77e5\u8fd9\u4f1a\u5bfc\u81f4\u989d\u5916\u7684\u6210\u672c\u3002\u6211\u4eec\u7684\u6392\u540d\u7b97\u6cd5\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u6211\u4eec\u7684\u7406\u8bba\u754c\u9650\u5339\u914d\uff0c\u5dee\u8ddd\u4ec5\u4e3a\u5bf9\u6570\u56e0\u5b50\u3002"}}
{"id": "2510.02219", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.02219", "abs": "https://arxiv.org/abs/2510.02219", "authors": ["Linh Tran", "Yulong Li", "Radu Florian", "Wei Sun"], "title": "Contrastive Retrieval Heads Improve Attention-Based Re-Ranking", "comment": null, "summary": "The strong zero-shot and long-context capabilities of recent Large Language\nModels (LLMs) have paved the way for highly effective re-ranking systems.\nAttention-based re-rankers leverage attention weights from transformer heads to\nproduce relevance scores, but not all heads are created equally: many\ncontribute noise and redundancy, thus limiting performance. To address this, we\nintroduce CoRe heads, a small set of retrieval heads identified via a\ncontrastive scoring metric that explicitly rewards high attention heads that\ncorrelate with relevant documents, while downplaying nodes with higher\nattention that correlate with irrelevant documents. This relative ranking\ncriterion isolates the most discriminative heads for re-ranking and yields a\nstate-of-the-art list-wise re-ranker. Extensive experiments with three LLMs\nshow that aggregated signals from CoRe heads, constituting less than 1% of all\nheads, substantially improve re-ranking accuracy over strong baselines. We\nfurther find that CoRe heads are concentrated in middle layers, and pruning the\ncomputation of final 50% of model layers preserves accuracy while significantly\nreducing inference time and memory usage.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aCoRe\u7684\u68c0\u7d22\u5934\uff0c\u4f7f\u5f97\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u65b0\u6392\u5e8f\u4e2d\u53d6\u5f97\u66f4\u597d\u7684\u6548\u679c\u3002\u901a\u8fc7\u5bf9\u6bd4\u5f97\u5206\u77e9\u9635\u6765\u8bc6\u522b\u8fd9\u4e9b\u68c0\u7d22\u5934\uff0c\u4ece\u800c\u63d0\u9ad8\u76f8\u5173\u6587\u6863\u7684\u6ce8\u610f\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u4e0e\u4e0d\u76f8\u5173\u6587\u6863\u76f8\u5173\u7684\u8282\u70b9\u7684\u5173\u6ce8\uff0c\u6700\u7ec8\u63d0\u9ad8\u6392\u5e8f\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\u5728\u91cd\u65b0\u6392\u5e8f\u4e2d\u7684\u8868\u73b0\u53d7\u4e00\u4e9b\u4f4e\u6548\u7684\u6ce8\u610f\u529b\u5934\u9650\u5236\uff0c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4e00\u79cd\u5bf9\u6bd4\u8bc4\u5206\u77e9\u9635\u8bc6\u522b\u51fa\u4e00\u7ec4\u68c0\u7d22\u5934\uff0c\u9ad8\u5ea6\u5173\u6ce8\u76f8\u5173\u6587\u6863\uff0c\u540c\u65f6\u51cf\u5c11\u4e0d\u76f8\u5173\u6587\u6863\u7684\u5173\u6ce8\u3002", "result": "CoRe\u68c0\u7d22\u5934\u4ec5\u5360\u6ce8\u610f\u529b\u5934\u76841%\u5374\u663e\u7740\u63d0\u9ad8\u6392\u5e8f\u51c6\u786e\u6027\uff0c\u4e14\u96c6\u4e2d\u4e8e\u4e2d\u95f4\u5c42\uff0c\u51cf\u5c11\u6700\u540e50%\u5c42\u7684\u8ba1\u7b97\u91cf\u53ef\u4fdd\u6301\u51c6\u786e\u6027\u540c\u65f6\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "\u4f7f\u7528CoRe\u68c0\u7d22\u5934\u80fd\u591f\u6709\u6548\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u65b0\u6392\u5e8f\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u8017\u65f6\u548c\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2510.02241", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02241", "abs": "https://arxiv.org/abs/2510.02241", "authors": ["Daniel Gwon", "Nour Jedidi", "Jimmy Lin"], "title": "Study on LLMs for Promptagator-Style Dense Retriever Training", "comment": "CIKM 2025 short research paper", "summary": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot\nprompts can be used as task-specific query generators for fine-tuning\ndomain-specialized dense retrieval models. However, the original Promptagator\napproach relied on proprietary and large-scale LLMs which users may not have\naccess to or may be prohibited from using with sensitive data. In this work, we\nstudy the impact of open-source LLMs at accessible scales ($\\leq$14B\nparameters) as an alternative. Our results demonstrate that open-source LLMs as\nsmall as 3B parameters can serve as effective Promptagator-style query\ngenerators. We hope our work will inform practitioners with reliable\nalternatives for synthetic data generation and give insights to maximize\nfine-tuning results for domain-specific applications.", "AI": {"tldr": "\u5f00\u653e\u6e90\u7801\u7684\u5c0f\u578bLLM\u53ef\u4ee5\u66ff\u4ee3\u5927\u89c4\u6a21LLM\uff0c\u7528\u4e8e\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u67e5\u8be2\uff0c\u5e76\u6709\u6548\u8fdb\u884c\u9886\u57df\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u5fae\u8c03\u3002", "motivation": "\u539f\u6765\u7684Promptagator\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e13\u6709\u7684\u5927\u89c4\u6a21LLM\uff0c\u8fd9\u4e9b\u6a21\u578b\u7528\u6237\u53ef\u80fd\u65e0\u6cd5\u8bbf\u95ee\u6216\u7981\u6b62\u4e0e\u654f\u611f\u6570\u636e\u4e00\u8d77\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5f00\u653e\u6e90\u7801\u7684LLM\uff08\u53c2\u6570\u6570\u91cf\u4e0d\u8d85\u8fc714B\uff09\u4f5c\u4e3a\u4efb\u52a1\u7279\u5b9a\u7684\u67e5\u8be2\u751f\u6210\u5668\uff0c\u5bf9\u9886\u57df\u4e13\u7528\u7684\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5f00\u653e\u6e90\u7801\u7684LLM\uff0c\u5373\u4f7f\u53ea\u67093B\u53c2\u6570\uff0c\u4e5f\u80fd\u591f\u4f5c\u4e3a\u6709\u6548\u7684Promptagator\u98ce\u683c\u67e5\u8be2\u751f\u6210\u5668\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u9886\u57df\u7279\u5b9a\u5e94\u7528\u6700\u5927\u5316\u5fae\u8c03\u7ed3\u679c\u7684\u89c1\u89e3\u3002"}}
