<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Understand your Users, An Ensemble Learning Framework for Natural Noise Filtering in Recommender Systems](https://arxiv.org/abs/2509.18560)
*Clarita Hawat,Wissam Al Jurdi,Jacques Bou Abdo,Jacques Demerjian,Abdallah Makhoul*

Main category: cs.IR

TL;DR: 本文提出了一种新框架，通过对不同类型的噪声进行模块化处理，以提高推荐系统的准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 由于用户偏好和行为的变化以及这些变化背后噪声的存在，推荐系统的准确性面临挑战。

Method: 框架由三个层次组成：已知的自然噪声算法用于项目分类，集成学习模型用于项目的细化评估，以及基于签名的噪声识别。

Result: 所提出的框架能够改善推荐系统的训练数据集，增加用户满意度和参与度。

Conclusion: 提出了一个模块化框架，通过识别和过滤噪声，提高推荐系统的效果和用户满意度。

Abstract: The exponential growth of web content is a major key to the success for
Recommender Systems. This paper addresses the challenge of defining noise,
which is inherently related to variability in human preferences and behaviors.
In classifying changes in user tendencies, we distinguish three kinds of
phenomena: external factors that directly influence users' sentiment,
serendipity causing unexpected preference, and incidental interaction perceived
as noise. To overcome these problems, we present a new framework that
identifies noisy ratings. In this context, the proposed framework is modular,
consisting of three layers: known natural noise algorithms for item
classification, an Ensemble learning model for refined evaluation of the items
and signature-based noise identification. We further advocate the metrics that
quantitatively assess serendipity and group validation, offering higher
robustness in recommendation accuracy. Our approach aims to provide a cleaner
training dataset that would inherently improve user satisfaction and engagement
with Recommender Systems.

</details>


### [2] [The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking](https://arxiv.org/abs/2509.18575)
*Yaoyao Qian,Yifan Zeng,Yuchao Jiang,Chelsi Jain,Huazheng Wang*

Main category: cs.IR

TL;DR: 研究揭示了LLM在信息检索中的"排名盲点"，并探索了可能被内容提供者利用的攻击方法。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在多文档比较任务中的表现及其弱点，以识别排名过程中的潜在安全隐患。

Method: 采用了两种攻击手法：决策目标劫持和决策标准劫持，以模拟内容提供者如何影响LLM排名系统。

Result: 实验证明这些攻击方法对各种LLM有效，并揭示出更强的LLM更易受到影响。

Conclusion: 更强大的LLM在面临排名相关的攻击时表现出更高的脆弱性，需要注意相关安全问题。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot

</details>


### [3] [Agentic AutoSurvey: Let LLMs Survey LLMs](https://arxiv.org/abs/2509.18661)
*Yixin Liu,Yonghui Wu,Denghui Zhang,Lichao Sun*

Main category: cs.IR

TL;DR: 提出多智能体框架Agentic AutoSurvey，以提高自动文献综述生成的质量，在六个研究主题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 应对科学文献的指数增长对研究人员跨快速发展的领域综合知识的挑战。

Method: 采用多智能体框架，其中包括论文搜索专家、主题挖掘与聚类、学术综述撰写员和质量评估员，通过协调来生成全面的文献综述。

Result: 在六个代表性的LLM研究主题上进行实验，证明多智能体方法比现有基准显著提高，得分从AutoSurvey的4.77/10上升到8.18/10。

Conclusion: 多智能体架构能够在快速发展的科学领域中实现自动文献综述生成的显著进步。

Abstract: The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.

</details>


### [4] [Robust Denoising Neural Reranker for Recommender Systems](https://arxiv.org/abs/2509.18736)
*Wenyu Mao,Shuchang Liu,Hailan Yang,Xiaobei Wang,Xiaoyu Yang,Xu Gao,Xiang Li,Lantao Hu,Han Li,Kun Gai,An Zhang,Xiang Wang*

Main category: cs.IR

TL;DR: 提出了一种对抗性降噪重排序框架DNR，通过优化检索评分的降噪过程来提高推荐系统的性能，在三个数据集上的实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前工业界多阶段推荐系统中，经过检索模块排序的相关物品列表通常由一个较慢但更复杂的深度重排序模型进一步优化。然而，重排序模型对上一阶段检索评分的重要性探索有限，文章意在解决此问题并改进评分利用效率。

Method: 文章采用了两个主要的方法。首先，理论分析了直接使用检索评分作为重排序输入的局限性，并提出重排序任务本质上是一个从检索评分中降噪的问题。其次，提出了一个对抗性框架DNR，包含设计精巧的噪声生成模块，用于与降噪重排序器关联。框架中扩展了传统的评分误差最小化项，并加入了三个增强目标：1）降噪目标，旨在将有噪声的检索评分与用户反馈进行对齐；2）对抗性的检索评分生成目标，以改进检索评分空间中的探索；3）分布正则化项，旨在将生成的有噪声检索评分的分布与真实评分对齐。

Result: 通过在三个公开数据集上的广泛实验，以及分析性支持，验证了所提出的DNR框架的有效性。

Conclusion: 文章通过理论分析和实际实验，证明了对抗性框架DNR能够有效提高检索评分的利用率和推荐系统的整体性能。

Abstract: For multi-stage recommenders in industry, a user request would first trigger
a simple and efficient retriever module that selects and ranks a list of
relevant items, then calls a slower but more sophisticated deep reranking model
that refines the item arrangement before exposure to the user. The latter model
typically reranks the item list conditioned on the user's history content and
the initial ranking from retrievers. Although this two-stage retrieval-ranking
framework demonstrates practical effectiveness, the significance of retriever
scores from the previous stage has been limitedly explored, which is
informative. In this work, we first theoretically analyze the limitations of
using retriever scores as the rerankers' input directly and argue that the
reranking task is essentially a noise reduction problem from the retriever
scores. Following this notion, we derive an adversarial framework, DNR, that
associates the denoising reranker with a carefully designed noise generation
module. We extend the conventional score error minimization term with three
augmented objectives, including: 1) a denoising objective that aims to denoise
the noisy retriever scores to align with the user feedback; 2) an adversarial
retriever score generation objective that improves the exploration in the
retriever score space; and 3) a distribution regularization term that aims to
align the distribution of generated noisy retriever scores with the real ones.
Extensive experiments are conducted on three public datasets, together with
analytical support, validating the effectiveness of the proposed DNR.

</details>


### [5] [Single-Branch Network Architectures to Close the Modality Gap in Multimodal Recommendation](https://arxiv.org/abs/2509.18807)
*Christian Ganhör,Marta Moscati,Anna Hausberger,Shah Nawaz,Markus Schedl*

Main category: cs.IR

TL;DR: 研究了一种单分支神经网络，以解决推荐系统中模态缺失导致的推荐质量下降问题，实验结果显示该方法在缺失模态情境下表现优于多分支网络。


<details>
  <summary>Details</summary>
Motivation: 在冷启动及缺少交互历史时，推荐质量下降。需要一种方法来克服模态缺失对推荐质量的影响。

Method: 通过单分支神经网络，利用权重共享、模态采样和对比损失来提供精确推荐，从而缩小模态差距。

Result: 单分支网络在多个数据集上进行对比实验显示，单分支网络在缺失模态情况下性能更优，并增加了项目模态在嵌入空间的接近程度。

Conclusion: 单分支网络在变暖启动情景下表现具有竞争力，并在缺失模态设置中表现显著更佳。

Abstract: Traditional recommender systems rely on collaborative filtering, using past
user-item interactions to help users discover new items in a vast collection.
In cold start, i.e., when interaction histories of users or items are not
available, content-based recommender systems use side information instead.
Hybrid recommender systems (HRSs) often employ multimodal learning to combine
collaborative and side information, which we jointly refer to as modalities.
Though HRSs can provide recommendations when some modalities are missing, their
quality degrades. In this work, we utilize single-branch neural networks
equipped with weight sharing, modality sampling, and contrastive loss to
provide accurate recommendations even in missing modality scenarios by
narrowing the modality gap. We compare these networks with multi-branch
alternatives and conduct extensive experiments on three datasets. Six
accuracy-based and four beyond-accuracy-based metrics help assess the
recommendation quality for the different training paradigms and their
hyperparameters in warm-start and missing modality scenarios. We quantitatively
and qualitatively study the effects of these different aspects on bridging the
modality gap. Our results show that single-branch networks achieve competitive
performance in warm-start scenarios and are significantly better in missing
modality settings. Moreover, our approach leads to closer proximity of an
item's modalities in the embedding space. Our full experimental setup is
available at https://github.com/hcai-mms/single-branch-networks.

</details>


### [6] [RELATE: Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints](https://arxiv.org/abs/2509.19057)
*Olawumi Olasunkanmi,Mathew Satursky,Hong Yi,Chris Bizon,Harlin Lee,Stanley Ahalt*

Main category: cs.IR

TL;DR: RELATE通过三阶段管道将LLM提取的生物医学关系映射到标准化本体，显著提升了知识图谱的质量和整合度。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱对于药物发现和临床决策支持至关重要，但仍不完整。尽管大型语言模型在提取生物医学关系上表现优异，其输出缺乏标准化和与本体的对齐，从而限制了知识图谱的整合。

Method: 介绍了一种名为RELATE的三阶段管道，利用ChemProt和Biolink Model将从LLM提取的关系映射到标准化本体谓词。

Result: 在ChemProt基准上，RELATE实现了52%的严格匹配和94%的准确性@10，并在2,400篇HEAL项目摘要中有效拒绝了不相关的关联（0.4%）并识别出被否定的断言。

Conclusion: 通过将向量搜索与上下文LLM推理相结合，RELATE提供了一种可扩展、语义准确的框架，用于将非结构化生物医学文献转换为标准化知识图谱。

Abstract: Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical
decision support but remain incomplete. Large language models (LLMs) excel at
extracting biomedical relations, yet their outputs lack standardization and
alignment with ontologies, limiting KG integration. We introduce RELATE, a
three-stage pipeline that maps LLM-extracted relations to standardized ontology
predicates using ChemProt and the Biolink Model. The pipeline includes: (1)
ontology preprocessing with predicate embeddings, (2) similarity-based
retrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit
negation handling. This approach transforms relation extraction from free-text
outputs to structured, ontology-constrained representations. On the ChemProt
benchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400
HEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)
and identifies negated assertions. RELATE captures nuanced biomedical
relationships while ensuring quality for KG augmentation. By combining vector
search with contextual LLM reasoning, RELATE provides a scalable, semantically
accurate framework for converting unstructured biomedical literature into
standardized KGs.

</details>


### [7] [A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent](https://arxiv.org/abs/2509.19209)
*Olalekan K. Akindele,Bhupesh Kumar Mishra,Kenneth Y. Wertheim*

Main category: cs.IR

TL;DR: 该研究提出了一种RAG聊天机器人，通过知识图谱和向量搜索提供准确的响应，并引入RAG-Eval框架进行有效性评估。


<details>
  <summary>Details</summary>
Motivation: 提升聊天机器人的领域特定准确性，避免事实不一致，尤其是在处理大型数据集时的挑战。

Method: 设计一个结合知识图谱和向量搜索检索的检索增强生成（RAG）聊天机器人，用于回应工程项目相关邮件中的查询，减少文档分块需求。引入RAG-Eval评估框架以对RAG应用进行全面评估。

Result: RAG-Eval在评估任务中的有效性被实验验证，能够可靠检测事实差距和查询不匹配，提高数据中心化环境中的信任度。

Conclusion: 研究展示了一条开发准确、可用户验证的聊天机器人的可扩展路径，成功连接高水平对话流畅性和事实准确性之间的差距。

Abstract: Large Language Models (LLMs) have significantly enhanced conversational
Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the
avoidance of factual inconsistencies remain pressing challenges, particularly
for large datasets. Designing an effective chatbot with appropriate methods and
evaluating its effectiveness is among the challenges in this domain. This study
presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a
knowledge graph and vector search retrieval to deliver precise, context-rich
responses in an exemplary use case from over high-volume engineering
project-related emails, thereby minimising the need for document chunking. A
central innovation of this work is the introduction of RAG Evaluation
(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework
specifically developed to assess RAG applications. This framework operates in
parallel with the chatbot, jointly assessing the user's query, the retrieved
document, and the generated response, enabling a holistic evaluation across
multiple quality metrics like query relevance, factual accuracy, coverage,
coherence and fluency. The resulting scoring system is provided directly to
users as a confidence score (1 to 100%), enabling quick identification of
possible misaligned or incomplete answers. This proposed approach promotes
transparency and rapid verification by incorporating metadata email IDs,
timestamps into responses. Experimental comparisons against BERTScore and
G-EVAL for summarisation evaluation tasks confirm its effectiveness, and
empirical analysis also shows RAG-Eval reliably detects factual gaps and query
mismatches, thereby fostering trust in high demand, data centric environments.
These findings highlight a scalable path for developing accurate,
user-verifiable chatbots that bridge the gap between high-level conversational
fluency and factual accuracy.

</details>
