{"id": "2510.00137", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00137", "abs": "https://arxiv.org/abs/2510.00137", "authors": ["Nima Sheikholeslami", "Erfan Hosseini", "Patrice Bechard", "Srivatsava Daruru", "Sai Rajeswar"], "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval", "comment": null, "summary": "Dual-encoder retrievers depend on the principle that relevant documents\nshould score higher than irrelevant ones for a given query. Yet the dominant\nNoise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,\noptimizes a softened ranking surrogate that we rigorously prove is\nfundamentally oblivious to score separation quality and unrelated to AUC. This\nmismatch leads to poor calibration and suboptimal performance in downstream\ntasks like retrieval-augmented generation (RAG). To address this fundamental\nlimitation, we introduce the MW loss, a new training objective that maximizes\nthe Mann-Whitney U statistic, which is mathematically equivalent to the Area\nunder the ROC Curve (AUC). MW loss encourages each positive-negative pair to be\ncorrectly ranked by minimizing binary cross entropy over score differences. We\nprovide theoretical guarantees that MW loss directly upper-bounds the AoC,\nbetter aligning optimization with retrieval goals. We further promote ROC\ncurves and AUC as natural threshold free diagnostics for evaluating retriever\ncalibration and ranking quality. Empirically, retrievers trained with MW loss\nconsistently outperform contrastive counterparts in AUC and standard retrieval\nmetrics. Our experiments show that MW loss is an empirically superior\nalternative to Contrastive Loss, yielding better-calibrated and more\ndiscriminative retrievers for high-stakes applications like RAG.", "AI": {"tldr": "\u63d0\u51faMW\u635f\u5931\u7528\u4e8e\u6539\u8fdb\u53cc\u7f16\u7801\u68c0\u7d22\u5668\u7684\u6821\u51c6\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u4f20\u7edf\u7684\u5bf9\u6bd4\u635f\u5931\u3002", "motivation": "\u9488\u5bf9\u53cc\u7f16\u7801\u68c0\u7d22\u5668\u4e2d\u7684\u73b0\u6709\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\uff08NCE\uff09\u76ee\u6807\u5728\u76f8\u5173\u6027\u8bc4\u5206\u4e0e\u6392\u540d\u8d28\u91cf\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faMW\u635f\u5931\u4f5c\u4e3a\u65b0\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u6700\u5927\u5316Mann-Whitney U\u7edf\u8ba1\u91cf\uff0c\u63d0\u9ad8\u68c0\u7d22\u5668\u7684\u6821\u51c6\u548c\u6392\u540d\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528MW\u635f\u5931\u8bad\u7ec3\u7684\u68c0\u7d22\u5668\u5728AUC\u548c\u6807\u51c6\u68c0\u7d22\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "MW\u635f\u5931\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u5bf9\u6bd4\u635f\u5931\u66f4\u4f18\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6821\u51c6\u548c\u66f4\u5f3a\u7684\u5206\u8fa8\u80fd\u529b\u3002"}}
{"id": "2510.00143", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.00143", "abs": "https://arxiv.org/abs/2510.00143", "authors": ["Eugene Yang", "Dawn Lawrie", "Orion Weller", "James Mayfield"], "title": "HLTCOE at TREC 2024 NeuCLIR Track", "comment": "TREC 2024 System Paper; 6 pages; 7 tables", "summary": "The HLTCOE team applied PLAID, an mT5 reranker, GPT-4 reranker, score fusion,\nand document translation to the TREC 2024 NeuCLIR track. For PLAID we included\na variety of models and training techniques -- Translate Distill (TD), Generate\nDistill (GD) and multi-lingual translate-distill (MTD). TD uses scores from the\nmT5 model over English MS MARCO query-document pairs to learn how to score\nquery-document pairs where the documents are translated to match the CLIR\nsetting. GD follows TD but uses passages from the collection and queries\ngenerated by an LLM for training examples. MTD uses MS MARCO translated into\nmultiple languages, allowing experiments on how to batch the data during\ntraining. Finally, for report generation we experimented with system\ncombination over different runs. One family of systems used either GPT-4o or\nClaude-3.5-Sonnet to summarize the retrieved results from a series of\ndecomposed sub-questions. Another system took the output from those two models\nand verified/combined them with Claude-3.5-Sonnet. The other family used GPT4o\nand GPT3.5Turbo to extract and group relevant facts from the retrieved\ndocuments based on the decomposed queries. The resulting submissions directly\nconcatenate the grouped facts to form the report and their documents of origin\nas the citations. The team submitted runs to all NeuCLIR tasks: CLIR and MLIR\nnews tasks as well as the technical documents task and the report generation\ntask.", "AI": {"tldr": "HLTCOE\u56e2\u961f\u5728TREC 2024 NeuCLIR\u8d5b\u9053\u4e0a\u5e94\u7528\u4e86PLAID\u3001\u91cd\u65b0\u6392\u5e8f\u5668\u548c\u7cfb\u7edf\u7ec4\u5408\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7528\u4e8eCLIR\u548cMLIR\u4efb\u52a1\u3002", "motivation": "\u63d0\u9ad8\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\uff08CLIR\uff09\u548c\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\uff08MLIR\uff09\u7684\u8f93\u51fa\u6765\u5e2e\u52a9\u9ad8\u6548\u7684\u62a5\u544a\u751f\u6210\u3002", "method": "\u91c7\u7528\u4e86PLAID\u3001mT5\u91cd\u65b0\u6392\u5e8f\u5668\u3001GPT-4\u91cd\u65b0\u6392\u5e8f\u5668\u3001\u8bc4\u5206\u878d\u5408\u4ee5\u53ca\u6587\u6863\u7ffb\u8bd1\u6765\u53c2\u4e0eTREC 2024 NeuCLIR\u8d5b\u9053\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u5730\u751f\u6210\u4e86\u4e0d\u540c\u5b50\u95ee\u9898\u7684\u7ed3\u679c\u62a5\u544a\uff0c\u7cfb\u7edf\u7ec4\u5408\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86\u53ef\u4ee5\u63d0\u9ad8\u62a5\u544a\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4f7f\u7528\u4e86\u591a\u79cd\u6280\u672f\u63d0\u9ad8\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u6548\u7387\uff0c\u5e76\u6210\u529f\u63d0\u4ea4\u4e86\u6240\u6709NeuCLIR\u4efb\u52a1\u3002"}}
{"id": "2510.00165", "categories": ["cs.IR", "cs.AI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.00165", "abs": "https://arxiv.org/abs/2510.00165", "authors": ["Prabhav Goyal", "Vinesh Sridhar", "Wilson Zheng"], "title": "Privacy-Preserving Learning-Augmented Data Structures", "comment": "6 pages, 2 figures", "summary": "Learning-augmented data structures use predicted frequency estimates to\nretrieve frequently occurring database elements faster than standard data\nstructures. Recent work has developed data structures that optimally exploit\nthese frequency estimates while maintaining robustness to adversarial\nprediction errors. However, the privacy and security implications of this\nsetting remain largely unexplored.\n  In the event of a security breach, data structures should reveal minimal\ninformation beyond their current contents. This is even more crucial for\nlearning-augmented data structures, whose layout adapts to the data. A data\nstructure is history independent if its memory representation reveals no\ninformation about past operations except what is inferred from its current\ncontents. In this work, we take the first step towards privacy and security\nguarantees in this setting by proposing the first learning-augmented data\nstructure that is strongly history independent, robust, and supports dynamic\nupdates.\n  To achieve this, we introduce two techniques: thresholding, which\nautomatically makes any learning-augmented data structure robust, and pairing,\na simple technique that provides strong history independence in the dynamic\nsetting. Our experimental results demonstrate a tradeoff between security and\nefficiency but are still competitive with the state of the art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5386\u53f2\u72ec\u7acb\u548c\u7a33\u5065\u7684\u5b66\u4e60\u589e\u5f3a\u6570\u636e\u7ed3\u6784\uff0c\u901a\u8fc7\u95e8\u69db\u548c\u914d\u5bf9\u6280\u672f\u5b9e\u73b0\uff0c\u517c\u5177\u9690\u79c1\u4e0e\u5b89\u5168\u6027\uff0c\u4e14\u652f\u6301\u52a8\u6001\u66f4\u65b0\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u7684\u6570\u636e\u7ed3\u6784\u80fd\u5229\u7528\u9891\u7387\u4f30\u8ba1\u4f18\u5316\u6027\u80fd\uff0c\u4f46\u5176\u9690\u79c1\u4e0e\u5b89\u5168\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u52a8\u673a\u5728\u4e8e\u589e\u5f3a\u6570\u636e\u7ed3\u6784\u5728\u9690\u79c1\u548c\u5b89\u5168\u65b9\u9762\u7684\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165\u4e86\u95e8\u69db\u548c\u914d\u5bf9\u4e24\u4e2a\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u6027\u548c\u5f3a\u5386\u53f2\u72ec\u7acb\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5b89\u5168\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u589e\u5f3a\u6570\u636e\u7ed3\u6784\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5386\u53f2\u72ec\u7acb\u6027\u548c\u7a33\u5065\u6027\uff0c\u540c\u65f6\u652f\u6301\u52a8\u6001\u66f4\u65b0\u3002"}}
{"id": "2510.00671", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00671", "abs": "https://arxiv.org/abs/2510.00671", "authors": ["Thong Nguyen", "Yibin Lei", "Jia-Huei Ju", "Eugene Yang", "Andrew Yates"], "title": "Milco: Learned Sparse Retrieval Across Languages via a Multilingual Connector", "comment": null, "summary": "Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with\nthe transparency of lexical matching, but existing approaches struggle to scale\nbeyond English. We introduce MILCO, an LSR architecture that maps queries and\ndocuments from different languages into a shared English lexical space via a\nmultilingual connector. MILCO is trained with a specialized two-stage regime\nthat combines Sparse Alignment Pretraining with contrastive training to provide\nrepresentation transparency and effectiveness while mitigating semantic\ncollapse. Motivated by the observation that uncommon entities are often lost\nwhen projected into English, we propose a new LexEcho head, which enhances\nrobustness by augmenting the English lexical representation with a\nsource-language view obtained through a special [ECHO] token. MILCO achieves\nstate-of-the-art multilingual and cross-lingual LSR performance, outperforming\nleading dense, sparse, and multi-vector baselines such as BGE-M3 and\nQwen3-Embed on standard multilingual benchmarks, while supporting dynamic\nefficiency through post-hoc pruning. Notably, when using mass-based pruning to\nreduce document representations to only 30 active dimensions on average, MILCO\n560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86MILCO\uff0c\u4e00\u79cd\u65b0\u7684LSR\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u8fde\u63a5\u5668\u5c06\u4e0d\u540c\u8bed\u8a00\u6620\u5c04\u5230\u5171\u4eab\u82f1\u8bed\u8bcd\u6c47\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u53cc\u7f16\u7801\u5668\u5728\u6269\u5c55\u5230\u82f1\u8bed\u4e4b\u5916\u8bed\u8a00\u65f6\u7684\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u7559\u8bcd\u6c47\u5339\u914d\u7684\u900f\u660e\u6027\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff0c\u5305\u62ec\u7a00\u758f\u5bf9\u9f50\u9884\u8bad\u7ec3\u548c\u5bf9\u6bd4\u8bad\u7ec3\uff0c\u7ed3\u5408LexEcho\u5934\u6765\u589e\u5f3a\u82f1\u8bed\u8bcd\u6c47\u8868\u793a\u548c\u6e90\u8bed\u8a00\u89c6\u56fe\u3002", "result": "MILCO\u5728\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u7a00\u758f\u68c0\u7d22\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u9886\u5148\u7684\u5bc6\u96c6\u3001\u7a00\u758f\u548c\u591a\u5411\u91cf\u57fa\u7ebf\uff0c\u5e76\u652f\u6301\u52a8\u6001\u6548\u7387\u8c03\u6574\u3002", "conclusion": "MILCO\u901a\u8fc7\u65b0\u9896\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6848\u6539\u5584\u4e86\u591a\u8bed\u8a00\u68c0\u7d22\u7684\u900f\u660e\u6027\u548c\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86LexEcho\u5934\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.00887", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.00887", "abs": "https://arxiv.org/abs/2510.00887", "authors": ["Soyoung Yoon", "Jongho Kim", "Daeyong Kwon", "Avishek Anand", "Seung-won Hwang"], "title": "On Listwise Reranking for Corpus Feedback", "comment": "Under review", "summary": "Reranker improves retrieval performance by capturing document interactions.\nAt one extreme, graph-aware adaptive retrieval (GAR) represents an\ninformation-rich regime, requiring a pre-computed document similarity graph in\nreranking. However, as such graphs are often unavailable, or incur quadratic\nmemory costs even when available, graph-free rerankers leverage large language\nmodel (LLM) calls to achieve competitive performance. We introduce L2G, a novel\nframework that implicitly induces document graphs from listwise reranker logs.\nBy converting reranker signals into a graph structure, L2G enables scalable\ngraph-based retrieval without the overhead of explicit graph computation.\nResults on the TREC-DL and BEIR subset show that L2G matches the effectiveness\nof oracle-based graph methods, while incurring zero additional LLM calls.", "AI": {"tldr": "L2G\u65e0\u9700\u989d\u5916\u7684LLM\u547c\u53eb\u5373\u53ef\u5b9e\u73b0\u4e0e\u57fa\u4e8e\u9884\u8ba1\u7b97\u56fe\u65b9\u6cd5\u76f8\u5f53\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6539\u5584\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\uff0c\u907f\u514d\u6602\u8d35\u7684\u56fe\u7ed3\u6784\u8ba1\u7b97\u548cLLM\u8c03\u7528\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aL2G\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5c06reranker\u65e5\u5fd7\u9690\u5f0f\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\u4ee5\u589e\u5f3a\u68c0\u7d22\u80fd\u529b\u3002", "result": "L2G\u6846\u67b6\u901a\u8fc7\u5c06reranker\u7684\u4fe1\u53f7\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\uff0c\u5927\u5e45\u5ea6\u63d0\u9ad8\u4e86\u68c0\u7d22\u6027\u80fd\u3002\u5b83\u53ef\u4ee5\u5728\u6ca1\u6709\u663e\u5f0f\u56fe\u8ba1\u7b97\u7684\u5f00\u9500\u4e0b\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u6548\u679c\u3002", "conclusion": "L2G\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u6602\u8d35\u7684\u56fe\u7ed3\u6784\u548cLLM\u8c03\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e0e\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u65b9\u6cd5\u76f8\u5f53\u7684\u6548\u80fd\u3002"}}
{"id": "2510.00908", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00908", "abs": "https://arxiv.org/abs/2510.00908", "authors": ["Roksana Goworek", "Olivia Macmillan-Scott", "Eda B. \u00d6zyi\u011fit"], "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs", "comment": null, "summary": "Cross-lingual information retrieval (CLIR) addresses the challenge of\nretrieving relevant documents written in languages different from that of the\noriginal query. Research in this area has typically framed the task as\nmonolingual retrieval augmented by translation, treating retrieval methods and\ncross-lingual capabilities in isolation. Both monolingual and cross-lingual\nretrieval usually follow a pipeline of query expansion, ranking, re-ranking\nand, increasingly, question answering. Recent advances, however, have shifted\nfrom translation-based methods toward embedding-based approaches and leverage\nmultilingual large language models (LLMs), for which aligning representations\nacross languages remains a central challenge. The emergence of cross-lingual\nembeddings and multilingual LLMs has introduced a new paradigm, offering\nimproved retrieval performance and enabling answer generation. This survey\nprovides a comprehensive overview of developments from early translation-based\nmethods to state-of-the-art embedding-driven and generative techniques. It\npresents a structured account of core CLIR components, evaluation practices,\nand available resources. Persistent challenges such as data imbalance and\nlinguistic variation are identified, while promising directions are suggested\nfor advancing equitable and effective cross-lingual information retrieval. By\nsituating CLIR within the broader landscape of information retrieval and\nmultilingual language processing, this work not only reviews current\ncapabilities but also outlines future directions for building retrieval systems\nthat are robust, inclusive, and adaptable.", "AI": {"tldr": "\u7814\u7a76\u4ece\u7ffb\u8bd1\u65b9\u6cd5\u8f6c\u5411\u5d4c\u5165\u9a71\u52a8\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u6570\u636e\u5931\u8861\u548c\u8bed\u8a00\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u5b9e\u73b0\u7b54\u6848\u751f\u6210\u3002", "method": "\u4ece\u7ffb\u8bd1\u65b9\u6cd5\u8fc7\u6e21\u5230\u5d4c\u5165\u9a71\u52a8\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u3002", "result": "\u5f15\u5165\u8de8\u8bed\u8a00\u5d4c\u5165\u548c\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u542f\u7528\u4e86\u7b54\u6848\u751f\u6210\u529f\u80fd\u3002", "conclusion": "\u5728\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u9886\u57df\uff0c\u5d4c\u5165\u9a71\u52a8\u65b9\u6cd5\u548c\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u68c0\u7d22\u6a21\u5f0f\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u542f\u7528\u7b54\u6848\u751f\u6210\uff0c\u672a\u6765\u9700\u8981\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8bed\u8a00\u53d8\u5f02\u95ee\u9898\u4ee5\u5b9e\u73b0\u66f4\u5065\u5168\u7684\u68c0\u7d22\u7cfb\u7edf\u3002"}}
{"id": "2510.00966", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00966", "abs": "https://arxiv.org/abs/2510.00966", "authors": ["Sara Saad Soliman", "Ahmed Younes", "Islam Elkabani", "Ashraf Elsayed"], "title": "Deep Learning-Based Approach for Improving Relational Aggregated Search", "comment": null, "summary": "Due to an information explosion on the internet, there is a need for the\ndevelopment of aggregated search systems that can boost the retrieval and\nmanagement of content in various formats. To further improve the clustering of\nArabic text data in aggregated search environments, this research investigates\nthe application of advanced natural language processing techniques, namely\nstacked autoencoders and AraBERT embeddings. By transcending the limitations of\ntraditional search engines, which are imprecise, not contextually relevant, and\nnot personalized, we offer more enriched, context-aware characterizations of\nsearch results, so we used a K-means clustering algorithm to discover\ndistinctive features and relationships in these results, we then used our\napproach on different Arabic queries to evaluate its effectiveness. Our model\nillustrates that using stacked autoencoders in representation learning suits\nclustering tasks and can significantly improve clustering search results. It\nalso demonstrates improved accuracy and relevance of search results.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u4f7f\u7528\u5806\u53e0\u81ea\u52a8\u7f16\u7801\u5668\u548cAraBERT\u5d4c\u5165\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u6570\u636e\u5728\u805a\u5408\u641c\u7d22\u73af\u5883\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u4fe1\u606f\u7206\u70b8\uff0c\u6025\u9700\u5f00\u53d1\u805a\u5408\u641c\u7d22\u7cfb\u7edf\uff0c\u4ee5\u589e\u5f3a\u5185\u5bb9\u68c0\u7d22\u548c\u7ba1\u7406\u3002", "method": "\u5e94\u7528\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5982\u5806\u53e0\u81ea\u52a8\u7f16\u7801\u5668\u548cAraBERT\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528K-means\u805a\u7c7b\u7b97\u6cd5\u6765\u5206\u6790\u963f\u62c9\u4f2f\u8bed\u67e5\u8be2\u6570\u636e\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u5806\u53e0\u81ea\u52a8\u7f16\u7801\u5668\u5728\u8868\u793a\u5b66\u4e60\u4e0a\u9002\u7528\u4e8e\u805a\u7c7b\u4efb\u52a1\uff0c\u5e76\u80fd\u663e\u8457\u6539\u5584\u805a\u7c7b\u641c\u7d22\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u901a\u8fc7\u6539\u8fdb\u805a\u7c7b\u7b97\u6cd5\u63d0\u9ad8\u641c\u7d22\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002"}}
{"id": "2510.01149", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01149", "abs": "https://arxiv.org/abs/2510.01149", "authors": ["Paul Teiletche", "Quentin Mac\u00e9", "Max Conti", "Antonio Loison", "Gautier Viaud", "Pierre Colombo", "Manuel Faysse"], "title": "ModernVBERT: Towards Smaller Visual Document Retrievers", "comment": null, "summary": "Multimodal embedding models are gaining prevalence, notably for document\nretrieval as efficient alternatives to text-only pipelines. These models are\ntypically built by finetuning large vision-language decoders (VLMs) with\ncontrastive losses on text-image pairs. In this work, we show that, while\ncost-efficient, this repurposing approach often bottlenecks retrieval\nperformance. Through controlled experiments, we establish a principled recipe\nfor improving visual document retrieval models. We notably measure the impact\nof attention masking, image resolution, modality alignment data regimes, and\nlate interaction centered contrastive objectives which emerge as central\nperformance factors. Building on these insights, we release ModernVBERT, a\ncompact 250M-parameter vision-language encoder that outperforms models up to 10\ntimes larger when finetuned on document retrieval tasks. Models and code are\nmade available at https://huggingface.co/ModernVBERT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u6a21\u578b\u7684\u539f\u5219\u6027\u65b9\u6848\uff0c\u5e76\u5f00\u53d1\u51fa\u6027\u80fd\u4f18\u5f02\u7684ModernVBERT\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u5728\u6587\u6863\u68c0\u7d22\u4e2d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf9\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5206\u6790\u6ce8\u610f\u529b\u63a9\u7801\u3001\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u6a21\u6001\u5bf9\u9f50\u6570\u636e\u65b9\u6848\u548c\u540e\u671f\u4ea4\u4e92\u4e2d\u5fc3\u5bf9\u6bd4\u76ee\u6807\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5236\u5b9a\u63d0\u5347\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u6a21\u578b\u7684\u539f\u5219\u6027\u65b9\u6848\u3002", "result": "\u7814\u53d1\u51faModernVBERT\uff0c\u4e00\u4e2a\u62e5\u6709250M\u53c2\u6570\u7684\u89c6\u56fe-\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u5728\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u4e0a\u6bd4\u4f53\u79ef\u5927\u5341\u500d\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u5b9e\u9a8c\u548c\u8c03\u6574\u6a21\u578b\u914d\u7f6e\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u7684\u6027\u80fd\uff0cModernVBERT\u662f\u8fd9\u4e00\u52aa\u529b\u7684\u6210\u529f\u6848\u4f8b\u3002"}}
