{"id": "2509.21323", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.21323", "abs": "https://arxiv.org/abs/2509.21323", "authors": ["Ana Rodrigues", "Jo\u00e3o Mata", "Rui Rego"], "title": "SPELUNKER: Item Similarity Search Using Large Language Models and Custom K-Nearest Neighbors", "comment": "6 pages, 4 figures", "summary": "This paper presents a hybrid system for intuitive item similarity search that\ncombines a Large Language Model (LLM) with a custom K-Nearest Neighbors (KNN)\nalgorithm. Unlike black-box dense vector systems, this architecture provides\nsuperior interpretability by first using an LLM to convert natural language\nqueries into structured, attribute-based searches. This structured query then\nserves as input to a custom KNN algorithm with a BallTree search strategy,\nwhich uses a heterogeneous distance metric to preserve distinct data types. Our\nevaluation, conducted on a dataset of 500 wine reviews, demonstrates the\nsystem's effectiveness. The LLM achieved an F1-score of 0.9779 in information\nextraction, while also demonstrating high fidelity with a Jaro string\nsimilarity of 0.9321. When we augmented the KNN algorithm with LLM-based\nre-ranking, we observed a statistically significant improvement in recall\n(p=0.013), indicating the LLM's ability to identify and promote relevant items\nthat align with nuanced user intent. This approach effectively bridges the gap\nbetween human language and machine-understandable item representations,\noffering a transparent and nuanced search capability.", "AI": {"tldr": "\u4f7f\u7528\u7ed3\u5408LLM\u548cKNN\u7684\u6df7\u5408\u7cfb\u7edf\u8fdb\u884c\u7269\u54c1\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u63d0\u9ad8\u4e86\u641c\u7d22\u7684\u900f\u660e\u6027\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u5e76\u5728\u8461\u8404\u9152\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002", "motivation": "\u4e0e\u9ed1\u7bb1\u5bc6\u96c6\u5411\u91cf\u7cfb\u7edf\u4e0d\u540c\uff0c\u6b64\u67b6\u6784\u901a\u8fc7\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u65e8\u5728\u4f7f\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u641c\u7d22\uff0c\u4ece\u800c\u66f4\u597d\u5730\u4fdd\u7559\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7684\u7279\u6027\uff0c\u4ee5\u4fbf\u5728\u7269\u54c1\u76f8\u4f3c\u6027\u641c\u7d22\u4e2d\u66f4\u7cbe\u51c6\u5730\u6ee1\u8db3\u7528\u6237\u7684\u6df1\u5c42\u610f\u56fe\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5b9a\u5236\u7684K\u4e34\u8fd1\u7b97\u6cd5\uff08KNN\uff09\u6765\u5b9e\u73b0\u76f4\u89c2\u7684\u7269\u54c1\u76f8\u4f3c\u6027\u641c\u7d22\u3002LLM\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u57fa\u4e8e\u5c5e\u6027\u7684\u641c\u7d22\uff0c\u968f\u540e\u8f93\u5165\u5230\u4f7f\u7528BallTree\u641c\u7d22\u7b56\u7565\u7684\u5b9a\u5236KNN\u7b97\u6cd5\u4e2d\u3002", "result": "\u5728500\u7bc7\u8461\u8404\u9152\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u663e\u793a\uff0cLLM\u5728\u4fe1\u606f\u62bd\u53d6\u4e0a\u7684F1\u5206\u6570\u4e3a0.9779\uff0c\u5e76\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u5fe0\u5b9e\u5ea6\uff08Jaro\u5b57\u7b26\u4e32\u76f8\u4f3c\u5ea6\u4e3a0.9321\uff09\u3002\u901a\u8fc7\u4e0eLLM\u4e3a\u57fa\u7840\u7684\u91cd\u6392\u540d\u7ed3\u5408\uff0cKNN\u7b97\u6cd5\u5728\u53ec\u56de\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff08p=0.013\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u4f9b\u4eba\u7c7b\u8bed\u8a00\u4e0e\u673a\u5668\u53ef\u7406\u89e3\u7684\u7269\u54c1\u8868\u793a\u4e4b\u95f4\u7684\u6865\u6881\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u7ec6\u81f4\u7684\u641c\u7d22\u80fd\u529b\u3002"}}
{"id": "2509.21324", "categories": ["cs.IR", "cs.AI", "I.2.1; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.21324", "abs": "https://arxiv.org/abs/2509.21324", "authors": ["Gurbinder Gill", "Ritvik Gupta", "Denis Lusson", "Anand Chandrashekar", "Donald Nguyen"], "title": "From Search to Reasoning: A Five-Level RAG Capability Framework for Enterprise Data", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as the standard paradigm for\nanswering questions on enterprise data. Traditionally, RAG has centered on\ntext-based semantic search and re-ranking. However, this approach falls short\nwhen dealing with questions beyond data summarization or non-text data. This\nhas led to various attempts to supplement RAG to bridge the gap between RAG,\nthe implementation paradigm, and the question answering problem that enterprise\nusers expect it to solve. Given that contemporary RAG is a collection of\ntechniques rather than a defined implementation, discussion of RAG and related\nquestion-answering systems benefits from a problem-oriented understanding.\n  We propose a new classification framework (L1-L5) to categorize systems based\non data modalities and task complexity of the underlying question answering\nproblems: L1 (Surface Knowledge of Unstructured Data) through L4 (Reflective\nand Reasoned Knowledge) and the aspirational L5 (General Intelligence). We also\nintroduce benchmarks aligned with these levels and evaluate four\nstate-of-the-art platforms: LangChain, Azure AI Search, OpenAI, and Corvic AI.\nOur experiments highlight the value of multi-space retrieval and dynamic\norchestration for enabling L1-L4 capabilities. We empirically validate our\nfindings using diverse datasets indicative of enterprise use cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\u5212\u5206\u4f01\u4e1a\u95ee\u7b54\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u5e76\u8bc4\u4ef7\u4e86\u76f8\u5173\u5e73\u53f0\u3002", "motivation": "\u4f20\u7edf\u7684RAG\u65b9\u6cd5\u5728\u5904\u7406\u4f01\u4e1a\u6570\u636e\u4e0a\u7684\u95ee\u7b54\u95ee\u9898\u65f6\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8bed\u4e49\u641c\u7d22\u548c\u91cd\u6392\u5e8f\uff0c\u4f46\u5728\u5904\u7406\u603b\u7ed3\u6587\u672c\u4ee5\u5916\u7684\u95ee\u9898\u4ee5\u53ca\u975e\u6587\u672c\u6570\u636e\u65f6\u6548\u679c\u6709\u9650\u3002\u56e0\u6b64\uff0c\u4eba\u4eec\u5c1d\u8bd5\u901a\u8fc7\u8865\u5145RAG\u6765\u5f25\u5408\u95ee\u9898\u5b9e\u65bd\u8303\u5f0f\u548c\u4f01\u4e1a\u7528\u6237\u5bf9\u95ee\u7b54\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6846\u67b6L1-L5\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u6548\u679c\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u56db\u4e2a\u95ee\u7b54\u5e73\u53f0\u7684\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7ea7\u6846\u67b6\uff08L1-L5\uff09\uff0c\u7528\u4e8e\u6839\u636e\u6570\u636e\u6a21\u6001\u548c\u95ee\u7b54\u4efb\u52a1\u590d\u6742\u6027\u5bf9\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u4e0e\u8fd9\u4e9b\u7b49\u7ea7\u5bf9\u9f50\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4ef7\u56db\u79cd\u6700\u5148\u8fdb\u7684\u5e73\u53f0\uff1aLangChain\u3001Azure AI Search\u3001OpenAI \u548c Corvic AI\u3002\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u591a\u7a7a\u95f4\u68c0\u7d22\u548c\u52a8\u6001\u7f16\u6392\u5728\u53d1\u6325L1-L4\u80fd\u529b\u4e0a\u7684\u4ef7\u503c\u3002", "conclusion": "\u591a\u7a7a\u95f4\u68c0\u7d22\u548c\u52a8\u6001\u7f16\u6392\u5bf9\u4e8e\u5b9e\u73b0\u4f01\u4e1a\u6570\u636e\u95ee\u7b54\u7cfb\u7edf\u7684\u591a\u7ea7\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.21325", "categories": ["cs.IR", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21325", "abs": "https://arxiv.org/abs/2509.21325", "authors": ["Baiqiang Wang", "Qian Lou", "Mengxin Zheng", "Dongfang Zhao"], "title": "PIR-RAG: A System for Private Information Retrieval in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a foundational component of\nmodern AI systems, yet it introduces significant privacy risks by exposing user\nqueries to service providers. To address this, we introduce PIR-RAG, a\npractical system for privacy-preserving RAG. PIR-RAG employs a novel\narchitecture that uses coarse-grained semantic clustering to prune the search\nspace, combined with a fast, lattice-based Private Information Retrieval (PIR)\nprotocol. This design allows for the efficient retrieval of entire document\nclusters, uniquely optimizing for the end-to-end RAG workflow where full\ndocument content is required. Our comprehensive evaluation against strong\nbaseline architectures, including graph-based PIR and Tiptoe-style private\nscoring, demonstrates PIR-RAG's scalability and its superior performance in\nterms of \"RAG-Ready Latency\"-the true end-to-end time required to securely\nfetch content for an LLM. Our work establishes PIR-RAG as a viable and highly\nefficient solution for privacy in large-scale AI systems.", "AI": {"tldr": "PIR-RAG\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u548c\u534f\u8bae\u4f18\u5316\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ec4\u4ef6\u66b4\u9732\u7528\u6237\u67e5\u8be2\u7ed9\u670d\u52a1\u63d0\u4f9b\u5546\uff0c\u5e26\u6765\u4e86\u9690\u79c1\u98ce\u9669\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u8bed\u4e49\u805a\u7c7b\u6765\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u5feb\u901f\u7684\u57fa\u4e8e\u6676\u683c\u7684\u79c1\u4eba\u4fe1\u606f\u68c0\u7d22\uff08PIR\uff09\u534f\u8bae\u3002", "result": "PIR-RAG\u5728\u4e0e\u5f3a\u5927\u7684\u57fa\u51c6\u67b6\u6784\u5bf9\u6bd4\u4e2d\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u5728\u201cRAG\u51c6\u5907\u5ef6\u8fdf\u201d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PIR-RAG\u4e3a\u5927\u89c4\u6a21AI\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21336", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21336", "abs": "https://arxiv.org/abs/2509.21336", "authors": ["Guohang Yan", "Yue Zhang", "Pinlong Cai", "Ding Wang", "Song Mao", "Hongwei Zhang", "Yaoze Zhang", "Hairong Zhang", "Xinyu Cai", "Botian Shi"], "title": "HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores", "comment": "15 pages, 4 figures", "summary": "Retrieval-augmented generation (RAG) has become a dominant paradigm for\nmitigating knowledge hallucination and staleness in large language models\n(LLMs) while preserving data security. By retrieving relevant evidence from\nprivate, domain-specific corpora and injecting it into carefully engineered\nprompts, RAG delivers trustworthy responses without the prohibitive cost of\nfine-tuning. Traditional retrieval-augmented generation (RAG) systems are\ntext-only and often rely on a single storage backend, most commonly a vector\ndatabase. In practice, this monolithic design suffers from unavoidable\ntrade-offs: vector search captures semantic similarity yet loses global\ncontext; knowledge graphs excel at relational precision but struggle with\nrecall; full-text indexes are fast and exact yet semantically blind; and\nrelational engines such as MySQL provide strong transactional guarantees but no\nsemantic understanding. We argue that these heterogeneous retrieval paradigms\nare complementary, and propose a principled fusion scheme to orchestrate them\nsynergistically, mitigating the weaknesses of any single modality. In this work\nwe introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework\nthat orchestrates cross-modal evidence from heterogeneous data stores. We plan\nto design a system that unifies vector indices, knowledge graphs, full-text\nengines, and structured databases into a single retrieval plane, dynamically\nrouting and fusing evidence to maximize recall, precision, and contextual\nfidelity. To achieve this design goal, we carried out preliminary explorations\nand constructed an initial RAG pipeline; this technical report provides a brief\noverview. The partial code is available at\nhttps://github.com/KnowledgeXLab/HetaRAG.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HetaRAG\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5f02\u6784\u6570\u636e\u5b58\u50a8\u89e3\u51b3\u4f20\u7edfRAG\u7cfb\u7edf\u7684\u8bbe\u8ba1\u7f3a\u9677\uff0c\u4ee5\u6539\u5584\u68c0\u7d22\u53ec\u56de\u7387\u3001\u7cbe\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u5e7b\u89c9\u548c\u9648\u65e7\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u969c\u6570\u636e\u5b89\u5168\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u3002", "method": "\u5f15\u5165HetaRAG\u6846\u67b6\uff0c\u5c06\u5411\u91cf\u7d22\u5f15\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u5168\u6587\u5f15\u64ce\u548c\u7ed3\u6784\u5316\u6570\u636e\u5e93\u7ed3\u5408\u4e3a\u4e00\u4e2a\u5355\u4e00\u7684\u68c0\u7d22\u5e73\u9762\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u878d\u5408\u8bc1\u636e\u6765\u6700\u5927\u5316\u68c0\u7d22\u7684\u53ec\u56de\u7387\u3001\u7cbe\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\u3002", "result": "\u8fdb\u884c\u4e86\u521d\u6b65\u63a2\u7d22\uff0c\u6784\u5efa\u4e86\u521d\u59cb\u7684RAG\u7ba1\u9053\uff0c\u5e76\u63d0\u4f9b\u4e86\u90e8\u5206\u4ee3\u7801\u4ee5\u4f9b\u67e5\u770b\u3002", "conclusion": "\u4e0d\u540c\u68c0\u7d22\u65b9\u5f0f\u6709\u5404\u81ea\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u878d\u5408\u8fd9\u4e9b\u5f02\u6784\u68c0\u7d22\u6a21\u5f0f\u53ef\u4ee5\u76f8\u4e92\u8865\u5145\uff0c\u589e\u5f3aRAG\u6846\u67b6\u7684\u6027\u80fd\u3002"}}
{"id": "2509.21339", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.21339", "abs": "https://arxiv.org/abs/2509.21339", "authors": ["Jiahao Zhang", "Wenzhe Yin", "Shujian Yu"], "title": "Cross-Modal Retrieval with Cauchy-Schwarz Divergence", "comment": "Accepted by ACMMM-25", "summary": "Effective cross-modal retrieval requires robust alignment of heterogeneous\ndata types. Most existing methods focus on bi-modal retrieval tasks and rely on\ndistributional alignment techniques such as Kullback-Leibler divergence,\nMaximum Mean Discrepancy, and correlation alignment. However, these methods\noften suffer from critical limitations, including numerical instability,\nsensitivity to hyperparameters, and their inability to capture the full\nstructure of the underlying distributions. In this paper, we introduce the\nCauchy-Schwarz (CS) divergence, a hyperparameter-free measure that improves\nboth training stability and retrieval performance. We further propose a novel\nGeneralized CS (GCS) divergence inspired by H\\\"older's inequality. This\nextension enables direct alignment of three or more modalities within a unified\nmathematical framework through a bidirectional circular comparison scheme,\neliminating the need for exhaustive pairwise comparisons. Extensive experiments\non six benchmark datasets demonstrate the effectiveness of our method in both\nbi-modal and tri-modal retrieval tasks. The code of our CS/GCS divergence is\npublicly available at https://github.com/JiahaoZhang666/CSD.", "AI": {"tldr": "Introduce Cauchy-Schwarz divergence for stable and effective cross-modal retrieval, and extend it to Generalized CS divergence for multi-modality alignment, improving efficiency and performance.", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5f02\u8d28\u6570\u636e\u7c7b\u578b\u4e4b\u95f4\u8fdb\u884c\u53cc\u6a21\u6001\u68c0\u7d22\u65f6\u5e38\u53d7\u9650\u4e8e\u5206\u5e03\u5bf9\u9f50\u6280\u672f\uff0c\u5982Kullback-Leibler\u53d1\u6563\u3001\u6700\u5927\u5747\u503c\u5dee\u5f02\u548c\u76f8\u5173\u6027\u5bf9\u9f50\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u3001\u5bf9\u8d85\u53c2\u6570\u654f\u611f\u4ee5\u53ca\u65e0\u6cd5\u6355\u6349\u6f5c\u5728\u5206\u5e03\u7684\u5b8c\u6574\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528Cauchy-Schwarz (CS) \u53d1\u6563\u548c\u5e7f\u4e49CS (GCS) \u53d1\u6563\uff0c\u901a\u8fc7\u53cc\u5411\u5faa\u73af\u6bd4\u8f83\u65b9\u6848\u5b9e\u73b0\u8de8\u591a\u4e2a\u6a21\u6001\u7684\u76f4\u63a5\u5bf9\u9f50\u3002", "result": "\u63d0\u51fa\u4e86Cauchy-Schwarz (CS) \u53d1\u6563\uff0c\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u7684\u5ea6\u91cf\uff0c\u80fd\u591f\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u68c0\u7d22\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u57fa\u4e8eH\u00f6lder\u4e0d\u7b49\u5f0f\u7684\u5e7f\u4e49CS (GCS) \u53d1\u6563\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\u5185\u76f4\u63a5\u5bf9\u9f50\u4e09\u4e2a\u6216\u591a\u4e2a\u6a21\u6001\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u8be6\u5c3d\u7684\u6210\u5bf9\u6bd4\u8f83\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u53cc\u6a21\u6001\u548c\u4e09\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.21371", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21371", "abs": "https://arxiv.org/abs/2509.21371", "authors": ["Dayu Yang", "Hui Fang"], "title": "ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems", "comment": "Accepted by WISE 2025: 26th International Web Information Systems\n  Engineering conference. Our code is publicly available at the link:\n  https://github.com/dayuyang1999/ReGeS", "summary": "Connecting conversation with external domain knowledge is vital for\nconversational recommender systems (CRS) to correctly understand user\npreferences. However, existing solutions either require domain-specific\nengineering, which limits flexibility, or rely solely on large language models,\nwhich increases the risk of hallucination. While Retrieval-Augmented Generation\n(RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that\nweaken retrieval and by overlooked nuances among similar items. We propose\nReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies\ngeneration-augmented retrieval to distill informative user intent from\nconversations and retrieval-augmented generation to differentiate subtle item\nfeatures. This synergy obviates the need for extra annotations, reduces\nhallucinations, and simplifies continuous updates. Experiments on multiple CRS\nbenchmarks show that ReGeS achieves state-of-the-art performance in\nrecommendation accuracy, demonstrating the effectiveness of reciprocal synergy\nfor knowledge-intensive CRS tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6ReGeS\uff0c\u901a\u8fc7\u589e\u5f3a\u68c0\u7d22\u548c\u751f\u6210\u5b9e\u73b0\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5229\u7528\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u6765\u51c6\u786e\u7406\u89e3\u7528\u6237\u504f\u597d\u5bf9\u4e8e\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u7279\u5b9a\u9886\u57df\u7684\u5de5\u7a0b\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\uff0c\u8981\u4e48\u8fc7\u4e8e\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7684\u98ce\u9669\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u4e92\u7684\u68c0\u7d22-\u751f\u6210\u534f\u540c\u6846\u67b6ReGeS\uff0c\u901a\u8fc7\u751f\u6210\u589e\u5f3a\u68c0\u7d22\u6765\u63d0\u70bc\u6765\u81ea\u5bf9\u8bdd\u7684\u7528\u6237\u610f\u56fe\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u533a\u5206\u7ec6\u5fae\u7684\u9879\u76ee\u7279\u5f81\u3002", "result": "ReGeS\u5728\u591a\u4e2aCRS\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u63a8\u8350\u7cbe\u5ea6\u7684\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "\u76f8\u4e92\u534f\u4f5c\u7684\u751f\u6210\u548c\u68c0\u7d22\u6280\u672f\u5bf9\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4efb\u52a1\u7684\u6709\u6548\u6027\u5f97\u5230\u8bc1\u660e\u3002"}}
{"id": "2509.21391", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21391", "abs": "https://arxiv.org/abs/2509.21391", "authors": ["Lihui Liu", "Carl J. Yang"], "title": "MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance across a\nwide range of applications. However, they often suffer from hallucinations in\nknowledge-intensive domains due to their reliance on static pretraining\ncorpora. To address this limitation, Retrieval-Augmented Generation (RAG)\nenhances LLMs by incorporating external knowledge sources during inference.\nAmong these sources, textual graphs provide structured and semantically rich\ninformation that supports more precise and interpretable reasoning. This has\nled to growing interest in graph-based RAG systems. Despite their potential,\nmost existing approaches rely on a single retriever to identify relevant\nsubgraphs, which limits their ability to capture the diverse aspects of complex\nqueries. Moreover, these systems often struggle to accurately judge the\nrelevance of retrieved content, making them prone to distraction by irrelevant\nnoise. To address these challenges, in this paper, we propose MIXRAG, a\nMixture-of-Experts Graph-RAG framework that introduces multiple specialized\ngraph retrievers and a dynamic routing controller to better handle diverse\nquery intents. Each retriever is trained to focus on a specific aspect of graph\nsemantics, such as entities, relations, or subgraph topology. A\nMixture-of-Experts module adaptively selects and fuses relevant retrievers\nbased on the input query. To reduce noise in the retrieved information, we\nintroduce a query-aware GraphEncoder that carefully analyzes relationships\nwithin the retrieved subgraphs, highlighting the most relevant parts while\ndown-weighting unnecessary noise. Empirical results demonstrate that our method\nachieves state-of-the-art performance and consistently outperforms various\nbaselines. MIXRAG is effective across a wide range of graph-based tasks in\ndifferent domains. The code will be released upon paper acceptance.", "AI": {"tldr": "MIXRAG\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u591a\u4e2a\u56fe\u68c0\u7d22\u5668\u548c\u52a8\u6001\u8def\u7531\u63a7\u5236\u5668\u589e\u5f3aLLMs\u6027\u80fd\uff0c\u5728\u57fa\u4e8e\u56fe\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u57df\u5b58\u5728\u5e7b\u89c9\u73b0\u8c61\uff0c\u5176\u539f\u56e0\u5728\u4e8e\u4f9d\u8d56\u9759\u6001\u9884\u8bad\u7ec3\u8bed\u6599\u3002", "method": "\u63d0\u51faMIXRAG\u6846\u67b6\uff0c\u5f15\u5165\u591a\u4e2a\u4e13\u4e1a\u56fe\u68c0\u7d22\u5668\u548c\u52a8\u6001\u8def\u7531\u63a7\u5236\u5668\uff0c\u6bcf\u4e2a\u68c0\u7d22\u5668\u4e13\u6ce8\u4e8e\u56fe\u8bed\u4e49\u7684\u7279\u5b9a\u65b9\u9762\uff0c\u5982\u5b9e\u4f53\u3001\u5173\u7cfb\u6216\u5b50\u56fe\u62d3\u6251\u3002\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\u6839\u636e\u8f93\u5165\u67e5\u8be2\u81ea\u9002\u5e94\u9009\u62e9\u548c\u878d\u5408\u76f8\u5173\u68c0\u7d22\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u9886\u57df\u7684\u57fa\u4e8e\u56fe\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MIXRAG\u6709\u6548\u5730\u5904\u7406\u67e5\u8be2\u610f\u56fe\u591a\u6837\u6027\uff0c\u5e76\u901a\u8fc7\u67e5\u8be2\u611f\u77e5\u7684GraphEncoder\u51cf\u5c11\u68c0\u7d22\u4fe1\u606f\u4e2d\u7684\u566a\u58f0\u3002"}}
{"id": "2509.21966", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.21966", "abs": "https://arxiv.org/abs/2509.21966", "authors": ["Taiga Sasaki", "Takehiro Yamamoto", "Hiroaki Ohshima", "Sumio Fujita"], "title": "Effect of Model Merging in Domain-Specific Ad-hoc Retrieval", "comment": "Accepted at CIKM 2025, 5 pages", "summary": "In this study, we evaluate the effect of model merging in ad-hoc retrieval\ntasks. Model merging is a technique that combines the diverse characteristics\nof multiple models. We hypothesized that applying model merging to\ndomain-specific ad-hoc retrieval tasks could improve retrieval effectiveness.\nTo verify this hypothesis, we merged the weights of a source retrieval model\nand a domain-specific (non-retrieval) model using a linear interpolation\napproach. A key advantage of our approach is that it requires no additional\nfine-tuning of the models. We conducted two experiments each in the medical and\nJapanese domains. The first compared the merged model with the source retrieval\nmodel, and the second compared it with a LoRA fine-tuned model under both full\nand limited data settings for model construction. The experimental results\nindicate that model merging has the potential to produce more effective\ndomain-specific retrieval models than the source retrieval model, and may serve\nas a practical alternative to LoRA fine-tuning, particularly when only a\nlimited amount of data is available.", "AI": {"tldr": "\u6a21\u578b\u5408\u5e76\u53ef\u4ee5\u63d0\u9ad8\u7279\u5b9a\u9886\u57df\u7684\u4e34\u65f6\u68c0\u7d22\u4efb\u52a1\u6548\u679c\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u7279\u522b\u9002\u5408\u6709\u9650\u6570\u636e\u60c5\u51b5\uff0c\u662fLoRA\u5fae\u8c03\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u6211\u4eec\u5047\u8bbe\u5c06\u6a21\u578b\u5408\u5e76\u5e94\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u4e34\u65f6\u68c0\u7d22\u4efb\u52a1\u53ef\u4ee5\u63d0\u9ad8\u68c0\u7d22\u6548\u679c\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u63d2\u503c\u65b9\u6cd5\u5408\u5e76\u4e86\u6e90\u68c0\u7d22\u6a21\u578b\u548c\u7279\u5b9a\u9886\u57df\uff08\u975e\u68c0\u7d22\uff09\u6a21\u578b\u7684\u6743\u91cd\uff0c\u4e24\u8005\u7684\u5408\u5e76\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u5fae\u8c03\u3002\u6211\u4eec\u5728\u533b\u7597\u548c\u65e5\u672c\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5408\u5e76\u6709\u53ef\u80fd\u6bd4\u6e90\u68c0\u7d22\u6a21\u578b\u4ea7\u751f\u66f4\u6709\u6548\u7684\u7279\u5b9a\u9886\u57df\u68c0\u7d22\u6a21\u578b\uff0c\u5e76\u4e14\u53ef\u80fd\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6210\u4e3aLoRA\u5fae\u8c03\u7684\u5b9e\u9645\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5728\u8fdb\u884c\u5b9e\u9a8c\u7684\u9886\u57df\u4e2d\uff0c\u6a21\u578b\u5408\u5e76\u6280\u672f\u80fd\u63d0\u9ad8\u68c0\u7d22\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2509.22046", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.22046", "abs": "https://arxiv.org/abs/2509.22046", "authors": ["Kaike Zhang", "Xiaobei Wang", "Shuchang Liu", "Hailan Yang", "Xiang Li", "Lantao Hu", "Han Li", "Qi Cao", "Fei Sun", "Kun Gai"], "title": "GoalRank: Group-Relative Optimization for a Large Ranking Model", "comment": null, "summary": "Mainstream ranking approaches typically follow a Generator-Evaluator\ntwo-stage paradigm, where a generator produces candidate lists and an evaluator\nselects the best one. Recent work has attempted to enhance performance by\nexpanding the number of candidate lists, for example, through multi-generator\nsettings. However, ranking involves selecting a recommendation list from a\ncombinatorially large space. Simply enlarging the candidate set remains\nineffective, and performance gains quickly saturate. At the same time, recent\nadvances in large recommendation models have shown that end-to-end one-stage\nmodels can achieve promising performance with the expectation of scaling laws.\nMotivated by this, we revisit ranking from a generator-only one-stage\nperspective. We theoretically prove that, for any (finite\nMulti-)Generator-Evaluator model, there always exists a generator-only model\nthat achieves strictly smaller approximation error to the optimal ranking\npolicy, while also enjoying scaling laws as its size increases. Building on\nthis result, we derive an evidence upper bound of the one-stage optimization\nobjective, from which we find that one can leverage a reward model trained on\nreal user feedback to construct a reference policy in a group-relative manner.\nThis reference policy serves as a practical surrogate of the optimal policy,\nenabling effective training of a large generator-only ranker. Based on these\ninsights, we propose GoalRank, a generator-only ranking framework. Extensive\noffline experiments on public benchmarks and large-scale online A/B tests\ndemonstrate that GoalRank consistently outperforms state-of-the-art methods.", "AI": {"tldr": "\u88ab\u8bc1\u660e\u751f\u6210\u5668\u5355\u9636\u6bb5\u6a21\u578b\u6bd4\u4e24\u9636\u6bb5\u6a21\u578b\u66f4\u6709\u6548\uff0c\u63d0\u51fa GoalRank \u6846\u67b6\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u8d85\u8fc7\u73b0\u6709\u6392\u5e8f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u6d41\u6392\u5e8f\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u751f\u6210\u5668-\u8bc4\u4f30\u5668\u4e24\u9636\u6bb5\u8303\u5f0f\uff0c\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u968f\u7740\u5927\u63a8\u8350\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u4e00\u9636\u6bb5\u6a21\u578b\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u6fc0\u53d1\u4e86\u4ece\u751f\u6210\u5668\u5355\u9636\u6bb5\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u6392\u5e8f\u95ee\u9898\u7684\u52a8\u673a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ea\u6709\u751f\u6210\u5668\u7684\u6392\u5e8f\u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u5c0f\u7684\u903c\u8fd1\u8bef\u5dee\uff0c\u5e76\u4e14\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u4eab\u6709\u4f38\u7f29\u6027\u3002\u57fa\u4e8e\u6b64\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e00\u9636\u6bb5\u4f18\u5316\u76ee\u6807\u7684\u4e0a\u8bc1\u636e\u754c\uff0c\u5229\u7528\u771f\u5b9e\u7528\u6237\u53cd\u9988\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u6784\u5efa\u4e86\u53c2\u8003\u7b56\u7565\u3002", "result": "GoalRank\u6846\u67b6\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u5927\u89c4\u6a21\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86\u6700\u65b0\u7684\u6392\u5e8f\u65b9\u6cd5\u3002", "conclusion": "\u7406\u8bba\u8bc1\u660e\u751f\u6210\u5668\u5355\u9636\u6bb5\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u66f4\u5c0f\u7684\u903c\u8fd1\u8bef\u5dee\u3002\u5b9e\u884c\u751f\u6210\u5668\u5355\u9636\u6bb5\u6392\u540d\u6846\u67b6 GoalRank \uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6392\u5e8f\u6027\u80fd\u3002"}}
{"id": "2509.22116", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.22116", "abs": "https://arxiv.org/abs/2509.22116", "authors": ["Yingchen Zhang", "Ruqing Zhang", "Jiafeng Guo", "Maarten de Rijke", "Yixing Fan", "Xueqi Cheng"], "title": "Does Generative Retrieval Overcome the Limitations of Dense Retrieval?", "comment": null, "summary": "Generative retrieval (GR) has emerged as a new paradigm in neural information\nretrieval, offering an alternative to dense retrieval (DR) by directly\ngenerating identifiers of relevant documents. In this paper, we theoretically\nand empirically investigate how GR fundamentally diverges from DR in both\nlearning objectives and representational capacity. GR performs globally\nnormalized maximum-likelihood optimization and encodes corpus and relevance\ninformation directly in the model parameters, whereas DR adopts locally\nnormalized objectives and represents the corpus with external embeddings before\ncomputing similarity via a bilinear interaction. Our analysis suggests that,\nunder scaling, GR can overcome the inherent limitations of DR, yielding two\nmajor benefits. First, with larger corpora, GR avoids the sharp performance\ndegradation caused by the optimization drift induced by DR's local\nnormalization. Second, with larger models, GR's representational capacity\nscales with parameter size, unconstrained by the global low-rank structure that\nlimits DR. We validate these theoretical insights through controlled\nexperiments on the Natural Questions and MS MARCO datasets, across varying\nnegative sampling strategies, embedding dimensions, and model scales. But\ndespite its theoretical advantages, GR does not universally outperform DR in\npractice. We outline directions to bridge the gap between GR's theoretical\npotential and practical performance, providing guidance for future research in\nscalable and robust generative retrieval.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u68c0\u7d22\uff08GR\uff09\u4e0e\u5bc6\u96c6\u68c0\u7d22\uff08DR\uff09\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u5dee\u5f02\u3002GR\u901a\u8fc7\u751f\u6210\u76f8\u5173\u6587\u6863\u7684\u6807\u8bc6\u7b26\u8fdb\u884c\u68c0\u7d22\uff0c\u5177\u6709\u5168\u7403\u5f52\u4e00\u5316\u6700\u5927\u4f3c\u7136\u4f18\u5316\u7684\u7279\u70b9\uff1b\u800cDR\u4f7f\u7528\u5c40\u90e8\u5f52\u4e00\u5316\u76ee\u6807\u5e76\u901a\u8fc7\u5916\u90e8\u5d4c\u5165\u8868\u793a\u6587\u6863\u7136\u540e\u8fdb\u884c\u76f8\u4f3c\u6027\u8ba1\u7b97\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGR\u53ef\u4ee5\u5728\u5927\u89c4\u6a21\u4e0b\u514b\u670dDR\u7684\u5c40\u9650\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u4e2d\u5e76\u4e0d\u603b\u662f\u4f18\u4e8eDR\u3002", "motivation": "\u63a2\u7a76\u751f\u6210\u68c0\u7d22\uff08GR\uff09\u5982\u4f55\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u4e0e\u5bc6\u96c6\u68c0\u7d22\uff08DR\uff09\u4e0d\u540c\uff0c\u5e76\u5bfb\u627e\u7406\u8bba\u652f\u6301\u6765\u4f18\u5316\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\u3002", "method": "\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u901a\u8fc7\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u6761\u4ef6\uff08\u5982\u8d1f\u91c7\u6837\u7b56\u7565\u3001\u5d4c\u5165\u7ef4\u5ea6\u548c\u6a21\u578b\u89c4\u6a21\uff09\u7684\u63a7\u5236\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u7ed3\u8bba\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u5927\u89c4\u6a21\u8bed\u6599\u548c\u6a21\u578b\u6761\u4ef6\u4e0b\uff0cGR\u80fd\u514b\u670dDR\u7684\u4f18\u5316\u6f02\u79fb\u95ee\u9898\u548c\u8868\u793a\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u6548\u679c\u672a\u80fd\u5168\u9762\u8d85\u8d8aDR\u3002", "conclusion": "\u5c3d\u7ba1GR\u5728\u7406\u8bba\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e76\u672a\u666e\u904d\u4f18\u4e8eDR\u3002\u7814\u7a76\u5efa\u8bae\u63a2\u7d22\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u7f29\u77ed\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.22325", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22325", "abs": "https://arxiv.org/abs/2509.22325", "authors": ["JiaYing Zheng", "HaiNan Zhang", "Liang Pang", "YongXin Tong", "ZhiMing Zheng"], "title": "Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?", "comment": "10 pages, 6 figures", "summary": "Multi-turn RAG systems often face queries with colloquial omissions and\nambiguous references, posing significant challenges for effective retrieval and\ngeneration. Traditional query rewriting relies on human annotators to clarify\nqueries, but due to limitations in annotators' expressive ability and depth of\nunderstanding, manually rewritten queries often diverge from those needed in\nreal-world RAG systems, resulting in a gap between user intent and system\nresponse. We observe that high-quality synthetic queries can better bridge this\ngap, achieving superior performance in both retrieval and generation compared\nto human rewrites. This raises an interesting question: Can rewriting models\ntrained on synthetic queries better capture user intent than human annotators?\nIn this paper, we propose SynRewrite, a synthetic data-driven query rewriting\nmodel to generate high-quality synthetic rewrites more aligned with user\nintent. To construct training data, we prompt GPT-4o with dialogue history,\ncurrent queries, positive documents, and answers to synthesize high-quality\nrewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue\nhistory and queries to synthetic rewrites. Finally, we further enhance the\nrewriter using the generator's feedback through the DPO algorithm to boost\nend-task performance. Experiments on TopiOCQA and QRECC datasets show that\nSynRewrite consistently outperforms human rewrites in both retrieval and\ngeneration tasks. Our results demonstrate that synthetic rewrites can serve as\na scalable and effective alternative to human annotations.", "AI": {"tldr": "SynRewrite\u901a\u8fc7\u751f\u6210\u5408\u6210\u67e5\u8be2\u91cd\u5199\u63d0\u9ad8RAG\u7cfb\u7edf\u7684\u68c0\u7d22\u548c\u751f\u6210\u6027\u80fd\uff0c\u4f18\u4e8e\u4eba\u5de5\u91cd\u5199\u3002", "motivation": "\u4f20\u7edf\u7684\u67e5\u8be2\u91cd\u5199\u4f9d\u8d56\u4e8e\u4eba\u5de5\u6ce8\u91ca\u6765\u6f84\u6e05\u67e5\u8be2\uff0c\u4f46\u7531\u4e8e\u4eba\u7c7b\u6ce8\u91ca\u8005\u5728\u8868\u8fbe\u80fd\u529b\u548c\u7406\u89e3\u6df1\u5ea6\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u4eba\u5de5\u91cd\u5199\u7684\u67e5\u8be2\u5f80\u5f80\u4e0e\u771f\u5b9e\u4e16\u754cRAG\u7cfb\u7edf\u4e2d\u6240\u9700\u7684\u67e5\u8be2\u5b58\u5728\u5dee\u5f02\uff0c\u4ece\u800c\u5bfc\u81f4\u7528\u6237\u610f\u56fe\u548c\u7cfb\u7edf\u54cd\u5e94\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u591a\u8f6eRAG\u7cfb\u7edf\u5e38\u56e0\u53e3\u8bed\u7701\u7565\u548c\u6a21\u7cca\u53c2\u7167\u9762\u4e34\u91cd\u5927\u7684\u68c0\u7d22\u4e0e\u751f\u6210\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u67e5\u8be2\u91cd\u5199\u6a21\u578bSynRewrite\uff0c\u4ee5\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u5408\u6210\u91cd\u5199\u3002\u6570\u636e\u6784\u5efa\u4e0a\uff0c\u9996\u5148\u5229\u7528GPT-4o\u6839\u636e\u5bf9\u8bdd\u5386\u53f2\u3001\u5f53\u524d\u67e5\u8be2\u3001\u6b63\u9762\u6587\u6863\u4ee5\u53ca\u7b54\u6848\u751f\u6210\u9ad8\u8d28\u91cf\u91cd\u5199\uff0c\u7136\u540e\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Flan-T5\u6a21\u578b\u4ee5\u5c06\u5bf9\u8bdd\u5386\u53f2\u548c\u67e5\u8be2\u6620\u5c04\u5230\u5408\u6210\u91cd\u5199\u3002\u6700\u540e\uff0c\u4f7f\u7528DPO\u7b97\u6cd5\u901a\u8fc7\u751f\u6210\u5668\u7684\u53cd\u9988\u8fdb\u4e00\u6b65\u63d0\u5347\u91cd\u5199\u5668\u7684\u672b\u7aef\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSynRewrite\u5728TopiOCQA\u548cQRECC\u6570\u636e\u96c6\u7684\u68c0\u7d22\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4eba\u5de5\u91cd\u5199\u3002", "conclusion": "\u5408\u6210\u91cd\u5199\u53ef\u4ee5\u6210\u4e3a\u4eba\u5de5\u6ce8\u91ca\u7684\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.22486", "categories": ["cs.IR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22486", "abs": "https://arxiv.org/abs/2509.22486", "authors": ["Gaurav Bagwe", "Saket S. Chaturvedi", "Xiaolong Ma", "Xiaoyong Yuan", "Kuang-Ching Wang", "Lan Zhang"], "title": "Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks", "comment": "Accepted by EMNLP 2025", "summary": "Retrieval-augmented generation (RAG) enhances factual grounding by\nintegrating retrieval mechanisms with generative models but introduces new\nattack surfaces, particularly through backdoor attacks. While prior research\nhas largely focused on disinformation threats, fairness vulnerabilities remain\nunderexplored. Unlike conventional backdoors that rely on direct\ntrigger-to-target mappings, fairness-driven attacks exploit the interaction\nbetween retrieval and generation models, manipulating semantic relationships\nbetween target groups and social biases to establish a persistent and covert\ninfluence on content generation.\n  This paper introduces BiasRAG, a systematic framework that exposes fairness\nvulnerabilities in RAG through a two-phase backdoor attack. During the\npre-training phase, the query encoder is compromised to align the target group\nwith the intended social bias, ensuring long-term persistence. In the\npost-deployment phase, adversarial documents are injected into knowledge bases\nto reinforce the backdoor, subtly influencing retrieved content while remaining\nundetectable under standard fairness evaluations. Together, BiasRAG ensures\nprecise target alignment over sensitive attributes, stealthy execution, and\nresilience. Empirical evaluations demonstrate that BiasRAG achieves high attack\nsuccess rates while preserving contextual relevance and utility, establishing a\npersistent and evolving threat to fairness in RAG.", "AI": {"tldr": "BiasRAG\u6846\u67b6\u7cfb\u7edf\u6027\u63ed\u9732\u4fe1\u606f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u516c\u5e73\u6027\u6f0f\u6d1e\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u95e8\u653b\u51fb\u5b9e\u73b0\u9ad8\u6548\u4e14\u9690\u853d\u7684\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5728\u4fe1\u606f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\uff0c\u4f20\u7edf\u7684\u540e\u95e8\u653b\u51fb\u4f9d\u8d56\u4e8e\u76f4\u63a5\u7684\u89e6\u53d1\u5230\u76ee\u6807\u7684\u6620\u5c04\uff0c\u800c\u516c\u5e73\u6027\u9a71\u52a8\u7684\u653b\u51fb\u901a\u8fc7\u64cd\u63a7\u76ee\u6807\u7fa4\u4f53\u4e0e\u793e\u4f1a\u504f\u89c1\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u5efa\u7acb\u4e00\u4e2a\u9690\u853d\u4e14\u6301\u4e45\u7684\u5185\u5bb9\u751f\u6210\u5f71\u54cd\uff0c\u7814\u7a76\u5c1a\u672a\u6df1\u5165\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBiasRAG\u7684\u4e24\u9636\u6bb5\u540e\u95e8\u653b\u51fb\u6846\u67b6\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5229\u7528\u67e5\u8be2\u7f16\u7801\u5668\u4f7f\u76ee\u6807\u7fa4\u4f53\u4e0e\u9884\u8bbe\u7684\u793e\u4f1a\u504f\u89c1\u5bf9\u9f50\u3002\u5728\u90e8\u7f72\u540e\u9636\u6bb5\uff0c\u5411\u77e5\u8bc6\u5e93\u4e2d\u6ce8\u5165\u5bf9\u6297\u6027\u6587\u6863\uff0c\u4ee5\u52a0\u5f3a\u540e\u95e8\u653b\u51fb\u3002", "result": "BiasRAG\u6846\u67b6\u5728RAG\u7cfb\u7edf\u4e2d\u9ad8\u6548\u5b9e\u65bd\u540e\u95e8\u653b\u51fb\uff0c\u6210\u529f\u5728\u5185\u5bb9\u751f\u6210\u4e2d\u9690\u853d\u5730\u52a0\u5165\u793e\u4f1a\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "conclusion": "BiasRAG\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u63ed\u9732RAG\u4e2d\u7684\u516c\u5e73\u6027\u6f0f\u6d1e\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff0c\u5e76\u80fd\u591f\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u6548\u7528\u7684\u540c\u65f6\uff0c\u6301\u7eed\u5f71\u54cd\u5185\u5bb9\u751f\u6210\u3002"}}
