{"id": "2509.08919", "categories": ["cs.IR", "cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.08919", "abs": "https://arxiv.org/abs/2509.08919", "authors": ["Mahe Chen", "Xiaoxuan Wang", "Kaiwen Chen", "Nick Koudas"], "title": "Generative Engine Optimization: How to Dominate AI Search", "comment": null, "summary": "The rapid adoption of generative AI-powered search engines like ChatGPT,\nPerplexity, and Gemini is fundamentally reshaping information retrieval, moving\nfrom traditional ranked lists to synthesized, citation-backed answers. This\nshift challenges established Search Engine Optimization (SEO) practices and\nnecessitates a new paradigm, which we term Generative Engine Optimization\n(GEO).\n  This paper presents a comprehensive comparative analysis of AI Search and\ntraditional web search (Google). Through a series of large-scale, controlled\nexperiments across multiple verticals, languages, and query paraphrases, we\nquantify critical differences in how these systems source information. Our key\nfindings reveal that AI Search exhibit a systematic and overwhelming bias\ntowards Earned media (third-party, authoritative sources) over Brand-owned and\nSocial content, a stark contrast to Google's more balanced mix. We further\ndemonstrate that AI Search services differ significantly from each other in\ntheir domain diversity, freshness, cross-language stability, and sensitivity to\nphrasing.\n  Based on these empirical results, we formulate a strategic GEO agenda. We\nprovide actionable guidance for practitioners, emphasizing the critical need\nto: (1) engineer content for machine scannability and justification, (2)\ndominate earned media to build AI-perceived authority, (3) adopt\nengine-specific and language-aware strategies, and (4) overcome the inherent\n\"big brand bias\" for niche players. Our work provides the foundational\nempirical analysis and a strategic framework for achieving visibility in the\nnew generative search landscape."}
{"id": "2509.09037", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09037", "abs": "https://arxiv.org/abs/2509.09037", "authors": ["Amanda Aird", "Ben Armstrong", "Nicholas Mattei", "Robin Burke"], "title": "Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in Personalized Recommendation", "comment": null, "summary": "Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have\nbeen used as fairness concepts in the economics, game theory, and social choice\nliteratures since the 1960s, and have recently gained popularity within the\nrecommendation systems communities. In this short position paper we will give\nan overview of envy-freeness and its use in economics and recommendation\nsystems; and illustrate why envy is not appropriate to measure fairness for use\nin settings where personalization plays a role."}
{"id": "2509.09114", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09114", "abs": "https://arxiv.org/abs/2509.09114", "authors": ["Kelin Ren", "Chan-Yang Ju", "Dong-Ho Lee"], "title": "Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation", "comment": "Accepted by CIKM 2025", "summary": "Multimodal recommendation systems are increasingly becoming foundational\ntechnologies for e-commerce and content platforms, enabling personalized\nservices by jointly modeling users' historical behaviors and the multimodal\nfeatures of items (e.g., visual and textual). However, most existing methods\nrely on either static fusion strategies or graph-based local interaction\nmodeling, facing two critical limitations: (1) insufficient ability to model\nfine-grained cross-modal associations, leading to suboptimal fusion quality;\nand (2) a lack of global distribution-level consistency, causing\nrepresentational bias. To address these, we propose MambaRec, a novel framework\nthat integrates local feature alignment and global distribution regularization\nvia attention-guided learning. At its core, we introduce the Dilated Refinement\nAttention Module (DREAM), which uses multi-scale dilated convolutions with\nchannel-wise and spatial attention to align fine-grained semantic patterns\nbetween visual and textual modalities. This module captures hierarchical\nrelationships and context-aware associations, improving cross-modal semantic\nmodeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive\nloss functions to constrain global modality alignment, enhancing semantic\nconsistency. This dual regularization reduces mode-specific deviations and\nboosts robustness. To improve scalability, MambaRec employs a dimensionality\nreduction strategy to lower the computational cost of high-dimensional\nmultimodal features. Extensive experiments on real-world e-commerce datasets\nshow that MambaRec outperforms existing methods in fusion quality,\ngeneralization, and efficiency. Our code has been made publicly available at\nhttps://github.com/rkl71/MambaRec."}
{"id": "2509.09342", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09342", "abs": "https://arxiv.org/abs/2509.09342", "authors": ["Yifan Wang", "Shen Gao", "Jiabao Fang", "Rui Yan", "Billy Chiu", "Shuo Shang"], "title": "CESRec: Constructing Pseudo Interactions for Sequential Recommendation via Conversational Feedback", "comment": null, "summary": "Sequential Recommendation Systems (SRS) have become essential in many\nreal-world applications. However, existing SRS methods often rely on\ncollaborative filtering signals and fail to capture real-time user preferences,\nwhile Conversational Recommendation Systems (CRS) excel at eliciting immediate\ninterests through natural language interactions but neglect historical\nbehavior. To bridge this gap, we propose CESRec, a novel framework that\nintegrates the long-term preference modeling of SRS with the real-time\npreference elicitation of CRS. We introduce semantic-based pseudo interaction\nconstruction, which dynamically updates users'historical interaction sequences\nby analyzing conversational feedback, generating a pseudo-interaction sequence\nthat seamlessly combines long-term and real-time preferences. Additionally, we\nreduce the impact of outliers in historical items that deviate from users'core\npreferences by proposing dual alignment outlier items masking, which identifies\nand masks such items using semantic-collaborative aligned representations.\nExtensive experiments demonstrate that CESRec achieves state-of-the-art\nperformance by boosting strong SRS models, validating its effectiveness in\nintegrating conversational feedback into SRS."}
{"id": "2509.09414", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09414", "abs": "https://arxiv.org/abs/2509.09414", "authors": ["Alan Said", "Maria Soledad Pera", "Michael D. Ekstrand"], "title": "We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later", "comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  accepted for publication in the Beyond Algorithms: Reclaiming the\n  Interdisciplinary Roots of Recommender Systems Workshop (BEYOND 2025),\n  September 26th, 2025, co-located with the 19th ACM Recommender Systems\n  Conference, Prague, Czech Republic", "summary": "In 2011, Xavier Amatriain sounded the alarm: recommender systems research was\n\"doing it all wrong\" [1]. His critique, rooted in statistical misinterpretation\nand methodological shortcuts, remains as relevant today as it was then. But\nrather than correcting course, we added new layers of sophistication on top of\nthe same broken foundations. This paper revisits Amatriain's diagnosis and\nargues that many of the conceptual, epistemological, and infrastructural\nfailures he identified still persist, in more subtle or systemic forms. Drawing\non recent work in reproducibility, evaluation methodology, environmental\nimpact, and participatory design, we showcase how the field's accelerating\ncomplexity has outpaced its introspection. We highlight ongoing community-led\ninitiatives that attempt to shift the paradigm, including workshops, evaluation\nframeworks, and calls for value-sensitive and participatory research. At the\nsame time, we contend that meaningful change will require not only new metrics\nor better tooling, but a fundamental reframing of what recommender systems\nresearch is for, who it serves, and how knowledge is produced and validated.\nOur call is not just for technical reform, but for a recommender systems\nresearch agenda grounded in epistemic humility, human impact, and sustainable\npractice."}
{"id": "2509.09459", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09459", "abs": "https://arxiv.org/abs/2509.09459", "authors": ["Chao Huang", "Fengran Mo", "Yufeng Chen", "Changhao Guan", "Zhenrui Yue", "Xinyu Wang", "Jinan Xu", "Kaiyu Huang"], "title": "Boosting Data Utilization for Multilingual Dense Retrieval", "comment": "Accepted by EMNLP 2025 (main)", "summary": "Multilingual dense retrieval aims to retrieve relevant documents across\ndifferent languages based on a unified retriever model. The challenge lies in\naligning representations of different languages in a shared vector space. The\ncommon practice is to fine-tune the dense retriever via contrastive learning,\nwhose effectiveness highly relies on the quality of the negative sample and the\nefficacy of mini-batch data. Different from the existing studies that focus on\ndeveloping sophisticated model architecture, we propose a method to boost data\nutilization for multilingual dense retrieval by obtaining high-quality hard\nnegative samples and effective mini-batch data. The extensive experimental\nresults on a multilingual retrieval benchmark, MIRACL, with 16 languages\ndemonstrate the effectiveness of our method by outperforming several existing\nstrong baselines."}
{"id": "2509.09622", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09622", "abs": "https://arxiv.org/abs/2509.09622", "authors": ["Shashank Gupta"], "title": "AskDoc -- Identifying Hidden Healthcare Disparities", "comment": null, "summary": "The objective of this study is to understand the online Ask the Doctor\nservices medical advice on internet platforms via AskDoc, a Reddit community\nthat serves as a public AtD platform and study if platforms mirror existing\nhurdles and partiality in healthcare across various demographic groups. We\ndownloaded data from January 2020 to May 2022 from AskDoc -- a subreddit, and\ncreated regular expressions to identify self-reported demographics (Gender,\nRace, and Age) from the posts, and performed statistical analysis to understand\nthe interaction between peers and physicians with the posters. Half of the\nposts did not receive comments from peers or physicians. At least 90% of the\npeople disclose their gender and age, and 80% of the people do not disclose\ntheir race. It was observed that the subreddit is dominated by adult (age group\n20-39) white males. Some disparities were observed in the engagement between\nthe users and the posters with certain demographics. Beyond the confines of\nclinics and hospitals, social media could bring patients and providers closer\ntogether, however, as observed, current physicians participation is low\ncompared to posters."}
{"id": "2509.09651", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09651", "abs": "https://arxiv.org/abs/2509.09651", "authors": ["Zakaria El Kassimi", "Fares Fourati", "Mohamed-Slim Alouini"], "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations", "comment": null, "summary": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG."}
