{"id": "2509.03661", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03661", "abs": "https://arxiv.org/abs/2509.03661", "authors": ["Daryl Chang", "Yi Wu", "Jennifer She", "Li Wei", "Lukasz Heldt"], "title": "ACT: Automated Constraint Targeting for Multi-Objective Recommender Systems", "comment": null, "summary": "Recommender systems often must maximize a primary objective while ensuring\nsecondary ones satisfy minimum thresholds, or \"guardrails.\" This is critical\nfor maintaining a consistent user experience and platform ecosystem, but\nenforcing these guardrails despite orthogonal system changes is challenging and\noften requires manual hyperparameter tuning. We introduce the Automated\nConstraint Targeting (ACT) framework, which automatically finds the minimal set\nof hyperparameter changes needed to satisfy these guardrails. ACT uses an\noffline pairwise evaluation on unbiased data to find solutions and continuously\nretrains to adapt to system and user behavior changes. We empirically\ndemonstrate its efficacy and describe its deployment in a large-scale\nproduction environment.", "AI": {"tldr": "\u5f15\u5165\u4e00\u79cd\u81ea\u52a8\u5316\u7ea6\u675f\u76ee\u6807\uff08ACT\uff09\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u8c03\u6574\u63a8\u8350\u7cfb\u7edf\u7684\u8d85\u53c2\u6570\uff0c\u4ee5\u6ee1\u8db3\u6b21\u8981\u76ee\u6807\u7684\u6700\u4f4e\u8981\u6c42\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u5728\u6700\u5927\u5316\u4e3b\u8981\u76ee\u6807\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u6b21\u8981\u76ee\u6807\u6ee1\u8db3\u6700\u4f4e\u9608\u503c\u3002\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u7684\u6210\u5bf9\u8bc4\u4f30\u5728\u65e0\u504f\u6570\u636e\u4e0a\u627e\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e0d\u65ad\u91cd\u65b0\u8bad\u7ec3\u4ee5\u9002\u5e94\u7cfb\u7edf\u548c\u7528\u6237\u884c\u4e3a\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86ACT\u7684\u6709\u6548\u6027\uff0c\u5e76\u63cf\u8ff0\u4e86\u5b83\u5728\u5927\u578b\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "conclusion": "ACT\u6846\u67b6\u53ef\u4ee5\u81ea\u52a8\u8c03\u6574\u63a8\u8350\u7cfb\u7edf\u7684\u8d85\u53c2\u6570\uff0c\u4ece\u800c\u4fdd\u8bc1\u6b21\u8981\u76ee\u6807\u7684\u7ea6\u675f\u6761\u4ef6\u5728\u53d8\u5316\u7684\u7cfb\u7edf\u73af\u5883\u4e2d\u5f97\u5230\u6ee1\u8db3\u3002"}}
{"id": "2509.03692", "categories": ["cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03692", "abs": "https://arxiv.org/abs/2509.03692", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann"], "title": "lifeXplore at the Lifelog Search Challenge 2021", "comment": null, "summary": "Since its first iteration in 2018, the Lifelog Search Challenge (LSC)\ncontinues to rise in popularity as an interactive lifelog data retrieval\ncompetition, co-located at the ACM International Conference on Multimedia\nRetrieval (ICMR). The goal of this annual live event is to search a large\ncorpus of lifelogging data for specifically announced memories using a\npurposefully developed tool within a limited amount of time. As long-standing\nparticipants, we present our improved lifeXplore - a retrieval system combining\nchronologic day summary browsing with interactive combinable concept filtering.\nCompared to previous versions, the tool is improved by incorporating temporal\nqueries, advanced day summary features as well as usability improvements.", "AI": {"tldr": "LSC\u6bd4\u8d5b\u4e2d\uff0c\u6539\u8fdb\u7248lifeXplore\u63d0\u5347\u4e86lifelog\u6570\u636e\u68c0\u7d22\u7cfb\u7edf\u7684\u529f\u80fd\u3002", "motivation": "\u63d0\u9ad8LSC\u6bd4\u8d5b\u4e2dlifelog\u6570\u636e\u68c0\u7d22\u7684\u6548\u7387\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u67e5\u8be2\u548c\u4e92\u52a8\u53ef\u7ec4\u5408\u6982\u5ff5\u8fc7\u6ee4\u6280\u672f\u6539\u8fdblifelog\u6570\u636e\u68c0\u7d22\u5de5\u5177\u3002", "result": "\u5f00\u53d1\u4e86\u6539\u8fdb\u7248\u7684lifeXplore\u68c0\u7d22\u7cfb\u7edf\uff0c\u589e\u52a0\u4e86\u65f6\u5e8f\u67e5\u8be2\u3001\u5148\u8fdb\u7684\u6bcf\u65e5\u6458\u8981\u529f\u80fd\u4ee5\u53ca\u53ef\u7528\u6027\u6539\u8fdb\u3002", "conclusion": "\u65b0\u7248\u672c\u7684lifeXplore\u5728LSC\u6bd4\u8d5b\u4e2d\u7684\u8868\u73b0\u53ef\u80fd\u66f4\u4e3a\u4f18\u5f02\u3002"}}
{"id": "2509.03696", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.03696", "abs": "https://arxiv.org/abs/2509.03696", "authors": ["Aleksandr V. Petrov", "Michael Murtagh", "Karthik Nagesh"], "title": "LLMs for estimating positional bias in logged interaction data", "comment": "Accepted at the CONSEQUENCES Workshop @ RecSys'25", "summary": "Recommender and search systems commonly rely on Learning To Rank models\ntrained on logged user interactions to order items by predicted relevance.\nHowever, such interaction data is often subject to position bias, as users are\nmore likely to click on items that appear higher in the ranking, regardless of\ntheir actual relevance. As a result, newly trained models may inherit and\nreinforce the biases of prior ranking models rather than genuinely improving\nrelevance. A standard approach to mitigate position bias is Inverse Propensity\nScoring (IPS), where the model's loss is weighted by the inverse of a\npropensity function, an estimate of the probability that an item at a given\nposition is examined. However, accurate propensity estimation is challenging,\nespecially in interfaces with complex non-linear layouts. In this paper, we\npropose a novel method for estimating position bias using Large Language Models\n(LLMs) applied to logged user interaction data. This approach offers a\ncost-effective alternative to online experimentation. Our experiments show that\npropensities estimated with our LLM-as-a-judge approach are stable across score\nbuckets and reveal the row-column effects of Viator's grid layout that simpler\nheuristics overlook. An IPS-weighted reranker trained with these propensities\nmatches the production model on standard NDCG@10 while improving weighted\nNDCG@10 by roughly 2%. We will verify these offline gains in forthcoming\nlive-traffic experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u6539\u5584\u6392\u5e8f\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u56e0\u4f4d\u7f6e\u504f\u5dee\u5bfc\u81f4\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\u7ee7\u627f\u524d\u6a21\u578b\u7684\u504f\u89c1\uff0c\u672a\u80fd\u771f\u6b63\u63d0\u9ad8\u76f8\u5173\u6027\uff0c\u8be5\u6587\u63d0\u51fa\u65b0\u7684\u4f30\u8ba1\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u4f30\u8ba1\u4f4d\u7f6e\u504f\u5dee\u7684\u65b9\u6cd5\uff0c\u66ff\u4ee3\u590d\u6742\u7684\u7ebf\u4e0a\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5bf9\u7528\u6237\u4ea4\u4e92\u6570\u636e\u7684\u6a21\u578b\u8bc4\u4f30\u6765\u8fdb\u884c\u6743\u91cd\u8c03\u6574\u3002", "result": "\u6240\u63d0\u51fa\u7684\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4f4d\u7f6e\u504f\u5dee\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4e8e\u7b80\u5355\u542f\u53d1\u5f0f\u80fd\u591f\u66f4\u7a33\u5b9a\u5730\u4f30\u8ba1\u4f4d\u7f6e\u6982\u7387\uff0c\u5e76\u63ed\u793a\u590d\u6742\u754c\u9762\u5e03\u5c40\u7684\u5f71\u54cd\uff0c\u5728\u52a0\u6743NDCG@10\u63d0\u5347\u7ea62%\u3002", "conclusion": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8bb0\u5f55\u7684\u7528\u6237\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u4f4d\u7f6e\u504f\u5dee\u4f30\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u6539\u5584\u63a8\u8350\u6392\u5e8f\u8d28\u91cf\uff0c\u5e76\u80fd\u5728\u79bb\u7ebf\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e0e\u751f\u4ea7\u6a21\u578b\u76f8\u5f53\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5728\u52a0\u6743NDCG@10\u4e0a\u63d0\u5347\u5927\u7ea62%\u3002"}}
{"id": "2509.03746", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.03746", "abs": "https://arxiv.org/abs/2509.03746", "authors": ["Anushya Subbiah", "Vikram Aggarwal", "James Pine", "Steffen Rendle", "Krishna Sayana", "Kun Su"], "title": "Efficient Item ID Generation for Large-Scale LLM-based Recommendation", "comment": null, "summary": "Integrating product catalogs and user behavior into LLMs can enhance\nrecommendations with broad world knowledge, but the scale of real-world item\ncatalogs, often containing millions of discrete item identifiers (Item IDs),\nposes a significant challenge. This contrasts with the smaller, tokenized text\nvocabularies typically used in LLMs. The predominant view within the LLM-based\nrecommendation literature is that it is infeasible to treat item ids as a first\nclass citizen in the LLM and instead some sort of tokenization of an item into\nmultiple tokens is required. However, this creates a key practical bottleneck\nin serving these models for real-time low-latency applications.\n  Our paper challenges this predominant practice and integrates item ids as\nfirst class citizens into the LLM. We provide simple, yet highly effective,\nnovel training and inference modifications that enable single-token\nrepresentations of items and single-step decoding. Our method shows\nimprovements in recommendation quality (Recall and NDCG) over existing\ntechniques on the Amazon shopping datasets while significantly improving\ninference efficiency by 5x-14x. Our work offers an efficiency perspective\ndistinct from that of other popular approaches within LLM-based recommendation,\npotentially inspiring further research and opening up a new direction for\nintegrating IDs into LLMs. Our code is available here\nhttps://drive.google.com/file/d/1cUMj37rV0Z1bCWMdhQ6i4q4eTRQLURtC", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5c06\u9879\u76eeID\u76f4\u63a5\u96c6\u6210\u5230LLM\u4e2d\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u6548\u7387\u53ca\u6027\u80fd\u3002", "motivation": "\u5728\u9879\u76ee\u63a8\u8350\u4e2d\uff0c\u73b0\u6709\u6280\u672f\u5c06\u9879\u76eeID\u5206\u89e3\u6210\u591a\u4e2a\u6807\u8bb0\u7684\u65b9\u6cd5\u9762\u4e34\u6548\u7387\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u65b0\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u4fee\u6b63\uff0c\u652f\u6301\u9879\u76ee\u5355\u6807\u8bb0\u8868\u793a\u53ca\u5355\u6b65\u89e3\u7801\u3002", "result": "\u5728\u4e9a\u9a6c\u900a\u8d2d\u7269\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u8350\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5728\u63a8\u7406\u6548\u7387\u4e0a\u63d0\u9ad8\u4e865\u81f314\u500d\u3002", "conclusion": "\u672c\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u5c06\u9879\u76eeID\u4f5c\u4e3aLLM\u4e2d\u7684\u4e00\u7b49\u516c\u6c11\u8fdb\u884c\u5904\u7406\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u63a8\u8350\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2509.03764", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03764", "abs": "https://arxiv.org/abs/2509.03764", "authors": ["Han Wang", "Alex Whitworth", "Pak Ming Cheung", "Zhenjie Zhang", "Krishna Kamath"], "title": "LLM-based Relevance Assessment for Web-Scale Search Evaluation at Pinterest", "comment": "RecSys 2025 EARL Workshop", "summary": "Relevance evaluation plays a crucial role in personalized search systems to\nensure that search results align with a user's queries and intent. While human\nannotation is the traditional method for relevance evaluation, its high cost\nand long turnaround time limit its scalability. In this work, we present our\napproach at Pinterest Search to automate relevance evaluation for online\nexperiments using fine-tuned LLMs. We rigorously validate the alignment between\nLLM-generated judgments and human annotations, demonstrating that LLMs can\nprovide reliable relevance measurement for experiments while greatly improving\nthe evaluation efficiency. Leveraging LLM-based labeling further unlocks the\nopportunities to expand the query set, optimize sampling design, and\nefficiently assess a wider range of search experiences at scale. This approach\nleads to higher-quality relevance metrics and significantly reduces the Minimum\nDetectable Effect (MDE) in online experiment measurements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5fae\u8c03\u7684LLM\u81ea\u52a8\u8fdb\u884c\u641c\u7d22\u76f8\u5173\u6027\u8bc4\u4f30\uff0c\u53ef\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u5ab2\u7f8e\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\u7684MDE\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7c7b\u6ce8\u91ca\u76f8\u5173\u6027\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u8017\u65f6\u957f\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u4f7f\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684LLM\u81ea\u52a8\u8fdb\u884c\u76f8\u5173\u6027\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u4eba\u7c7b\u6ce8\u91ca\u8fdb\u884c\u4e25\u8c28\u7684\u9a8c\u8bc1\u6bd4\u8f83\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u76f8\u5173\u6027\u5ea6\u91cf\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5728\u7ebf\u5b9e\u9a8c\u6d4b\u91cf\u4e2d\u7684\u6700\u4f4e\u53ef\u68c0\u6d4b\u6548\u5e94(MDE)\u3002", "conclusion": "\u4f7f\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684LLM\u53ef\u4ee5\u81ea\u52a8\u8fdb\u884c\u76f8\u5173\u6027\u8bc4\u4f30\uff0c\u4e0e\u4eba\u7c7b\u6ce8\u91ca\u7ed3\u679c\u5177\u6709\u826f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\u3002"}}
{"id": "2509.03787", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03787", "abs": "https://arxiv.org/abs/2509.03787", "authors": ["Shakiba Amirshahi", "Amin Bigdeli", "Charles L. A. Clarke", "Amira Ghenai"], "title": "Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain", "comment": null, "summary": "Retrieval augmented generation (RAG) systems provide a method for factually\ngrounding the responses of a Large Language Model (LLM) by providing retrieved\nevidence, or context, as support. Guided by this context, RAG systems can\nreduce hallucinations and expand the ability of LLMs to accurately answer\nquestions outside the scope of their training data. Unfortunately, this design\nintroduces a critical vulnerability: LLMs may absorb and reproduce\nmisinformation present in retrieved evidence. This problem is magnified if\nretrieved evidence contains adversarial material explicitly intended to\npromulgate misinformation. This paper presents a systematic evaluation of RAG\nrobustness in the health domain and examines alignment between model outputs\nand ground-truth answers. We focus on the health domain due to the potential\nfor harm caused by incorrect responses, as well as the availability of\nevidence-based ground truth for many common health-related questions. We\nconduct controlled experiments using common health questions, varying both the\ntype and composition of the retrieved documents (helpful, harmful, and\nadversarial) as well as the framing of the question by the user (consistent,\nneutral, and inconsistent). Our findings reveal that adversarial documents\nsubstantially degrade alignment, but robustness can be preserved when helpful\nevidence is also present in the retrieval pool. These findings offer actionable\ninsights for designing safer RAG systems in high-stakes domains by highlighting\nthe need for retrieval safeguards. To enable reproducibility and facilitate\nfuture research, all experimental results are publicly available in our github\nrepository.\n  https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL", "AI": {"tldr": "\u7814\u7a76RAG\u7cfb\u7edf\u5728\u5065\u5eb7\u9886\u57df\u7684\u5065\u58ee\u6027\uff0c\u68c0\u6d4b\u6a21\u578b\u8f93\u51fa\u5bf9\u771f\u5b9e\u7b54\u6848\u7684\u5bf9\u9f50\u5ea6\uff0c\u53d1\u73b0\u6709\u76ca\u8bc1\u636e\u53ef\u7f13\u89e3\u5bf9\u6297\u6027\u6587\u6863\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8RAG\u7cfb\u7edf\u5728\u5065\u5eb7\u9886\u57df\u7684\u5065\u58ee\u6027\uff0c\u7814\u7a76\u6a21\u578b\u8f93\u51fa\u4e0e\u771f\u5b9e\u7b54\u6848\u7684\u5bf9\u9f50\u6027\u3002\u8003\u8651\u5230\u9519\u8bef\u56de\u7b54\u53ef\u80fd\u9020\u6210\u7684\u4f24\u5bb3\u4ee5\u53ca\u8bb8\u591a\u5065\u5eb7\u76f8\u5173\u95ee\u9898\u6709\u771f\u5b9e\u4f9d\u636e\u3002", "method": "\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5e38\u89c1\u5065\u5eb7\u95ee\u9898\uff0c\u53d8\u66f4\u68c0\u7d22\u6587\u6863\u7684\u7c7b\u578b\u548c\u6784\u6210\uff08\u6709\u76ca\u3001\u6709\u5bb3\u548c\u5bf9\u6297\u6027\uff09\uff0c\u4ee5\u53ca\u7528\u6237\u63d0\u95ee\u7684\u6846\u67b6\uff08\u4e00\u81f4\u3001\u4e2d\u7acb\u548c\u4e0d\u4e00\u81f4\uff09", "result": "\u5bf9\u6297\u6027\u6587\u6863\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u6027\uff0c\u4f46\u5728\u68c0\u7d22\u6c60\u4e2d\u4e5f\u5b58\u5728\u6709\u76ca\u8bc1\u636e\u65f6\u53ef\u4ee5\u4fdd\u7559\u7a33\u5065\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u66f4\u5b89\u5168RAG\u7cfb\u7edf\u7684\u53ef\u884c\u5efa\u8bae\u3002", "conclusion": "\u63d0\u51fa\u9700\u8981\u68c0\u7d22\u4fdd\u969c\uff0c\u4ee5\u8bbe\u8ba1\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u66f4\u5b89\u5168\u7684RAG\u7cfb\u7edf\u3002\u6240\u6709\u5b9e\u9a8c\u7ed3\u679c\u5747\u516c\u5f00\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.04011", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04011", "abs": "https://arxiv.org/abs/2509.04011", "authors": ["Or Shachar", "Uri Katz", "Yoav Goldberg", "Oren Glickman"], "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings", "comment": "Findings of EMNLP 2025", "summary": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever", "AI": {"tldr": "NER Retriever\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u547d\u540d\u5b9e\u4f53\u68c0\u7d22\u6846\u67b6\uff0c\u5728\u65e0\u9700\u9884\u5148\u5b9a\u4e49\u7c7b\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5b9e\u4f53\u91cd\u68c0\u7d22\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "motivation": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u7684\u67d0\u4e9b\u573a\u666f\u4e2d\uff0c\u611f\u5174\u8da3\u7684\u5b9e\u4f53\u7c7b\u578b\u5728\u4e8b\u524d\u4e0d\u80fd\u9884\u77e5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96f6\u6837\u672c\u68c0\u7d22\u6846\u67b6\u6765\u5904\u7406\u6b64\u7c7b\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5185\u90e8\u8868\u793a\uff0c\u5c06\u5b9e\u4f53\u63d0\u53ca\u548c\u7528\u6237\u5b9a\u4e49\u7684\u5f00\u653e\u5f0f\u7c7b\u578b\u63cf\u8ff0\u5d4c\u5165\u5230\u5171\u4eab\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u5bf9\u6bd4\u6295\u5f71\u7f51\u7edc\u6765\u7ec6\u5316\u8fd9\u4e9b\u8868\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cNER Retriever\u5728\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u9009\u62e9LLMs\u5185\u90e8\u8868\u793a\u7684\u6709\u6548\u6027\u3002", "conclusion": "NER Retriever\u5927\u5e45\u4f18\u4e8e\u8bcd\u6c47\u548c\u7a20\u5bc6\u53e5\u5b50\u7ea7\u91cd\u68c0\u7d22\u57fa\u51c6\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u65e0\u6a21\u5f0f\u5b9e\u4f53\u91cd\u68c0\u7d22\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04052", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.04052", "abs": "https://arxiv.org/abs/2509.04052", "authors": ["Sueun Hong", "Shuojie Fu", "Ovidiu Serban", "Brianna Bao", "James Kinross", "Francesa Toni", "Guy Martin", "Uddhav Vaghela"], "title": "Safeguarding Patient Trust in the Age of AI: Tackling Health Misinformation with Explainable AI", "comment": null, "summary": "AI-generated health misinformation poses unprecedented threats to patient\nsafety and healthcare system trust globally. This white paper presents an\nexplainable AI framework developed through the EPSRC INDICATE project to combat\nmedical misinformation while enhancing evidence-based healthcare delivery. Our\nsystematic review of 17 studies reveals the urgent need for transparent AI\nsystems in healthcare. The proposed solution demonstrates 95% recall in\nclinical evidence retrieval and integrates novel trustworthiness classifiers\nachieving 76% F1 score in detecting biomedical misinformation. Results show\nthat explainable AI can transform traditional 6-month expert review processes\ninto real-time, automated evidence synthesis while maintaining clinical rigor.\nThis approach offers a critical intervention to preserve healthcare integrity\nin the AI era.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684AI\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u533b\u7597\u9519\u8bef\u4fe1\u606f\uff0c\u5e76\u63d0\u5347\u5faa\u8bc1\u533b\u7597\u670d\u52a1\u3002", "motivation": "AI\u751f\u6210\u7684\u5065\u5eb7\u8bef\u5bfc\u4fe1\u606f\u5bf9\u60a3\u8005\u5b89\u5168\u548c\u5168\u7403\u533b\u7597\u7cfb\u7edf\u7684\u4fe1\u4efb\u6784\u6210\u524d\u6240\u672a\u6709\u7684\u5a01\u80c1\u3002", "method": "\u901a\u8fc7EPSRC INDICATE\u9879\u76ee\u5f00\u53d1\u7684\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u8fdb\u884c\u4e8617\u9879\u7814\u7a76\u7684\u7cfb\u7edf\u7efc\u8ff0\u4ee5\u8bc4\u4f30\u900f\u660eAI\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u81ea\u52a8\u5316\u7684\u8bc1\u636e\u7efc\u5408\u3002", "result": "\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5728\u4e34\u5e8a\u8bc1\u636e\u68c0\u7d22\u65b9\u9762\u663e\u793a\u51fa95%\u7684\u53ec\u56de\u7387\uff0c\u5e76\u4e14\u5728\u68c0\u6d4b\u751f\u7269\u533b\u5b66\u8bef\u5bfc\u4fe1\u606f\u65b9\u9762\u96c6\u6210\u4e86\u65b0\u7684\u53ef\u4fe1\u5ea6\u5206\u7c7b\u5668\uff0c\u8fbe\u5230\u4e8676%\u7684F1\u5206\u6570\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u5728AI\u65f6\u4ee3\u7ef4\u62a4\u533b\u7597\u4fdd\u5065\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2509.04139", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04139", "abs": "https://arxiv.org/abs/2509.04139", "authors": ["Songjiang Lai", "Tsun-Hin Cheung", "Ka-Chun Fung", "Kaiwen Xue", "Kwan-Ho Lin", "Yan-Ming Choi", "Vincent Ng", "Kin-Man Lam"], "title": "Enhancing Technical Documents Retrieval for RAG", "comment": null, "summary": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.", "AI": {"tldr": "Technical-Embeddings\u901a\u8fc7\u6269\u5c55\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\u603b\u7ed3\u63d0\u5347\u6280\u672f\u6587\u6863\u68c0\u7d22\u6548\u679c\uff0c\u7279\u522b\u5728\u5de5\u7a0b\u548c\u4ea7\u54c1\u5f00\u53d1\u4e2d\u5e94\u7528\u5e7f\u6cdb\u3002", "motivation": "\u89e3\u51b3\u6280\u672f\u6587\u6863\u4e2d\u590d\u6742\u5185\u5bb9\u7684\u8bed\u4e49\u68c0\u7d22\u6311\u6218\uff0c\u501f\u52a9LLMs\u63d0\u5347\u7406\u89e3\u5e76\u4f18\u5316\u4fe1\u606f\u68c0\u7d22\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u548c\u589e\u5f3a\u53cc\u7f16\u7801BERT\u6a21\u578b\uff0c\u7ed3\u5408\u8f6f\u63d0\u793a\u6280\u672f\u5b9e\u73b0\u67e5\u8be2\u548c\u6587\u6863\u4e0a\u4e0b\u6587\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63d0\u53d6\u3002", "result": "Technical-Embeddings\u5728RAG-EDA\u548cRust-Docs-QA\u4e24\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u51fa\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u878d\u5408\u67e5\u8be2\u6269\u5c55\u548c\u6587\u6863\u4e0a\u4e0b\u6587\u603b\u7ed3\uff0cTechnical-Embeddings\u80fd\u591f\u5728\u6280\u672f\u9886\u57df\u4e2d\u663e\u8457\u63d0\u9ad8\u4fe1\u606f\u8bbf\u95ee\u548c\u7406\u89e3\u7cbe\u51c6\u5ea6\u3002"}}
{"id": "2509.04330", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.04330", "abs": "https://arxiv.org/abs/2509.04330", "authors": ["Tian Miao"], "title": "Temporal Interest-Driven Multimodal Personalized Content Generation", "comment": null, "summary": "With the dynamic evolution of user interests and the increasing multimodal\ndemands in internet applications, personalized content generation strategies\nbased on static interest preferences struggle to meet practical application\nrequirements. The proposed TIMGen (Temporal Interest-driven Multimodal\nGeneration) model addresses this challenge by modeling the long-term temporal\nevolution of users' interests and capturing dynamic interest representations\nwith strong temporal dependencies. This model also supports the fusion of\nmultimodal features, such as text, images, video, and audio, and delivers\ncustomized content based on multimodal preferences. TIMGen jointly learns\ntemporal dependencies and modal preferences to obtain a unified interest\nrepresentation, which it then generates to meet users' personalized content\nneeds. TIMGen overcomes the shortcomings of personalized content recommendation\nmethods based on static preferences, enabling flexible and dynamic modeling of\nusers' multimodal interests, better understanding and capturing their interests\nand preferences. It can be extended to a variety of practical application\nscenarios, including e-commerce, advertising, online education, and precision\nmedicine, providing insights for future research.", "AI": {"tldr": "TIMGen\u6a21\u578b\u89e3\u51b3\u4e86\u52a8\u6001\u5174\u8da3\u6f14\u53d8\u548c\u591a\u6a21\u6001\u9700\u6c42\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5174\u8da3\u8868\u793a\u751f\u6210\u4e2a\u6027\u5316\u5185\u5bb9\uff0c\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u573a\u666f\u3002", "motivation": "\u968f\u7740\u7528\u6237\u5174\u8da3\u7684\u52a8\u6001\u6f14\u53d8\u548c\u4e92\u8054\u7f51\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u9700\u6c42\u7684\u589e\u52a0\uff0c\u57fa\u4e8e\u9759\u6001\u5174\u8da3\u504f\u597d\u7684\u4e2a\u6027\u5316\u5185\u5bb9\u751f\u6210\u7b56\u7565\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u8981\u6c42\u3002", "method": "TIMGen\u6a21\u578b\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u5174\u8da3\u7684\u957f\u671f\u65f6\u95f4\u6f14\u53d8\u548c\u6355\u83b7\u5177\u6709\u5f3a\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u52a8\u6001\u5174\u8da3\u8868\u793a\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u8be5\u6a21\u578b\u652f\u6301\u591a\u6a21\u6001\u7279\u5f81\u7684\u878d\u5408\uff0c\u5e76\u57fa\u4e8e\u591a\u6a21\u6001\u504f\u597d\u63d0\u4f9b\u5b9a\u5236\u5185\u5bb9\u3002", "result": "TIMGen\u514b\u670d\u4e86\u57fa\u4e8e\u9759\u6001\u504f\u597d\u7684\u4e2a\u6027\u5316\u5185\u5bb9\u63a8\u8350\u65b9\u6cd5\u7684\u7f3a\u70b9\uff0c\u80fd\u591f\u7075\u6d3b\u548c\u52a8\u6001\u5730\u5efa\u6a21\u7528\u6237\u7684\u591a\u6a21\u6001\u5174\u8da3\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7406\u89e3\u548c\u6355\u83b7\u7528\u6237\u5174\u8da3\u548c\u504f\u597d\u7684\u80fd\u529b\u3002", "conclusion": "TIMGen\u6a21\u578b\u80fd\u591f\u6269\u5c55\u5230\u5305\u62ec\u7535\u5b50\u5546\u52a1\u3001\u5e7f\u544a\u3001\u5728\u7ebf\u6559\u80b2\u548c\u7cbe\u51c6\u533b\u7597\u7b49\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.04337", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04337", "abs": "https://arxiv.org/abs/2509.04337", "authors": ["Jie Liu", "Yinrui Li", "Jiankai Sun", "Kungang Li", "Han Sun", "Sihan Wang", "Huasen Wu", "Siyuan Gao", "Paulo Soares", "Nan Li", "Zhifang Liu", "Haoyang Li", "Siping Ji", "Ling Leng", "Prathibha Deshikachar"], "title": "Decoupled Entity Representation Learning for Pinterest Ads Ranking", "comment": null, "summary": "In this paper, we introduce a novel framework following an\nupstream-downstream paradigm to construct user and item (Pin) embeddings from\ndiverse data sources, which are essential for Pinterest to deliver personalized\nPins and ads effectively. Our upstream models are trained on extensive data\nsources featuring varied signals, utilizing complex architectures to capture\nintricate relationships between users and Pins on Pinterest. To ensure\nscalability of the upstream models, entity embeddings are learned, and\nregularly refreshed, rather than real-time computation, allowing for\nasynchronous interaction between the upstream and downstream models. These\nembeddings are then integrated as input features in numerous downstream tasks,\nincluding ad retrieval and ranking models for CTR and CVR predictions. We\ndemonstrate that our framework achieves notable performance improvements in\nboth offline and online settings across various downstream tasks. This\nframework has been deployed in Pinterest's production ad ranking systems,\nresulting in significant gains in online metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6e38\u8303\u5f0f\u6846\u67b6\u6784\u5efa\u7528\u6237\u548cPins\u5d4c\u5165\uff0c\u63d0\u5347Pinterest\u5e7f\u544a\u6392\u540d\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8Pinterest\u4e2a\u6027\u5316Pins\u548c\u5e7f\u544a\u7684\u6548\u679c\uff0c\u6784\u5efa\u7528\u6237\u548c\u9879\u76ee\uff08Pin\uff09\u5d4c\u5165\u4ee5\u5904\u7406\u4e0d\u540c\u6570\u636e\u6765\u6e90\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6e38\u8303\u5f0f\u6784\u5efa\u7528\u6237\u548cPin\u5d4c\u5165\u6a21\u578b\uff0c\u4e0a\u6e38\u6a21\u578b\u57fa\u4e8e\u4ece\u4e0d\u540c\u6570\u636e\u6765\u6e90\u83b7\u5f97\u7684\u5e7f\u6cdb\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u590d\u6742\u67b6\u6784\u6355\u6349\u7528\u6237\u548cPin\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f53\u5d4c\u5165\u7684\u5f02\u6b65\u5237\u65b0\u673a\u5236\u786e\u4fdd\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\uff1b\u4e0b\u6e38\u4efb\u52a1\u96c6\u6210\u8fd9\u4e9b\u5d4c\u5165\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5df2\u5728Pinterest\u7684\u5e7f\u544a\u6392\u540d\u7cfb\u7edf\u4e2d\u90e8\u7f72\uff0c\u5728\u7ebf\u6307\u6807\u53d6\u5f97\u4e86\u663e\u8457\u589e\u76ca\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u6237\u548c\u9879\u76ee\u5d4c\u5165\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u540c\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u5e7f\u544a\u548c\u4e2a\u6027\u5316\u63a8\u8350\u7684\u6548\u679c\u3002"}}
{"id": "2509.04351", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04351", "abs": "https://arxiv.org/abs/2509.04351", "authors": ["Dror Aiger", "Bingyi Cao", "Kaifeng Chen", "Andre Araujo"], "title": "Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking", "comment": null, "summary": "The dominant paradigm in image retrieval systems today is to search large\ndatabases using global image features, and re-rank those initial results with\nlocal image feature matching techniques. This design, dubbed global-to-local,\nstems from the computational cost of local matching approaches, which can only\nbe afforded for a small number of retrieved images. However, emerging efficient\nlocal feature search approaches have opened up new possibilities, in particular\nenabling detailed retrieval at large scale, to find partial matches which are\noften missed by global feature search. In parallel, global feature-based\nre-ranking has shown promising results with high computational efficiency. In\nthis work, we leverage these building blocks to introduce a local-to-global\nretrieval paradigm, where efficient local feature search meets effective global\nfeature re-ranking. Critically, we propose a re-ranking method where global\nfeatures are computed on-the-fly, based on the local feature retrieval\nsimilarities. Such re-ranking-only global features leverage multidimensional\nscaling techniques to create embeddings which respect the local similarities\nobtained during search, enabling a significant re-ranking boost.\nExperimentally, we demonstrate solid retrieval performance, setting new\nstate-of-the-art results on the Revisited Oxford and Paris datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c40\u90e8\u5230\u5168\u5c40\u68c0\u7d22\u8303\u5f0f\uff0c\u7ed3\u5408\u4e86\u6709\u6548\u7684\u5c40\u90e8\u7279\u5f81\u641c\u7d22\u548c\u9ad8\u6548\u7684\u5168\u5c40\u7279\u5f81\u91cd\u6392\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u884c\u7684\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u4e3b\u8981\u5229\u7528\u5168\u5c40\u7279\u5f81\u8fdb\u884c\u641c\u7d22\uff0c\u5e76\u7528\u5c40\u90e8\u7279\u5f81\u5339\u914d\u6280\u672f\u5bf9\u521d\u59cb\u7ed3\u679c\u8fdb\u884c\u91cd\u6392\u3002\u7136\u800c\uff0c\u5c40\u90e8\u5339\u914d\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u53ea\u80fd\u5e94\u7528\u4e8e\u5c11\u91cf\u68c0\u7d22\u56fe\u50cf\u3002\u6709\u6548\u7684\u5c40\u90e8\u7279\u5f81\u641c\u7d22\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u5927\u5c3a\u5ea6\u4e0a\u8fdb\u884c\u8be6\u7ec6\u68c0\u7d22\uff0c\u53d1\u73b0\u5168\u5c40\u7279\u5f81\u641c\u7d22\u5e38\u5e38\u9519\u8fc7\u7684\u90e8\u5206\u5339\u914d\u3002\u800c\u5168\u5c40\u7279\u5f81\u57fa\u7840\u7684\u91cd\u6392\u5219\u663e\u793a\u51fa\u9ad8\u6548\u7684\u8ba1\u7b97\u7ed3\u679c\u3002\u672c\u7814\u7a76\u5229\u7528\u8fd9\u4e9b\u57fa\u7840\u7ec4\u4ef6\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\u2014\u2014\u5c40\u90e8\u5230\u5168\u5c40\u68c0\u7d22\u3002", "method": "\u91cd\u65b0\u6392\u540d\u65f6\u5229\u7528\u591a\u7ef4\u5c3a\u5ea6\u6280\u672f\u521b\u5efa\u5c0a\u91cd\u5c40\u90e8\u7279\u5f81\u68c0\u7d22\u76f8\u4f3c\u6027\u7684\u5168\u5c40\u7279\u5f81\u5d4c\u5165\uff0c\u901a\u8fc7\u9ad8\u6548\u5c40\u90e8\u7279\u5f81\u641c\u7d22\u548c\u57fa\u4e8e\u5c40\u90e8\u76f8\u4f3c\u6027\u8ba1\u7b97\u7684\u5168\u5c40\u7279\u5f81\u91cd\u6392\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728Revisited Oxford\u548cParis\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f73\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u591a\u7ef4\u5c3a\u5ea6\u6280\u672f\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u5728\u641c\u7d22\u4e2d\u5c0a\u91cd\u5c40\u90e8\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u63d0\u5347\u91cd\u6392\u6548\u679c\uff0c\u6807\u5fd7\u7740\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u6027\u80fd\u7684\u65b0\u9ad8\u3002"}}
