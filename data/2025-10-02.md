<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval](https://arxiv.org/abs/2510.00137)
*Nima Sheikholeslami,Erfan Hosseini,Patrice Bechard,Srivatsava Daruru,Sai Rajeswar*

Main category: cs.IR

TL;DR: 提出MW损失用于改进双编码检索器的校准性能，实验显示其优于传统的对比损失。


<details>
  <summary>Details</summary>
Motivation: 针对双编码检索器中的现有噪声对比估计（NCE）目标在相关性评分与排名质量上的局限性。

Method: 提出MW损失作为新的训练目标，通过最大化Mann-Whitney U统计量，提高检索器的校准和排名性能。

Result: 实验表明，使用MW损失训练的检索器在AUC和标准检索指标上持续优于对比损失训练的模型。

Conclusion: MW损失提供了一种比对比损失更优的替代方案，为检索增强生成（RAG）等高风险应用提供了更好的校准和更强的分辨能力。

Abstract: Dual-encoder retrievers depend on the principle that relevant documents
should score higher than irrelevant ones for a given query. Yet the dominant
Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,
optimizes a softened ranking surrogate that we rigorously prove is
fundamentally oblivious to score separation quality and unrelated to AUC. This
mismatch leads to poor calibration and suboptimal performance in downstream
tasks like retrieval-augmented generation (RAG). To address this fundamental
limitation, we introduce the MW loss, a new training objective that maximizes
the Mann-Whitney U statistic, which is mathematically equivalent to the Area
under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be
correctly ranked by minimizing binary cross entropy over score differences. We
provide theoretical guarantees that MW loss directly upper-bounds the AoC,
better aligning optimization with retrieval goals. We further promote ROC
curves and AUC as natural threshold free diagnostics for evaluating retriever
calibration and ranking quality. Empirically, retrievers trained with MW loss
consistently outperform contrastive counterparts in AUC and standard retrieval
metrics. Our experiments show that MW loss is an empirically superior
alternative to Contrastive Loss, yielding better-calibrated and more
discriminative retrievers for high-stakes applications like RAG.

</details>


### [2] [HLTCOE at TREC 2024 NeuCLIR Track](https://arxiv.org/abs/2510.00143)
*Eugene Yang,Dawn Lawrie,Orion Weller,James Mayfield*

Main category: cs.IR

TL;DR: HLTCOE团队在TREC 2024 NeuCLIR赛道上应用了PLAID、重新排序器和系统组合技术进行实验，用于CLIR和MLIR任务。


<details>
  <summary>Details</summary>
Motivation: 提高跨语言信息检索（CLIR）和多语言信息检索（MLIR）的输出来帮助高效的报告生成。

Method: 采用了PLAID、mT5重新排序器、GPT-4重新排序器、评分融合以及文档翻译来参与TREC 2024 NeuCLIR赛道。

Result: 实验成功地生成了不同子问题的结果报告，系统组合实验也证明了可以提高报告质量。

Conclusion: 该研究使用了多种技术提高跨语言信息检索效率，并成功提交了所有NeuCLIR任务。

Abstract: The HLTCOE team applied PLAID, an mT5 reranker, GPT-4 reranker, score fusion,
and document translation to the TREC 2024 NeuCLIR track. For PLAID we included
a variety of models and training techniques -- Translate Distill (TD), Generate
Distill (GD) and multi-lingual translate-distill (MTD). TD uses scores from the
mT5 model over English MS MARCO query-document pairs to learn how to score
query-document pairs where the documents are translated to match the CLIR
setting. GD follows TD but uses passages from the collection and queries
generated by an LLM for training examples. MTD uses MS MARCO translated into
multiple languages, allowing experiments on how to batch the data during
training. Finally, for report generation we experimented with system
combination over different runs. One family of systems used either GPT-4o or
Claude-3.5-Sonnet to summarize the retrieved results from a series of
decomposed sub-questions. Another system took the output from those two models
and verified/combined them with Claude-3.5-Sonnet. The other family used GPT4o
and GPT3.5Turbo to extract and group relevant facts from the retrieved
documents based on the decomposed queries. The resulting submissions directly
concatenate the grouped facts to form the report and their documents of origin
as the citations. The team submitted runs to all NeuCLIR tasks: CLIR and MLIR
news tasks as well as the technical documents task and the report generation
task.

</details>


### [3] [Privacy-Preserving Learning-Augmented Data Structures](https://arxiv.org/abs/2510.00165)
*Prabhav Goyal,Vinesh Sridhar,Wilson Zheng*

Main category: cs.IR

TL;DR: 提出了一种强历史独立和稳健的学习增强数据结构，通过门槛和配对技术实现，兼具隐私与安全性，且支持动态更新。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的数据结构能利用频率估计优化性能，但其隐私与安全影响尚未充分探索。本文动机在于增强数据结构在隐私和安全方面的保证。

Method: 引入了门槛和配对两个技术，以实现稳健性和强历史独立性。

Result: 实验结果表明，在安全性与效率之间存在权衡，但其性能仍具有竞争力。

Conclusion: 该论文提出了一种新的学习增强数据结构，具有强大的历史独立性和稳健性，同时支持动态更新。

Abstract: Learning-augmented data structures use predicted frequency estimates to
retrieve frequently occurring database elements faster than standard data
structures. Recent work has developed data structures that optimally exploit
these frequency estimates while maintaining robustness to adversarial
prediction errors. However, the privacy and security implications of this
setting remain largely unexplored.
  In the event of a security breach, data structures should reveal minimal
information beyond their current contents. This is even more crucial for
learning-augmented data structures, whose layout adapts to the data. A data
structure is history independent if its memory representation reveals no
information about past operations except what is inferred from its current
contents. In this work, we take the first step towards privacy and security
guarantees in this setting by proposing the first learning-augmented data
structure that is strongly history independent, robust, and supports dynamic
updates.
  To achieve this, we introduce two techniques: thresholding, which
automatically makes any learning-augmented data structure robust, and pairing,
a simple technique that provides strong history independence in the dynamic
setting. Our experimental results demonstrate a tradeoff between security and
efficiency but are still competitive with the state of the art.

</details>


### [4] [Milco: Learned Sparse Retrieval Across Languages via a Multilingual Connector](https://arxiv.org/abs/2510.00671)
*Thong Nguyen,Yibin Lei,Jia-Huei Ju,Eugene Yang,Andrew Yates*

Main category: cs.IR

TL;DR: 介绍了MILCO，一种新的LSR架构，通过多语言连接器将不同语言映射到共享英语词汇空间，实现了先进的跨语言检索性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有双编码器在扩展到英语之外语言时的限制，同时保留词汇匹配的透明性。

Method: 通过一个两阶段训练方案，包括稀疏对齐预训练和对比训练，结合LexEcho头来增强英语词汇表示和源语言视图。

Result: MILCO在多语言和跨语言稀疏检索中实现了最先进的性能，超过了领先的密集、稀疏和多向量基线，并支持动态效率调整。

Conclusion: MILCO通过新颖的架构和训练方案改善了多语言检索的透明性和效果，并提出了LexEcho头以增强鲁棒性。

Abstract: Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with
the transparency of lexical matching, but existing approaches struggle to scale
beyond English. We introduce MILCO, an LSR architecture that maps queries and
documents from different languages into a shared English lexical space via a
multilingual connector. MILCO is trained with a specialized two-stage regime
that combines Sparse Alignment Pretraining with contrastive training to provide
representation transparency and effectiveness while mitigating semantic
collapse. Motivated by the observation that uncommon entities are often lost
when projected into English, we propose a new LexEcho head, which enhances
robustness by augmenting the English lexical representation with a
source-language view obtained through a special [ECHO] token. MILCO achieves
state-of-the-art multilingual and cross-lingual LSR performance, outperforming
leading dense, sparse, and multi-vector baselines such as BGE-M3 and
Qwen3-Embed on standard multilingual benchmarks, while supporting dynamic
efficiency through post-hoc pruning. Notably, when using mass-based pruning to
reduce document representations to only 30 active dimensions on average, MILCO
560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.

</details>


### [5] [On Listwise Reranking for Corpus Feedback](https://arxiv.org/abs/2510.00887)
*Soyoung Yoon,Jongho Kim,Daeyong Kwon,Avishek Anand,Seung-won Hwang*

Main category: cs.IR

TL;DR: L2G无需额外的LLM呼叫即可实现与基于预计算图方法相当的检索性能。


<details>
  <summary>Details</summary>
Motivation: 为了改善信息检索性能，避免昂贵的图结构计算和LLM调用。

Method: 引入了一种名为L2G的新框架，它将reranker日志隐式转化为图结构以增强检索能力。

Result: L2G框架通过将reranker的信号转换为图结构，大幅度提高了检索性能。它可以在没有显式图计算的开销下，实现可扩展的基于图的检索效果。

Conclusion: L2G可以在不需要昂贵的图结构和LLM调用的情况下，达到与基于图的检索方法相当的效能。

Abstract: Reranker improves retrieval performance by capturing document interactions.
At one extreme, graph-aware adaptive retrieval (GAR) represents an
information-rich regime, requiring a pre-computed document similarity graph in
reranking. However, as such graphs are often unavailable, or incur quadratic
memory costs even when available, graph-free rerankers leverage large language
model (LLM) calls to achieve competitive performance. We introduce L2G, a novel
framework that implicitly induces document graphs from listwise reranker logs.
By converting reranker signals into a graph structure, L2G enables scalable
graph-based retrieval without the overhead of explicit graph computation.
Results on the TREC-DL and BEIR subset show that L2G matches the effectiveness
of oracle-based graph methods, while incurring zero additional LLM calls.

</details>


### [6] [Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs](https://arxiv.org/abs/2510.00908)
*Roksana Goworek,Olivia Macmillan-Scott,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 研究从翻译方法转向嵌入驱动方法，利用多语言大语言模型提升跨语言信息检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言信息检索中的数据失衡和语言差异问题，提升检索性能，实现答案生成。

Method: 从翻译方法过渡到嵌入驱动方法，利用多语言大语言模型进行跨语言信息检索。

Result: 引入跨语言嵌入和多语言大语言模型，提升了检索性能，启用了答案生成功能。

Conclusion: 在跨语言信息检索领域，嵌入驱动方法和多语言大语言模型提供了新的检索模式，提升了性能并启用答案生成，未来需要解决数据不平衡和语言变异问题以实现更健全的检索系统。

Abstract: Cross-lingual information retrieval (CLIR) addresses the challenge of
retrieving relevant documents written in languages different from that of the
original query. Research in this area has typically framed the task as
monolingual retrieval augmented by translation, treating retrieval methods and
cross-lingual capabilities in isolation. Both monolingual and cross-lingual
retrieval usually follow a pipeline of query expansion, ranking, re-ranking
and, increasingly, question answering. Recent advances, however, have shifted
from translation-based methods toward embedding-based approaches and leverage
multilingual large language models (LLMs), for which aligning representations
across languages remains a central challenge. The emergence of cross-lingual
embeddings and multilingual LLMs has introduced a new paradigm, offering
improved retrieval performance and enabling answer generation. This survey
provides a comprehensive overview of developments from early translation-based
methods to state-of-the-art embedding-driven and generative techniques. It
presents a structured account of core CLIR components, evaluation practices,
and available resources. Persistent challenges such as data imbalance and
linguistic variation are identified, while promising directions are suggested
for advancing equitable and effective cross-lingual information retrieval. By
situating CLIR within the broader landscape of information retrieval and
multilingual language processing, this work not only reviews current
capabilities but also outlines future directions for building retrieval systems
that are robust, inclusive, and adaptable.

</details>


### [7] [Deep Learning-Based Approach for Improving Relational Aggregated Search](https://arxiv.org/abs/2510.00966)
*Sara Saad Soliman,Ahmed Younes,Islam Elkabani,Ashraf Elsayed*

Main category: cs.IR

TL;DR: 研究开发了一种使用堆叠自动编码器和AraBERT嵌入的聚类方法，以提高阿拉伯语文本数据在聚合搜索环境中的效果。


<details>
  <summary>Details</summary>
Motivation: 随着互联网信息爆炸，急需开发聚合搜索系统，以增强内容检索和管理。

Method: 应用高级自然语言处理技术如堆叠自动编码器和AraBERT嵌入，并使用K-means聚类算法来分析阿拉伯语查询数据的效果。

Result: 研究表明，使用堆叠自动编码器在表示学习上适用于聚类任务，并能显著改善聚类搜索结果的准确性和相关性。

Conclusion: 该模型能通过改进聚类算法提高搜索结果的准确性和相关性。

Abstract: Due to an information explosion on the internet, there is a need for the
development of aggregated search systems that can boost the retrieval and
management of content in various formats. To further improve the clustering of
Arabic text data in aggregated search environments, this research investigates
the application of advanced natural language processing techniques, namely
stacked autoencoders and AraBERT embeddings. By transcending the limitations of
traditional search engines, which are imprecise, not contextually relevant, and
not personalized, we offer more enriched, context-aware characterizations of
search results, so we used a K-means clustering algorithm to discover
distinctive features and relationships in these results, we then used our
approach on different Arabic queries to evaluate its effectiveness. Our model
illustrates that using stacked autoencoders in representation learning suits
clustering tasks and can significantly improve clustering search results. It
also demonstrates improved accuracy and relevance of search results.

</details>


### [8] [ModernVBERT: Towards Smaller Visual Document Retrievers](https://arxiv.org/abs/2510.01149)
*Paul Teiletche,Quentin Macé,Max Conti,Antonio Loison,Gautier Viaud,Pierre Colombo,Manuel Faysse*

Main category: cs.IR

TL;DR: 提出了一种改进视觉文档检索模型的原则性方案，并开发出性能优异的ModernVBERT模型。


<details>
  <summary>Details</summary>
Motivation: 探索多模态嵌入模型在文档检索中的性能提升，并解决现有方法的瓶颈问题。

Method: 通过对多模态嵌入模型进行对比实验，分析注意力掩码、图像分辨率、模态对齐数据方案和后期交互中心对比目标对性能的影响，制定提升视觉文档检索模型的原则性方案。

Result: 研发出ModernVBERT，一个拥有250M参数的视图-语言编码器，在文档检索任务上比体积大十倍的模型表现更佳。

Conclusion: 通过结构化实验和调整模型配置，可以有效提升视觉文档检索的性能，ModernVBERT是这一努力的成功案例。

Abstract: Multimodal embedding models are gaining prevalence, notably for document
retrieval as efficient alternatives to text-only pipelines. These models are
typically built by finetuning large vision-language decoders (VLMs) with
contrastive losses on text-image pairs. In this work, we show that, while
cost-efficient, this repurposing approach often bottlenecks retrieval
performance. Through controlled experiments, we establish a principled recipe
for improving visual document retrieval models. We notably measure the impact
of attention masking, image resolution, modality alignment data regimes, and
late interaction centered contrastive objectives which emerge as central
performance factors. Building on these insights, we release ModernVBERT, a
compact 250M-parameter vision-language encoder that outperforms models up to 10
times larger when finetuned on document retrieval tasks. Models and code are
made available at https://huggingface.co/ModernVBERT.

</details>
