{"id": "2508.18379", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.18379", "abs": "https://arxiv.org/abs/2508.18379", "authors": ["Pinhuan Wang", "Zhiqiu Xia", "Chunhua Liao", "Feiyi Wang", "Hang Liu"], "title": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking", "comment": "Accepted to EMNLP 2025 (Main Conference). 13 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, promoting it as the\nnext-generation re-ranker for modern IR systems.", "AI": {"tldr": "REALM\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u9012\u5f52\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u6392\u540d\u6548\u7387\uff0c\u9886\u5148\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u4e8e\u65b9\u6cd5\u5b58\u5728\u6392\u540d\u4e0d\u786e\u5b9a\u6027\u3001\u4e0d\u7a33\u5b9a\u7684top-k\u6062\u590d\u548c\u9ad8\u989d\u4ee4\u724c\u6210\u672c\u7b49\u663e\u8457\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u4f7f\u7528\u9012\u5f52\u8d1d\u53f6\u65af\u66f4\u65b0\u6765\u5efa\u6a21\u548c\u4f18\u5316LLM\u5bfc\u51fa\u7684\u76f8\u5173\u6027\uff0c\u91cd\u65b0\u6392\u6392\u540d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eREALM\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u6392\u540d\u3002", "conclusion": "REALM\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u91cd\u6392\u5e8f\u5668\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u4ee4\u724c\u4f7f\u7528\u548c\u5ef6\u8fdf\uff0c\u6210\u4e3a\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u4e0b\u4e00\u4ee3\u91cd\u6392\u5e8f\u5668\u3002"}}
{"id": "2508.18442", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18442", "abs": "https://arxiv.org/abs/2508.18442", "authors": ["Jan Malte Lichtenberg", "Antonio De Candia", "Matteo Ruffini"], "title": "DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation", "comment": "EARL workshop @RecSys'25, Prague, Czech Republic", "summary": "Transformer-based sequential recommenders, such as SASRec or BERT4Rec,\ntypically rely solely on learned item ID embeddings, making them vulnerable to\nthe item cold-start problem, particularly in environments with dynamic item\ncatalogs. While dense content embeddings from pre-trained models offer\npotential solutions, direct integration into transformer-based recommenders has\nconsistently underperformed compared to ID-only approaches. We revisit this\nintegration challenge and propose DenseRec, a simple yet effective method that\nintroduces a dual-path embedding approach. DenseRec learns a linear projection\nfrom the dense embedding space into the ID embedding space during training,\nenabling seamless generalization to previously unseen items without requiring\nspecialized embedding models or complex infrastructure. In experiments on three\nreal-world datasets, we find DenseRec to consistently outperform an ID-only\nSASRec baseline, even without additional hyperparameter tuning and while using\ncompact embedding models. Our analysis suggests improvements primarily arise\nfrom better sequence representations in the presence of unseen items,\npositioning DenseRec as a practical and robust solution for cold-start\nsequential recommendation.", "AI": {"tldr": "DenseRec\u901a\u8fc7\u53cc\u8def\u5f84\u5d4c\u5165\u65b9\u6cd5\u6539\u5584\u4e86\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8fc7\u4e86ID-only\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8eTransformer\u7684\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u9879\u76eeID\u5d4c\u5165\uff0c\u5728\u52a8\u6001\u9879\u76ee\u76ee\u5f55\u73af\u5883\u4e2d\u5bb9\u6613\u906d\u9047\u51b7\u542f\u52a8\u95ee\u9898\u3002\u9700\u8981\u6709\u6548\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "DenseRec\u91c7\u7528\u4e86\u53cc\u8def\u5f84\u5d4c\u5165\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u4ece\u5bc6\u96c6\u5d4c\u5165\u7a7a\u95f4\u5b66\u4e60\u5230ID\u5d4c\u5165\u7a7a\u95f4\u7684\u7ebf\u6027\u6295\u5f71\u3002", "result": "DenseRec\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eSASRec\u57fa\u7ebf\u6a21\u578b\uff0c\u5373\u4f7f\u5728\u672a\u8fdb\u884c\u989d\u5916\u8d85\u53c2\u6570\u8c03\u8282\u4e14\u4f7f\u7528\u7d27\u51d1\u5d4c\u5165\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "DenseRec\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u4ece\u5bc6\u96c6\u5d4c\u5165\u7a7a\u95f4\u5230ID\u5d4c\u5165\u7a7a\u95f4\u7684\u7ebf\u6027\u6295\u5f71\uff0c\u4f7f\u5f97\u5728\u65e0\u9700\u7279\u6b8a\u5d4c\u5165\u6a21\u578b\u6216\u590d\u6742\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u4ee5\u524d\u672a\u89c1\u7269\u54c1\u7684\u65e0\u7f1d\u6cdb\u5316\u3002"}}
{"id": "2508.18661", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.18661", "abs": "https://arxiv.org/abs/2508.18661", "authors": ["Dongyoun Kim", "Hyung-do Choi", "Youngsun Jang", "John Kim"], "title": "Extracting Information from Scientific Literature via Visual Table Question Answering Models", "comment": "Accepted at ACM International Conference on Research in Adaptive and\n  Convergent Systems, November 5-8, 2024, Pompei, Italy", "summary": "This study explores three approaches to processing table data in scientific\npapers to enhance extractive question answering and develop a software tool for\nthe systematic review process. The methods evaluated include: (1) Optical\nCharacter Recognition (OCR) for extracting information from documents, (2)\nPre-trained models for document visual question answering, and (3) Table\ndetection and structure recognition to extract and merge key information from\ntables with textual content to answer extractive questions. In exploratory\nexperiments, we augmented ten sample test documents containing tables and\nrelevant content against RF- EMF-related scientific papers with seven\npredefined extractive question-answer pairs. The results indicate that\napproaches preserving table structure outperform the others, particularly in\nrepresenting and organizing table content. Accurately recognizing specific\nnotations and symbols within the documents emerged as a critical factor for\nimproved results. Our study concludes that preserving the structural integrity\nof tables is essential for enhancing the accuracy and reliability of extractive\nquestion answering in scientific documents.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5904\u7406\u79d1\u5b66\u6587\u6863\u4e2d\u8868\u683c\u6570\u636e\u7684\u4e09\u79cd\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u4fdd\u6301\u8868\u683c\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u5904\u7406\u65b9\u5f0f\u5728\u4fe1\u606f\u63d0\u53d6\u6548\u679c\u4e0a\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u79d1\u5b66\u6587\u6863\u4e2d\u8868\u683c\u6570\u636e\u7684\u5904\u7406\u6548\u679c\uff0c\u4ee5\u6539\u5584\u63d0\u53d6\u6027\u95ee\u7b54\u80fd\u529b\u5e76\u5f00\u53d1\u7cfb\u7edf\u7efc\u8ff0\u8f6f\u4ef6\u5de5\u5177\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a(1)\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\uff08OCR\uff09\uff0c(2)\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c(3)\u8868\u683c\u68c0\u6d4b\u548c\u7ed3\u6784\u8bc6\u522b\uff0c\u7528\u4ee5\u4ece\u8868\u683c\u548c\u6587\u672c\u5185\u5bb9\u4e2d\u63d0\u53d6\u548c\u6574\u5408\u5173\u952e\u4fe1\u606f\u4ee5\u56de\u7b54\u63d0\u53d6\u6027\u95ee\u9898\u3002\u8fdb\u884c\u63a2\u7d22\u6027\u5b9e\u9a8c\u4ee5\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4fdd\u6301\u8868\u683c\u7ed3\u6784\u7684\u5904\u7406\u65b9\u6cd5\u5728\u4ee3\u8868\u548c\u7ec4\u7ec7\u8868\u683c\u5185\u5bb9\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u6587\u6863\u4e2d\u7b26\u53f7\u548c\u6807\u6ce8\u65f6\u663e\u5f97\u5c24\u4e3a\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u5728\u79d1\u5b66\u6587\u6863\u7684\u8868\u683c\u6570\u636e\u5904\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4fdd\u6301\u8868\u683c\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u5bf9\u63d0\u9ad8\u4fe1\u606f\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.18665", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18665", "abs": "https://arxiv.org/abs/2508.18665", "authors": ["Jiajie He", "Yuechun Gu", "Min-Chun Chen", "Keke Chen"], "title": "Membership Inference Attacks on LLM-based Recommender Systems", "comment": null, "summary": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728LLM\u57fa\u7840\u4e0a\u7684\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u7684\u9690\u79c1\u653b\u51fb\u98ce\u9669\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u4f1a\u5458\u63a8\u65ad\u653b\u51fb\u5e76\u9a8c\u8bc1\u4e86\u5b83\u4eec\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5e26\u6765\u4e86\u7075\u6d3b\u6027\uff0c\u4f46\u7528\u6237\u9690\u79c1\u4fe1\u606f\u53ef\u80fd\u4f1a\u9762\u4e34\u65b0\u7684\u653b\u51fb\u98ce\u9669\uff0c\u800c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u79cd\u4f1a\u5458\u63a8\u65ad\u653b\u51fb\uff08MIA\uff09\uff0c\u5206\u522b\u662f\u76f4\u63a5\u8be2\u95ee\u3001\u5e7b\u60f3\u3001\u76f8\u4f3c\u6027\u548c\u6295\u6bd2\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u51fa\u4f1a\u5458\u63a8\u65ad\u653b\u51fb\u5728LLM RecSys\u7cfb\u7edf\u4e0a\u7684\u73b0\u5b9e\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u76f4\u63a5\u8be2\u95ee\u548c\u6295\u6bd2\u653b\u51fb\u5177\u6709\u660e\u663e\u7684\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u8bc4\u4f30\u5b9e\u9a8c\u786e\u5b9a\u4e86LLM RecSys\u7cfb\u7edf\u5728\u9690\u79c1\u653b\u51fb\u4e0b\u7684\u73b0\u5b9e\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u76f4\u63a5\u8be2\u95ee\u548c\u6295\u6bd2\u653b\u51fb\u5c55\u73b0\u51fa\u4e86\u663e\u8457\u7684\u653b\u51fb\u4f18\u52bf\u3002"}}
{"id": "2508.18700", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18700", "abs": "https://arxiv.org/abs/2508.18700", "authors": ["Yi-Ping Hsu", "Po-Wei Wang", "Chantat Eksombatchai", "Jiajing Xu"], "title": "Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training", "comment": "Published at RecSys'24, see\n  https://dl.acm.org/doi/10.1145/3640457.3688053", "summary": "ID-based embeddings are widely used in web-scale online recommendation\nsystems. However, their susceptibility to overfitting, particularly due to the\nlong-tail nature of data distributions, often limits training to a single\nepoch, a phenomenon known as the \"one-epoch problem.\" This challenge has driven\nresearch efforts to optimize performance within the first epoch by enhancing\nconvergence speed or feature sparsity. In this study, we introduce a novel\ntwo-stage training strategy that incorporates a pre-training phase using a\nminimal model with contrastive loss, enabling broader data coverage for the\nembedding system. Our offline experiments demonstrate that multi-epoch training\nduring the pre-training phase does not lead to overfitting, and the resulting\nembeddings improve online generalization when fine-tuned for more complex\ndownstream recommendation tasks. We deployed the proposed system in live\ntraffic at Pinterest, achieving significant site-wide engagement gains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6765\u6269\u5c55\u6570\u636e\u8986\u76d6\uff0c\u89e3\u51b3ID\u5d4c\u5165\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "ID\u5d4c\u5165\u5728\u5927\u89c4\u6a21\u5728\u7ebf\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u5206\u5e03\u7684\u957f\u5c3e\u6548\u5e94\uff0c\u5e38\u5bfc\u81f4\u8fc7\u62df\u5408\u3002\u8fd9\u79cd\u73b0\u8c61\u9650\u5236\u4e86\u8bad\u7ec3\u65f6\u671f\uff0c\u901a\u5e38\u4ec5\u9650\u4e8e\u5355\u4e2a\u5468\u671f\uff0c\u5373\u6240\u8c13\u7684\u201c\u4e00\u5468\u671f\u95ee\u9898\u201d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u524d\u9636\u6bb5\u4f7f\u7528\u5bf9\u6bd4\u635f\u5931\u7684\u7b80\u5316\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ece\u800c\u6269\u5c55\u6570\u636e\u8986\u76d6\u8303\u56f4\u3002", "result": "\u6211\u4eec\u7684\u79bb\u7ebf\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u8fdb\u884c\u591a\u5468\u671f\u8bad\u7ec3\u4e0d\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u5728\u8fdb\u884c\u590d\u6742\u7684\u4e0b\u6e38\u63a8\u8350\u4efb\u52a1\u5fae\u8c03\u65f6\uff0c\u6240\u5f97\u5d4c\u5165\u6539\u5584\u4e86\u5728\u7ebf\u6cdb\u5316\u3002\u5728Pinterest\u8fdb\u884c\u4e86\u73b0\u573a\u6d41\u91cf\u6d4b\u8bd5\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u5168\u7ad9\u53c2\u4e0e\u5ea6\u63d0\u5347\u3002", "conclusion": "\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u514b\u670dID\u5d4c\u5165\u7684\u201c\u4e00\u5468\u671f\u95ee\u9898\u201d\uff0c\u5e76\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.18877", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18877", "abs": "https://arxiv.org/abs/2508.18877", "authors": ["Kushagra Agrawal", "Nisharg Nargund", "Oishani Banerjee"], "title": "Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search", "comment": null, "summary": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u6f5c\u5728\u7a7a\u95f4\u538b\u7f29\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u5411\u91cf\u641c\u7d22\u7684\u6548\u7387\u548c\u8bed\u4e49\u6548\u7528\uff0c\u5e76\u5728\u4e0eFAISS\u7684\u6bd4\u8f83\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u9ad8\u7ef4\u6f5c\u5728\u8868\u793a\u4f7f\u5f97\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u53d7\u5230\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u538b\u7f29\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u5c06\u538b\u7f29\u7b56\u7565\u5efa\u6a21\u4e3a\u68c0\u7d22\u51c6\u786e\u6027\u548c\u5b58\u50a8\u6548\u7387\u4e4b\u95f4\u7684\u96f6\u548c\u6e38\u620f\u3002", "result": "\u76f8\u8f83\u4e8eFAISS\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e73\u5747\u76f8\u4f3c\u5ea6\u548c\u6548\u7528\u65b9\u9762\u663e\u8457\u66f4\u9ad8\uff0c\u4f46\u67e5\u8be2\u65f6\u95f4\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u4f7f\u7528\u535a\u5f08\u8bba\u6846\u67b6\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u538b\u7f29\u53ef\u4ee5\u63d0\u9ad8\u5411\u91cf\u641c\u7d22\u7684\u6548\u7387\u548c\u8bed\u4e49\u6548\u7528\u3002"}}
