{"id": "2509.09681", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09681", "abs": "https://arxiv.org/abs/2509.09681", "authors": ["Yikuan Xia", "Jiazun Chen", "Yirui Zhan", "Suifeng Zhao", "Weipeng Jiang", "Chaorui Zhang", "Wei Han", "Bo Bai", "Jun Gao"], "title": "DB3 Team's Solution For Meta KDD Cup' 25", "comment": null, "summary": "This paper presents the db3 team's winning solution for the Meta CRAG-MM\nChallenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,\nmulti-turn question answering benchmark (CRAG-MM), we developed a comprehensive\nframework that integrates tailored retrieval pipelines for different tasks with\na unified LLM-tuning approach for hallucination control. Our solution features\n(1) domain-specific retrieval pipelines handling image-indexed knowledge\ngraphs, web sources, and multi-turn conversations; and (2) advanced refusal\ntraining using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd\nplace in Task 2, and 1st place in Task 3, securing the grand prize for\nexcellence in ego-centric queries through superior handling of first-person\nperspective challenges."}
{"id": "2509.09682", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.09682", "abs": "https://arxiv.org/abs/2509.09682", "authors": ["Maxim Zhelnin", "Dmitry Redko", "Volkov Daniil", "Anna Volodkevich", "Petr Sokerin", "Valeriy Shevchenko", "Egor Shvetsov", "Alexey Vasilev", "Darya Denisova", "Ruslan Izmailov", "Alexey Zaytsev"], "title": "Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs", "comment": null, "summary": "Sequential recommendations (SR) with transformer-based architectures are\nwidely adopted in real-world applications, where SR models require frequent\nretraining to adapt to ever-changing user preferences. However, training\ntransformer-based SR models often encounters a high computational cost\nassociated with scoring extensive item catalogs, often exceeding thousands of\nitems. This occurs mainly due to the use of cross-entropy loss, where peak\nmemory scales proportionally to catalog size, batch size, and sequence length.\nRecognizing this, practitioners in the field of recommendation systems\ntypically address memory consumption by integrating the cross-entropy (CE) loss\nwith negative sampling, thereby reducing the explicit memory demands of the\nfinal layer. However, a small number of negative samples would degrade model\nperformance, and as we demonstrate in our work, increasing the number of\nnegative samples and the batch size further improves the model's performance,\nbut rapidly starts to exceed industrial GPUs' size (~40Gb).\n  In this work, we introduce the CCE- method, which offers a GPU-efficient\nimplementation of the CE loss with negative sampling. Our method accelerates\ntraining by up to two times while reducing memory consumption by more than 10\ntimes. Leveraging the memory savings afforded by using CCE- for model training,\nit becomes feasible to enhance its accuracy on datasets with a large item\ncatalog compared to those trained with original PyTorch-implemented loss\nfunctions. Finally, we perform an analysis of key memory-related\nhyperparameters and highlight the necessity of a delicate balance among these\nfactors. We demonstrate that scaling both the number of negative samples and\nbatch size leads to better results rather than maximizing only one of them. To\nfacilitate further adoption of CCE-, we release a Triton kernel that\nefficiently implements the proposed method."}
{"id": "2509.09683", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09683", "abs": "https://arxiv.org/abs/2509.09683", "authors": ["Briti Gangopadhyay", "Zhao Wang", "Shingo Takamatsu"], "title": "Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs", "comment": null, "summary": "Forecasting click volume is a key task in digital advertising, influencing\nboth revenue and campaign strategy. Traditional time series models rely solely\non numerical data, often overlooking rich contextual information embedded in\ntextual elements, such as keyword updates. We present a multimodal forecasting\nframework that combines click data with textual logs from real-world ad\ncampaigns and generates human-interpretable explanations alongside numeric\npredictions. Reinforcement learning is used to improve comprehension of textual\ninformation and enhance fusion of modalities. Experiments on a large-scale\nindustry dataset show that our method outperforms baselines in both accuracy\nand reasoning quality."}
{"id": "2509.09684", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.09684", "abs": "https://arxiv.org/abs/2509.09684", "authors": ["Bruno Yui Yamate", "Thais Rodrigues Neubauer", "Marcelo Fantinato", "Sarajane Marques Peres"], "title": "Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation", "comment": "33 pages", "summary": "This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)\nbenchmark dataset designed for the text-to-SQL task in the process mining\ndomain. Text-to-SQL conversion facilitates natural language querying of\ndatabases, increasing accessibility for users without SQL expertise and\nproductivity for those that are experts. The text-2-SQL-4-PM dataset is\ncustomized to address the unique challenges of process mining, including\nspecialized vocabularies and single-table relational structures derived from\nevent logs. The dataset comprises 1,655 natural language utterances, including\nhuman-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods\ninclude manual curation by experts, professional translations, and a detailed\nannotation process to enable nuanced analyses of task complexity. Additionally,\na baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility\nof the dataset for text-to-SQL applications. The results show that\ntext-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering\nbroader applicability for semantic parsing and other natural language\nprocessing tasks."}
{"id": "2509.09685", "categories": ["cs.IR", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09685", "abs": "https://arxiv.org/abs/2509.09685", "authors": ["Keunwoo Choi", "Seungheon Doh", "Juhan Nam"], "title": "TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation", "comment": null, "summary": "We present TalkPlayData 2, a synthetic dataset for multimodal conversational\nmusic recommendation generated by an agentic data pipeline. In TalkPlayData 2\npipeline, multiple large language model (LLM) agents are created under various\nroles with specialized prompts and access to different parts of information,\nand the chat data is acquired by logging the conversation between the Listener\nLLM and the Recsys LLM. To cover various conversation scenarios, for each\nconversation, the Listener LLM is conditioned on a finetuned conversation goal.\nFinally, all the LLMs are multimodal with audio and images, allowing a\nsimulation of multimodal recommendation and conversation. In the LLM-as-a-judge\nand subjective evaluation experiments, TalkPlayData 2 achieved the proposed\ngoal in various aspects related to training a generative recommendation model\nfor music. TalkPlayData 2 and its generation code are open-sourced at\nhttps://talkpl.ai/talkplaydata2.html."}
{"id": "2509.09686", "categories": ["cs.IR", "cs.AI", "I.2; I.7; H.4; H.5"], "pdf": "https://arxiv.org/pdf/2509.09686", "abs": "https://arxiv.org/abs/2509.09686", "authors": ["Fei Huang", "Fan Wu", "Zeqing Zhang", "Qihao Wang", "Long Zhang", "Grant Michael Boquet", "Hongyang Chen"], "title": "GeoGPT.RAG Technical Report", "comment": "19 pages, 10 figures, 10 tables", "summary": "GeoGPT is an open large language model system built to advance research in\nthe geosciences. To enhance its domain-specific capabilities, we integrated\nRetrieval Augmented Generation(RAG), which augments model outputs with relevant\ninformation retrieved from an external knowledge source. GeoGPT uses RAG to\ndraw from the GeoGPT Library, a specialized corpus curated for geoscientific\ncontent, enabling it to generate accurate, context-specific answers. Users can\nalso create personalized knowledge bases by uploading their own publication\nlists, allowing GeoGPT to retrieve and respond using user-provided materials.\nTo further improve retrieval quality and domain alignment, we fine-tuned both\nthe embedding model and a ranking model that scores retrieved passages by\nrelevance to the query. These enhancements optimize RAG for geoscience\napplications and significantly improve the system's ability to deliver precise\nand trustworthy outputs. GeoGPT reflects a strong commitment to open science\nthrough its emphasis on collaboration, transparency, and community driven\ndevelopment. As part of this commitment, we have open-sourced two core RAG\ncomponents-GeoEmbedding and GeoReranker-to support geoscientists, researchers,\nand professionals worldwide with powerful, accessible AI tools."}
{"id": "2509.09687", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.09687", "abs": "https://arxiv.org/abs/2509.09687", "authors": ["Hermann Kroll", "Pascal Sackhoff", "Bill Matthias Thang", "Christin Katharina Kreutz", "Wolf-Tilo Balke"], "title": "Demonstrating Narrative Pattern Discovery from Biomedical Literature", "comment": "Accepted Demo at TPDL2025, 10 pages, 3 figures", "summary": "Digital libraries maintain extensive collections of knowledge and need to\nprovide effective access paths for their users. For instance, PubPharm, the\nspecialized information service for Pharmacy in Germany, provides and develops\naccess paths to their underlying biomedical document collection. In brief,\nPubPharm supports traditional keyword-based search, search for chemical\nstructures, as well as novel graph-based discovery workflows, e.g., listing or\nsearching for interactions between different pharmaceutical entities. This\npaper introduces a new search functionality, called narrative pattern mining,\nallowing users to explore context-relevant entities and entity interactions. We\nperformed interviews with five domain experts to verify the usefulness of our\nprototype."}
{"id": "2509.09688", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09688", "abs": "https://arxiv.org/abs/2509.09688", "authors": ["Mohammad Atif", "Vincent Garonne", "Eric Lancon", "Jerome Lauret", "Alexandr Prozorov", "Michal Vranovsky"], "title": "AI-Powered Assistant for Long-Term Access to RHIC Knowledge", "comment": null, "summary": "As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National\nLaboratory concludes 25 years of operation, preserving not only its vast data\nholdings ($\\sim$1 ExaByte) but also the embedded scientific knowledge becomes a\ncritical priority. The RHIC Data and Analysis Preservation Plan (DAPP)\nintroduces an AI-powered assistant system that provides natural language access\nto documentation, workflows, and software, with the aim of supporting\nreproducibility, education, and future discovery. Built upon Large Language\nModels using Retrieval-Augmented Generation and the Model Context Protocol,\nthis assistant indexes structured and unstructured content from RHIC\nexperiments and enables domain-adapted interaction. We report on the\ndeployment, computational performance, ongoing multi-experiment integration,\nand architectural features designed for a sustainable and explainable long-term\nAI access. Our experience illustrates how modern AI/ML tools can transform the\nusability and discoverability of scientific legacy data."}
{"id": "2509.09689", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09689", "abs": "https://arxiv.org/abs/2509.09689", "authors": ["Himanshu Thakur", "Eshani Agrawal", "Smruthi Mukund"], "title": "Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors", "comment": null, "summary": "A long-standing challenge in developing accurate recommendation models is\nsimulating user behavior, mainly due to the complex and stochastic nature of\nuser interactions. Towards this, one promising line of work has been the use of\nLarge Language Models (LLMs) for simulating user behavior. However, aligning\nthese general-purpose large pre-trained models with user preferences\nnecessitates: (i) effectively and continously parsing large-scale tabular\nuser-item interaction data, (ii) overcoming pre-training-induced inductive\nbiases to accurately learn user specific knowledge, and (iii) achieving the\nformer two at scale for millions of users. While most previous works have\nfocused on complex methods to prompt an LLM or fine-tune it on tabular\ninteraction datasets, our approach shifts the focus to extracting robust\ntextual user representations using a frozen LLM and simulating cost-effective,\nresource-efficient user agents powered by fine-tuned Small Language Models\n(SLMs). Further, we showcase a method for training multiple low-rank adapters\nfor groups of users or \\textit{persona}, striking an optimal balance between\nscalability and performance of user behavior agents. Our experiments provide\ncompelling empirical evidence of the efficacy of our methods, demonstrating\nthat user agents developed using our approach have the potential to bridge the\ngap between offline metrics and real-world performance of recommender systems."}
{"id": "2509.09690", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09690", "abs": "https://arxiv.org/abs/2509.09690", "authors": ["Ping Liu", "Jianqiang Shen", "Qianqi Shen", "Chunnan Yao", "Kevin Kao", "Dan Xu", "Rajat Arora", "Baofen Zheng", "Caleb Johnson", "Liangjie Hong", "Jingwei Wu", "Wenjing Zhang"], "title": "Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems", "comment": "CIKM2025", "summary": "Query understanding is essential in modern relevance systems, where user\nqueries are often short, ambiguous, and highly context-dependent. Traditional\napproaches often rely on multiple task-specific Named Entity Recognition models\nto extract structured facets as seen in job search applications. However, this\nfragmented architecture is brittle, expensive to maintain, and slow to adapt to\nevolving taxonomies and language patterns. In this paper, we introduce a\nunified query understanding framework powered by a Large Language Model (LLM),\ndesigned to address these limitations. Our approach jointly models the user\nquery and contextual signals such as profile attributes to generate structured\ninterpretations that drive more accurate and personalized recommendations. The\nframework improves relevance quality in online A/B testing while significantly\nreducing system complexity and operational overhead. The results demonstrate\nthat our solution provides a scalable and adaptable foundation for query\nunderstanding in dynamic web applications."}
{"id": "2509.09691", "categories": ["cs.IR", "cs.AI", "cs.DB", "68T05 (Primary), 42C10, 94A12 (Secondary)", "I.2.6; H.2.4; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.09691", "abs": "https://arxiv.org/abs/2509.09691", "authors": ["Aleksandr Listopad"], "title": "Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores", "comment": "9 pages, 6 figures", "summary": "Conventional vector-based memory systems rely on cosine or inner product\nsimilarity within real-valued embedding spaces. While computationally\nefficient, such approaches are inherently phase-insensitive and limited in\ntheir ability to capture resonance phenomena crucial for meaning\nrepresentation. We propose Wave-Based Semantic Memory, a novel framework that\nmodels knowledge as wave patterns $\\psi(x) = A(x) e^{i\\phi(x)}$ and retrieves\nit through resonance-based interference. This approach preserves both amplitude\nand phase information, enabling more expressive and robust semantic similarity.\nWe demonstrate that resonance-based retrieval achieves higher discriminative\npower in cases where vector methods fail, including phase shifts, negations,\nand compositional queries. Our implementation, ResonanceDB, shows scalability\nto millions of patterns with millisecond latency, positioning wave-based memory\nas a viable alternative to vector stores for AGI-oriented reasoning and\nknowledge representation."}
{"id": "2509.10212", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.10212", "abs": "https://arxiv.org/abs/2509.10212", "authors": ["Alisa Rieger", "Stefan Dietze", "Ran Yu"], "title": "A Research Vision for Web Search on Emerging Topics", "comment": null, "summary": "We regularly encounter information on novel, emerging topics for which the\nbody of knowledge is still evolving, which can be linked, for instance, to\ncurrent events. A primary way to learn more about such topics is through web\nsearch. However, information on emerging topics is sparse and evolves\ndynamically as knowledge grows, making it uncertain and variable in quality and\ntrustworthiness and prone to deliberate or accidental manipulation,\nmisinformation, and bias. In this paper, we outline a research vision towards\nsearch systems and interfaces that support effective knowledge acquisition,\nawareness of the dynamic nature of topics, and responsible opinion formation\namong people searching the web for information on emerging topics. To realize\nthis vision, we propose three overarching research questions, aimed at\nunderstanding the status quo, determining requirements of systems aligned with\nour vision, and building these systems. For each of the three questions, we\nhighlight relevant literature, including pointers on how they could be\naddressed. Lastly, we discuss the challenges that will potentially arise in\npursuing the proposed vision."}
{"id": "2509.10245", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10245", "abs": "https://arxiv.org/abs/2509.10245", "authors": ["Irina Arévalo", "Jose L Salmeron"], "title": "Model-agnostic post-hoc explainability for recommender systems", "comment": null, "summary": "Recommender systems often benefit from complex feature embeddings and deep\nlearning algorithms, which deliver sophisticated recommendations that enhance\nuser experience, engagement, and revenue. However, these methods frequently\nreduce the interpretability and transparency of the system. In this research,\nwe develop a systematic application, adaptation, and evaluation of deletion\ndiagnostics in the recommender setting. The method compares the performance of\na model to that of a similar model trained without a specific user or item,\nallowing us to quantify how that observation influences the recommender, either\npositively or negatively. To demonstrate its model-agnostic nature, the\nproposal is applied to both Neural Collaborative Filtering (NCF), a widely used\ndeep learning-based recommender, and Singular Value Decomposition (SVD), a\nclassical collaborative filtering technique. Experiments on the MovieLens and\nAmazon Reviews datasets provide insights into model behavior and highlight the\ngenerality of the approach across different recommendation paradigms."}
{"id": "2509.10392", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10392", "abs": "https://arxiv.org/abs/2509.10392", "authors": ["Carole Ibrahim", "Hiba Bederina", "Daniel Cuesta", "Laurent Montier", "Cyrille Delabre", "Jill-Jênn Vie"], "title": "Diversified recommendations of cultural activities with personalized determinantal point processes", "comment": "7 pages, accepted at RecSys workshop RecSoGood 2025", "summary": "While optimizing recommendation systems for user engagement is a\nwell-established practice, effectively diversifying recommendations without\nnegatively impacting core business metrics remains a significant industry\nchallenge. In line with our initiative to broaden our audience's cultural\npractices, this study investigates using personalized Determinantal Point\nProcesses (DPPs) to sample diverse and relevant recommendations. We rely on a\nwell-known quality-diversity decomposition of the similarity kernel to give\nmore weight to user preferences. In this paper, we present our implementations\nof the personalized DPP sampling, evaluate the trade-offs between relevance and\ndiversity through both offline and online metrics, and give insights for\npractitioners on their use in a production environment. For the sake of\nreproducibility, we release the full code for our platform and experiments on\nGitHub."}
{"id": "2509.10397", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.10397", "abs": "https://arxiv.org/abs/2509.10397", "authors": ["Fei Liu", "Xinyu Lin", "Hanchao Yu", "Mingyuan Wu", "Jianyu Wang", "Qiang Zhang", "Zhuokai Zhao", "Yinglong Xia", "Yao Zhang", "Weiwei Li", "Mingze Gao", "Qifan Wang", "Lizhu Zhang", "Benyu Zhang", "Xiangjun Fan"], "title": "RecoWorld: Building Simulated Environments for Agentic Recommender Systems", "comment": null, "summary": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement."}
{"id": "2509.10448", "categories": ["cs.IR", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.10448", "abs": "https://arxiv.org/abs/2509.10448", "authors": ["Kausik Hira", "Mohd Zaki", "Mausam", "N. M. Anoop Krishnan"], "title": "MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables", "comment": null, "summary": "Scientific progress increasingly depends on synthesizing knowledge across\nvast literature, yet most experimental data remains trapped in semi-structured\nformats that resist systematic extraction and analysis. Here, we present\nMatSKRAFT, a computational framework that automatically extracts and integrates\nmaterials science knowledge from tabular data at unprecedented scale. Our\napproach transforms tables into graph-based representations processed by\nconstraint-driven GNNs that encode scientific principles directly into model\narchitecture. MatSKRAFT significantly outperforms state-of-the-art large\nlanguage models, achieving F1 scores of 88.68 for property extraction and 71.35\nfor composition extraction, while processing data $19$-$496\\times$ faster than\nthem (compared to the slowest and the fastest models, respectively) with modest\nhardware requirements. Applied to nearly 69,000 tables from more than 47,000\nresearch publications, we construct a comprehensive database containing over\n535,000 entries, including 104,000 compositions that expand coverage beyond\nmajor existing databases, pending manual validation. This systematic approach\nreveals previously overlooked materials with distinct property combinations and\nenables data-driven discovery of composition-property relationships forming the\ncornerstone of materials and scientific discovery."}
