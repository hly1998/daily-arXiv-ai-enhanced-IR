{"id": "2508.10116", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10116", "abs": "https://arxiv.org/abs/2508.10116", "authors": ["Yipeng Zhang", "Hongju Yu", "Aritra Mandal", "Canran Xu", "Qunzhi Zhou", "Zhe Wu"], "title": "Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment", "comment": null, "summary": "Item information, such as titles and attributes, is essential for effective\nuser engagement in e-commerce. However, manual or semi-manual entry of\nstructured item specifics often produces inconsistent quality, errors, and slow\nturnaround, especially for Customer-to-Customer sellers. Generating accurate\ndescriptions directly from item images offers a promising alternative. Existing\nretrieval-based solutions address some of these issues but often miss\nfine-grained visual details and struggle with niche or specialized categories.\n  We propose Optimized Preference-Based AI for Listings (OPAL), a framework for\ngenerating schema-compliant, high-quality item descriptions from images using a\nfine-tuned multimodal large language model (MLLM). OPAL addresses key\nchallenges in multimodal e-commerce applications, including bridging modality\ngaps and capturing detailed contextual information. It introduces two data\nrefinement methods: MLLM-Assisted Conformity Enhancement, which ensures\nalignment with structured schema requirements, and LLM-Assisted Contextual\nUnderstanding, which improves the capture of nuanced and fine-grained\ninformation from visual inputs.\n  OPAL uses visual instruction tuning combined with direct preference\noptimization to fine-tune the MLLM, reducing hallucinations and improving\nrobustness across different backbone architectures. We evaluate OPAL on\nreal-world e-commerce datasets, showing that it consistently outperforms\nbaseline methods in both description quality and schema completion rates. These\nresults demonstrate that OPAL effectively bridges the gap between visual and\ntextual modalities, delivering richer, more accurate, and more consistent item\ndescriptions. This work advances automated listing optimization and supports\nscalable, high-quality content generation in e-commerce platforms.", "AI": {"tldr": "OPAL\u6846\u67b6\u901a\u8fc7\u5fae\u8c03\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u56fe\u50cf\u751f\u6210\u7b26\u5408\u7ed3\u6784\u6a21\u5f0f\u8981\u6c42\u7684\u9ad8\u8d28\u91cf\u5546\u54c1\u63cf\u8ff0\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u7535\u5b50\u5546\u52a1\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u624b\u52a8\u6216\u534a\u624b\u52a8\u8f93\u5165\u7ed3\u6784\u5316\u5546\u54c1\u4fe1\u606f\u5e38\u4f1a\u5bfc\u81f4\u8d28\u91cf\u4e0d\u4e00\u81f4\u3001\u9519\u8bef\u548c\u7f13\u6162\u7684\u5468\u8f6c\uff0c\u5c24\u5176\u5bf9\u4e8eCustomer-to-Customer\u5356\u5bb6\u6765\u8bf4\u3002\u76f4\u63a5\u4ece\u5546\u54c1\u56fe\u50cf\u751f\u6210\u51c6\u786e\u63cf\u8ff0\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u7684\u504f\u597d\u4f18\u5316\u7ed3\u5408\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u6765\u5fae\u8c03\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u4ee5\u51cf\u5c11\u865a\u5047\u6210\u5206\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u57fa\u7840\u67b6\u6784\u4e2d\u63d0\u9ad8\u5065\u58ee\u6027\u3002", "result": "OPAL\u5728\u771f\u5b9e\u4e16\u754c\u7684\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u65f6\uff0c\u5c55\u793a\u51fa\u5728\u63cf\u8ff0\u8d28\u91cf\u548c\u6a21\u5f0f\u5b8c\u6210\u7387\u65b9\u9762\u6301\u7eed\u80dc\u8fc7\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OPAL\u80fd\u591f\u6709\u6548\u5730\u5728\u89c6\u89c9\u548c\u6587\u672c\u6a21\u5f0f\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u63d0\u4f9b\u66f4\u4e30\u5bcc\u3001\u66f4\u51c6\u786e\u3001\u66f4\u4e00\u81f4\u7684\u5546\u54c1\u63cf\u8ff0\u3002\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u81ea\u52a8\u5316\u5217\u8868\u4f18\u5316\uff0c\u5e76\u652f\u6301\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u4e2d\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u5185\u5bb9\u751f\u6210\u3002"}}
{"id": "2508.10238", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10238", "abs": "https://arxiv.org/abs/2508.10238", "authors": ["Xinyang Shao", "Tri Kurniawan Wijaya"], "title": "DS4RS: Community-Driven and Explainable Dataset Search Engine for Recommender System Research", "comment": null, "summary": "Accessing suitable datasets is critical for research and development in\nrecommender systems. However, finding datasets that match specific\nrecommendation task or domains remains a challenge due to scattered sources and\ninconsistent metadata. To address this gap, we propose a community-driven and\nexplainable dataset search engine tailored for recommender system research. Our\nsystem supports semantic search across multiple dataset attributes, such as\ndataset names, descriptions, and recommendation domain, and provides\nexplanations of search relevance to enhance transparency. The system encourages\ncommunity participation by allowing users to contribute standardized dataset\nmetadata in public repository. By improving dataset discoverability and search\ninterpretability, the system facilitates more efficient research reproduction.\nThe platform is publicly available at: https://ds4rs.com.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u793e\u533a\u9a71\u52a8\u7684\u53ef\u89e3\u91ca\u641c\u7d22\u5e73\u53f0\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6570\u636e\u96c6\u7684\u53d1\u73b0\u548c\u641c\u7d22\u89e3\u91ca\u6027\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u6e90\u5206\u6563\u548c\u5143\u6570\u636e\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u627e\u5230\u5339\u914d\u7279\u5b9a\u63a8\u8350\u4efb\u52a1\u6216\u9886\u57df\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u7684\u793e\u533a\u9a71\u52a8\u548c\u53ef\u89e3\u91ca\u7684\u6570\u636e\u96c6\u641c\u7d22\u5f15\u64ce\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u5c5e\u6027\uff08\u5982\u540d\u79f0\u3001\u63cf\u8ff0\u3001\u63a8\u8350\u9886\u57df\uff09\u7684\u8bed\u4e49\u641c\u7d22\uff0c\u5e76\u63d0\u4f9b\u641c\u7d22\u76f8\u5173\u6027\u7684\u89e3\u91ca\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u96c6\u7684\u53ef\u53d1\u73b0\u6027\u548c\u641c\u7d22\u7684\u89e3\u91ca\u6027\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9ad8\u6548\u7684\u7814\u7a76\u518d\u73b0\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u793e\u533a\u53c2\u4e0e\u548c\u53ef\u89e3\u91ca\u6027\u641c\u7d22\u5f15\u64ce\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u4e2d\u7684\u6570\u636e\u96c6\u53ef\u53d1\u73b0\u6027\u548c\u89e3\u91ca\u6027\uff0c\u4fc3\u8fdb\u4e86\u7814\u7a76\u7684\u9ad8\u6548\u518d\u73b0\u3002"}}
{"id": "2508.10377", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10377", "abs": "https://arxiv.org/abs/2508.10377", "authors": ["Michael Weiss", "Robert Rosenbach", "Christian Eggenberger"], "title": "Clicks Versus Conversion: Choosing a Recommender's Training Objective in E-Commerce", "comment": null, "summary": "Ranking product recommendations to optimize for a high click-through rate\n(CTR) or for high conversion, such as add-to-cart rate (ACR) and\nOrder-Submit-Rate (OSR, view-to-purchase conversion) are standard practices in\ne-commerce. Optimizing for CTR appears like a straightforward choice: Training\ndata (i.e., click data) are simple to collect and often available in large\nquantities. Additionally, CTR is used far beyond e-commerce, making it a\ngeneralist, easily implemented option. ACR and OSR, on the other hand, are more\ndirectly linked to a shop's business goals, such as the Gross Merchandise Value\n(GMV). In this paper, we compare the effects of using either of these\nobjectives using an online A/B test. Among our key findings, we demonstrate\nthat in our shops, optimizing for OSR produces a GMV uplift more than five\ntimes larger than when optimizing for CTR, without sacrificing new product\ndiscovery. Our results also provide insights into the different feature\nimportances for each of the objectives.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u7535\u5546\u54c1\u63a8\u8350\u4f18\u5316\u76ee\u6807\uff0c\u53d1\u73b0\u4f18\u5316\u8ba2\u5355\u63d0\u4ea4\u7387\uff08OSR\uff09\u6bd4\u70b9\u51fb\u7387\uff08CTR\uff09\u53ef\u5b9e\u73b0\u66f4\u9ad8\u7684GMV\u63d0\u5347\u3002", "motivation": "\u5728\u7535\u5b50\u5546\u52a1\u4e2d\uff0c\u4f18\u5316\u63a8\u8350\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u70b9\u51fb\u7387\uff08CTR\uff09\u6216\u8f6c\u5316\u7387\uff08\u5982\u6dfb\u52a0\u8d2d\u7269\u8f66\u7387\u548c\u8ba2\u5355\u63d0\u4ea4\u7387\uff09\u662f\u5e38\u89c1\u7684\u505a\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f18\u5316\u54ea\u4e00\u79cd\u8f6c\u5316\u7387\u76ee\u6807\u5bf9\u4e1a\u52a1\u6548\u679c\u66f4\u6709\u5e2e\u52a9\u3002", "method": "\u901a\u8fc7\u5728\u7ebfA/B\u6d4b\u8bd5\u5bf9\u6bd4\u70b9\u51fb\u7387\uff08CTR\uff09\u4e0e\u6dfb\u52a0\u8d2d\u7269\u8f66\u7387\uff08ACR\uff09\u53ca\u8ba2\u5355\u63d0\u4ea4\u7387\uff08OSR\uff09\u5bf9\u7535\u5546\u63a8\u8350\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u8ba2\u5355\u63d0\u4ea4\u7387\uff08OSR\uff09\u8fdb\u884c\u4f18\u5316\u53ef\u5b9e\u73b0\u8d85\u8fc7\u4e94\u500d\u4e8e\u70b9\u51fb\u7387\uff08CTR\uff09\u7684\u603b\u5546\u54c1\u4ea4\u6613\u989d\uff08GMV\uff09\u63d0\u5347\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u65b0\u4ea7\u54c1\u7684\u53d1\u6398\u3002", "conclusion": "\u4f18\u5316\u4ea7\u54c1\u63a8\u8350\u4ee5\u63d0\u9ad8\u8ba2\u5355\u63d0\u4ea4\u7387\uff08OSR\uff09\u8f83\u70b9\u51fb\u7387\uff08CTR\uff09\u80fd\u591f\u663e\u8457\u63d0\u5347\u603b\u5546\u54c1\u4ea4\u6613\u989d\uff08GMV\uff09\u3002"}}
{"id": "2508.10401", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10401", "abs": "https://arxiv.org/abs/2508.10401", "authors": ["Liang Qu", "Jianxin Li", "Wei Yuan", "Penghui Ruan", "Yuhui Shi", "Hongzhi Yin"], "title": "Proxy Model-Guided Reinforcement Learning for Client Selection in Federated Recommendation", "comment": "Under review", "summary": "Federated recommender systems have emerged as a promising privacy-preserving\nparadigm, enabling personalized recommendation services without exposing users'\nraw data. By keeping data local and relying on a central server to coordinate\ntraining across distributed clients, FedRSs protect user privacy while\ncollaboratively learning global models. However, most existing FedRS frameworks\nadopt fully random client selection strategy in each training round,\noverlooking the statistical heterogeneity of user data arising from diverse\npreferences and behavior patterns, thereby resulting in suboptimal model\nperformance. While some client selection strategies have been proposed in the\nbroader federated learning literature, these methods are typically designed for\ngeneric tasks and fail to address the unique challenges of recommendation\nscenarios, such as expensive contribution evaluation due to the large number of\nclients, and sparse updates resulting from long-tail item distributions. To\nbridge this gap, we propose ProxyRL-FRS, a proxy model-guided reinforcement\nlearning framework tailored for client selection in federated recommendation.\nSpecifically, we first introduce ProxyNCF, a dual-branch model deployed on each\nclient, which augments standard Neural Collaborative Filtering with an\nadditional proxy model branch that provides lightweight contribution\nestimation, thus eliminating the need for expensive per-round local training\ntraditionally required to evaluate a client's contribution. Furthermore, we\ndesign a staleness-aware SA reinforcement learning agent that selects clients\nbased on the proxy-estimated contribution, and is guided by a reward function\nbalancing recommendation accuracy and embedding staleness, thereby enriching\nthe update coverage of item embeddings. Experiments conducted on public\nrecommendation datasets demonstrate the effectiveness of ProxyRL-FRS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565ProxyRL-FRS\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6709\u6548\u6539\u5584\u4e86\u63a8\u8350\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff0c\u5b83\u4eec\u5728\u9009\u62e9\u53c2\u4e0e\u8bad\u7ec3\u7684\u5ba2\u6237\u7aef\u65f6\u662f\u968f\u673a\u7684\uff0c\u8fd9\u5bfc\u81f4\u5ffd\u7565\u4e86\u7528\u6237\u6570\u636e\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u4ece\u800c\u9020\u6210\u6a21\u578b\u6027\u80fd\u7684\u6b20\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86ProxyRL-FRS\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u4e2a\u521b\u65b0\u6027\u7684\u53cc\u5206\u652f\u6a21\u578bProxyNCF\uff0c\u7528\u4e8e\u8f7b\u91cf\u7ea7\u8d21\u732e\u4f30\u8ba1\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u5956\u52b1\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u7528\u4ee5\u5e73\u8861\u63a8\u8350\u51c6\u786e\u6027\u548c\u5d4c\u5165\u9648\u65e7\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cProxyRL-FRS\u5728\u516c\u5f00\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\u663e\u8457\uff0c\u8bf4\u660e\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u6539\u5584\u4e86\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\u3002", "conclusion": "ProxyRL-FRS\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u5728\u5ba2\u6237\u7aef\u9009\u62e9\u4e0a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u7528\u6237\u6570\u636e\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\uff0c\u901a\u8fc7\u5f15\u5165\u4ee3\u7406\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u63d0\u9ad8\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3002"}}
{"id": "2508.10478", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10478", "abs": "https://arxiv.org/abs/2508.10478", "authors": ["Gustavo Penha", "Edoardo D'Amico", "Marco De Nadai", "Enrico Palumbo", "Alexandre Tamborrino", "Ali Vardasbi", "Max Lefarov", "Shawn Lin", "Timothy Heath", "Francesco Fabbri", "Hugues Bouchard"], "title": "Semantic IDs for Joint Generative Search and Recommendation", "comment": "Accepted for publication in the 19th ACM Conference on Recommender\n  Systems (RecSys 2025), Late-Breaking Results track", "summary": "Generative models powered by Large Language Models (LLMs) are emerging as a\nunified solution for powering both recommendation and search tasks. A key\ndesign choice in these models is how to represent items, traditionally through\nunique identifiers (IDs) and more recently with Semantic IDs composed of\ndiscrete codes, obtained from embeddings. While task-specific embedding models\ncan improve performance for individual tasks, they may not generalize well in a\njoint setting. In this paper, we explore how to construct Semantic IDs that\nperform well both in search and recommendation when using a unified model. We\ncompare a range of strategies to construct Semantic IDs, looking into\ntask-specific and cross-tasks approaches, and also whether each task should\nhave its own semantic ID tokens in a joint search and recommendation generative\nmodel. Our results show that using a bi-encoder model fine-tuned on both search\nand recommendation tasks to obtain item embeddings, followed by the\nconstruction of a unified Semantic ID space provides an effective trade-off,\nenabling strong performance in both tasks. We hope these findings spark\nfollow-up work on generalisable, semantically grounded ID schemes and inform\nthe next wave of unified generative recommender architectures.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u5728\u751f\u6210\u6a21\u578b\u4e2d\u6784\u5efa\u7edf\u4e00\u7684\u8bed\u4e49ID\uff0c\u4ee5\u63d0\u5347\u641c\u7d22\u548c\u63a8\u8350\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u641c\u7d22\u548c\u63a8\u8350\u4efb\u52a1\u63d0\u4f9b\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u4e00\u4e2a\u5173\u952e\u7684\u8bbe\u8ba1\u9009\u62e9\u662f\u5982\u4f55\u8868\u793a\u9879\u76ee\u3002", "method": "\u63a2\u7d22\u5982\u4f55\u6784\u5efa\u540c\u65f6\u5728\u641c\u7d22\u548c\u63a8\u8350\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u7684\u8bed\u4e49ID\u3002\u6bd4\u8f83\u591a\u79cd\u6784\u5efa\u8bed\u4e49ID\u7684\u7b56\u7565\uff0c\u5305\u62ec\u4efb\u52a1\u7279\u5b9a\u548c\u8de8\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5728\u8054\u5408\u641c\u7d22\u548c\u63a8\u8350\u751f\u6210\u6a21\u578b\u4e2d\u662f\u5426\u6bcf\u4e2a\u4efb\u52a1\u90fd\u5e94\u8be5\u6709\u81ea\u5df1\u7684\u8bed\u4e49ID\u6807\u8bb0\u3002", "result": "\u4f7f\u7528\u4e00\u4e2a\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u641c\u7d22\u548c\u63a8\u8350\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u83b7\u5f97\u9879\u76ee\u5d4c\u5165\uff0c\u7136\u540e\u6784\u5efa\u7edf\u4e00\u7684\u8bed\u4e49ID\u7a7a\u95f4\uff0c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6743\u8861\uff0c\u4f7f\u5f97\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u52b2\u3002", "conclusion": "\u4f7f\u7528\u5728\u641c\u7d22\u548c\u63a8\u8350\u4efb\u52a1\u4e0a\u5171\u540c\u5fae\u8c03\u7684\u53cc\u7f16\u7801\u5668\u6a21\u578b\u53ef\u4ee5\u83b7\u5f97\u9879\u76ee\u5d4c\u5165\uff0c\u7136\u540e\u6784\u5efa\u7edf\u4e00\u7684\u8bed\u4e49ID\u7a7a\u95f4\uff0c\u80fd\u591f\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5f3a\u6027\u80fd\u3002"}}
{"id": "2508.10496", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10496", "abs": "https://arxiv.org/abs/2508.10496", "authors": ["Krzysztof Daniell", "Igor Buzhinsky", "Sebastian Bj\u00f6rkqvist"], "title": "Efficient Patent Searching Using Graph Transformers", "comment": "Accepted for publication at the PatentSemTech 2025 workshop, held in\n  conjunction with SIGIR 2025", "summary": "Finding relevant prior art is crucial when deciding whether to file a new\npatent application or invalidate an existing patent. However, searching for\nprior art is challenging due to the large number of patent documents and the\nneed for nuanced comparisons to determine novelty. An accurate search engine is\ntherefore invaluable for speeding up the process. We present a Graph\nTransformer-based dense retrieval method for patent searching where each\ninvention is represented by a graph describing its features and their\nrelationships. Our model processes these invention graphs and is trained using\nprior art citations from patent office examiners as relevance signals. Using\ngraphs as input significantly improves the computational efficiency of\nprocessing long documents, while leveraging examiner citations allows the model\nto learn domain-specific similarities beyond simple text-based matching. The\nresult is a search engine that emulates how professional patent examiners\nidentify relevant documents. We compare our approach against publicly available\ntext embedding models and show substantial improvements in both prior art\nretrieval quality and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u5f62\u53d8\u6362\u5668\u7684\u7a20\u5bc6\u68c0\u7d22\u4e13\u5229\u641c\u7d22\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u8f93\u5165\u63d0\u9ad8\u6548\u7387\uff0c\u5229\u7528\u5f15\u7528\u5b66\u4e60\u7279\u5b9a\u76f8\u4f3c\u6027\uff0c\u63d0\u9ad8\u68c0\u7d22\u8d28\u91cf\u3002", "motivation": "\u5bfb\u627e\u76f8\u5173\u7684\u73b0\u6709\u6280\u672f\u5bf9\u4e8e\u51b3\u5b9a\u662f\u5426\u63d0\u4ea4\u65b0\u7684\u4e13\u5229\u7533\u8bf7\u6216\u4f7f\u73b0\u6709\u4e13\u5229\u65e0\u6548\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e13\u5229\u6587\u6863\u6570\u91cf\u5e9e\u5927\u4e14\u9700\u8981\u8fdb\u884c\u7ec6\u81f4\u7684\u6bd4\u8f83\u6765\u786e\u5b9a\u65b0\u9896\u6027\uff0c\u8fd9\u4f7f\u5f97\u5bfb\u627e\u76f8\u5173\u7684\u73b0\u6709\u6280\u672f\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5f62\u53d8\u6362\u5668\u7684\u7a20\u5bc6\u68c0\u7d22\u65b9\u6cd5\u8fdb\u884c\u4e13\u5229\u641c\u7d22\uff0c\u5176\u4e2d\u6bcf\u4e2a\u53d1\u660e\u901a\u8fc7\u63cf\u8ff0\u5176\u7279\u5f81\u53ca\u5176\u5173\u7cfb\u7684\u56fe\u6765\u8868\u793a\u3002\u6211\u4eec\u7684\u6a21\u578b\u4f7f\u7528\u4e13\u5229\u529e\u516c\u5ba4\u5ba1\u67e5\u5458\u7684\u73b0\u6709\u6280\u672f\u5f15\u7528\u4f5c\u4e3a\u76f8\u5173\u6027\u4fe1\u53f7\u6765\u5904\u7406\u8fd9\u4e9b\u53d1\u660e\u56fe\uff0c\u5e76\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5229\u7528\u56fe\u4f5c\u4e3a\u8f93\u5165\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u957f\u6587\u6863\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u5229\u7528\u5ba1\u67e5\u5458\u5f15\u7528\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u8d85\u8d8a\u7b80\u5355\u6587\u672c\u5339\u914d\u7684\u9886\u57df\u7279\u5b9a\u76f8\u4f3c\u6027\u3002\u641c\u7d22\u5f15\u64ce\u80fd\u591f\u6a21\u62df\u4e13\u4e1a\u4e13\u5229\u5ba1\u67e5\u5458\u7684\u5de5\u4f5c\u65b9\u5f0f\u8bc6\u522b\u76f8\u5173\u6587\u6863\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u663e\u793a\u5728\u73b0\u6709\u6280\u672f\u68c0\u7d22\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.10584", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10584", "abs": "https://arxiv.org/abs/2508.10584", "authors": ["Wencai Ye", "Mingjie Sun", "Shaoyun Shi", "Peng Wang", "Wenjin Wu", "Peng Jiang"], "title": "DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System", "comment": "Accepted by CIKM 2025", "summary": "Semantic IDs are discrete identifiers generated by quantizing the Multi-modal\nLarge Language Models (MLLMs) embeddings, enabling efficient multi-modal\ncontent integration in recommendation systems. However, their lack of\ncollaborative signals results in a misalignment with downstream discriminative\nand generative recommendation objectives. Recent studies have introduced\nvarious alignment mechanisms to address this problem, but their two-stage\nframework design still leads to two main limitations: (1) inevitable\ninformation loss during alignment, and (2) inflexibility in applying adaptive\nalignment strategies, consequently constraining the mutual information\nmaximization during the alignment process. To address these limitations, we\npropose a novel and flexible one-stage Dual-Aligned Semantic IDs (DAS) method\nthat simultaneously optimizes quantization and alignment, preserving semantic\nintegrity and alignment quality while avoiding the information loss typically\nassociated with two-stage methods. Meanwhile, DAS achieves more efficient\nalignment between the semantic IDs and collaborative signals, with the\nfollowing two innovative and effective approaches: (1) Multi-view Constrative\nAlignment: To maximize mutual information between semantic IDs and\ncollaborative signals, we first incorporate an ID-based CF debias module, and\nthen design three effective contrastive alignment methods: dual user-to-item\n(u2i), dual item-to-item/user-to-user (i2i/u2u), and dual co-occurrence\nitem-to-item/user-to-user (i2i/u2u). (2) Dual Learning: By aligning the dual\nquantizations of users and ads, the constructed semantic IDs for users and ads\nachieve stronger alignment. Finally, we conduct extensive offline experiments\nand online A/B tests to evaluate DAS's effectiveness, which is now successfully\ndeployed across various advertising scenarios at Kuaishou App, serving over 400\nmillion users daily.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u7075\u6d3b\u7684DAS\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u8bed\u4e49ID\u7684\u5bf9\u9f50\u95ee\u9898\u5e76\u6210\u529f\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u8bbe\u8ba1\u5bfc\u81f4\u7684\u4fe1\u606f\u635f\u5931\u548c\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u95ee\u9898\u9650\u5236\u4e86\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u7684\u4e92\u4fe1\u606f\u6700\u5927\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u4e00\u9636\u6bb5\u53cc\u5bf9\u9f50\u8bed\u4e49ID\uff08DAS\uff09\u65b9\u6cd5\uff0c\u4ee5\u540c\u65f6\u4f18\u5316\u91cf\u5316\u548c\u5bf9\u9f50\u3002", "result": "\u5728Kuaishou App\u4e0a\u6210\u529f\u90e8\u7f72\u5e94\u7528\uff0c\u6bcf\u65e5\u4e3a\u8d85\u8fc7400\u4e07\u7528\u6237\u63d0\u4f9b\u670d\u52a1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u4e2a\u5e7f\u544a\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u9636\u6bb5DAS\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u91cf\u5316\u548c\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u5bf9\u9f50\u8d28\u91cf\uff0c\u907f\u514d\u4e86\u4e24\u9636\u6bb5\u65b9\u6cd5\u5e38\u89c1\u7684\u4fe1\u606f\u635f\u5931\u3002"}}
{"id": "2508.10615", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10615", "abs": "https://arxiv.org/abs/2508.10615", "authors": ["Yufei Ye", "Wei Guo", "Hao Wang", "Hong Zhu", "Yuyang Ye", "Yong Liu", "Huifeng Guo", "Ruiming Tang", "Defu Lian", "Enhong Chen"], "title": "FuXi-\u03b2: Towards a Lightweight and Fast Large-Scale Generative Recommendation Model", "comment": null, "summary": "Scaling laws for autoregressive generative recommenders reveal potential for\nlarger, more versatile systems but mean greater latency and training costs. To\naccelerate training and inference, we investigated the recent generative\nrecommendation models HSTU and FuXi-$\\alpha$, identifying two efficiency\nbottlenecks: the indexing operations in relative temporal attention bias and\nthe computation of the query-key attention map. Additionally, we observed that\nrelative attention bias in self-attention mechanisms can also serve as\nattention maps. Previous works like Synthesizer have shown that alternative\nforms of attention maps can achieve similar performance, naturally raising the\nquestion of whether some attention maps are redundant. Through empirical\nexperiments, we discovered that using the query-key attention map might degrade\nthe model's performance in recommendation tasks. To address these bottlenecks,\nwe propose a new framework applicable to Transformer-like recommendation\nmodels. On one hand, we introduce Functional Relative Attention Bias, which\navoids the time-consuming operations of the original relative attention bias,\nthereby accelerating the process. On the other hand, we remove the query-key\nattention map from the original self-attention layer and design a new\nAttention-Free Token Mixer module. Furthermore, by applying this framework to\nFuXi-$\\alpha$, we introduce a new model, FuXi-$\\beta$. Experiments across\nmultiple datasets demonstrate that FuXi-$\\beta$ outperforms previous\nstate-of-the-art models and achieves significant acceleration compared to\nFuXi-$\\alpha$, while also adhering to the scaling law. Notably, FuXi-$\\beta$\nshows an improvement of 27% to 47% in the NDCG@10 metric on large-scale\nindustrial datasets compared to FuXi-$\\alpha$. Our code is available in a\npublic repository: https://github.com/USTC-StarTeam/FuXi-beta", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u8350\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u5584\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u63a8\u8350\u6027\u80fd\u548c\u8bad\u7ec3\u63a8\u7406\u901f\u5ea6\uff0c\u8868\u73b0\u8d85\u8fc7\u4e86\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u52a0\u5feb\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u89e3\u51b3\u751f\u6210\u63a8\u8350\u6a21\u578b\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u63d0\u5347\u63a8\u8350\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8eTransformer\u578b\u63a8\u8350\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u529f\u80fd\u76f8\u5bf9\u6ce8\u610f\u504f\u7f6e\u53ca\u65e0\u6ce8\u610f\u529b\u4ee4\u724c\u6df7\u5408\u6a21\u5757\u3002", "result": "FuXi-\u03b2 \u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u7684 NDCG@10 \u6307\u6807\u4e0a\u76f8\u6bd4 FuXi-\u03b1 \u63d0\u5347\u4e86 27% \u5230 47%\u3002", "conclusion": "FuXi-\u03b2 \u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u4e14\u76f8\u6bd4\u4e8e FuXi-\u03b1 \u6781\u5927\u63d0\u9ad8\u4e86\u901f\u5ea6\uff0c\u540c\u65f6\u9075\u5faa\u4e86\u6269\u5c55\u5b9a\u5f8b\u3002"}}
{"id": "2508.10753", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10753", "abs": "https://arxiv.org/abs/2508.10753", "authors": ["Zheyu Chen", "Jinfeng Xu", "Hewei Wang", "Shuo Yang", "Zitong Wan", "Haibo Hu"], "title": "Hypercomplex Prompt-aware Multimodal Recommendation", "comment": "Accepted by CIKM 2025", "summary": "Modern recommender systems face critical challenges in handling information\noverload while addressing the inherent limitations of multimodal representation\nlearning. Existing methods suffer from three fundamental limitations: (1)\nrestricted ability to represent rich multimodal features through a single\nrepresentation, (2) existing linear modality fusion strategies ignore the deep\nnonlinear correlations between modalities, and (3) static optimization methods\nfailing to dynamically mitigate the over-smoothing problem in graph\nconvolutional network (GCN). To overcome these limitations, we propose HPMRec,\na novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which\nutilizes hypercomplex embeddings in the form of multi-components to enhance the\nrepresentation diversity of multimodal features. HPMRec adopts the hypercomplex\nmultiplication to naturally establish nonlinear cross-modality interactions to\nbridge semantic gaps, which is beneficial to explore the cross-modality\nfeatures. HPMRec also introduces the prompt-aware compensation mechanism to aid\nthe misalignment between components and modality-specific features loss, and\nthis mechanism fundamentally alleviates the over-smoothing problem. It further\ndesigns self-supervised learning tasks that enhance representation diversity\nand align different modalities. Extensive experiments on four public datasets\nshow that HPMRec achieves state-of-the-art recommendation performance.", "AI": {"tldr": "HPMRec\u6846\u67b6\u4f18\u5316\u4e86\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u8d85\u590d\u6570\u5d4c\u5165\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u4fe1\u606f\u8fc7\u8f7d\u5904\u7406\u548c\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u56fa\u6709\u5c40\u9650\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Hypercomplex Prompt-aware Multimodal Recommendation\u6846\u67b6\uff0c\u5373HPMRec\uff0c\u5229\u7528\u591a\u7ec4\u4ef6\u7684\u8d85\u590d\u6570\u5d4c\u5165\u6765\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\u7684\u8868\u5f81\u591a\u6837\u6027\u3002\u8fdb\u884c\u8de8\u6a21\u6001\u7279\u5f81\u63a2\u7d22\u3002\u5f15\u5165\u4e86\u63d0\u793a\u611f\u77e5\u8865\u507f\u673a\u5236\u4ee5\u89e3\u51b3\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u4e22\u5931\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u4ee5\u589e\u5f3a\u8868\u5f81\u591a\u6837\u6027\u5e76\u5bf9\u9f50\u4e0d\u540c\u7684\u6a21\u6001\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHPMRec\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "HPMRec\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4e2d\u591a\u6a21\u6001\u7279\u5f81\u8868\u5f81\u4e0d\u8db3\u3001\u672a\u8003\u8651\u975e\u7ebf\u6027\u6a21\u6001\u878d\u5408\u4ee5\u53ca\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u63a8\u8350\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2508.10851", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10851", "abs": "https://arxiv.org/abs/2508.10851", "authors": ["Ze Liu", "Xianquan Wang", "Shuochen Liu", "Jie Ma", "Huibo Xu", "Yupeng Han", "Zhe Yang", "Kai Zhang", "Longfei Li", "Jun Zhou"], "title": "CrossDenoise: Denoising Implicit Feedback via a Lightweight Entity-Aware Synergistic Framework", "comment": null, "summary": "Recommender systems heavily rely on implicit feedback, which is inherently\nnoisy due to false positives and negatives, severely degrading recommendation\naccuracy. Existing denoising strategies often overlook entity-aware modeling,\nsuffer from high computational overhead, or demand excessive hyperparameter\ntuning, limiting their real-world applicability. We propose CrossDenoise, a\nnovel and lightweight framework that addresses these challenges by\ndisentangling noise estimation into user-, item-, and interaction-specific\nfactors. Leveraging empirical observations that show significant heterogeneity\nin user and item noise propensities, CrossDenoise computes entity reputation\nfactors (user/item reliability) via a rank-based linear mapping of average\ntraining losses. These are fused with interaction-level weights derived from an\nempirical cumulative distribution function (ECDF) of individual losses. This\ndesign is model-agnostic, computationally efficient, and requires only two\nintuitive hyperparameters. Extensive experiments on ML-1M, Yelp, and\nAmazon-book datasets, across GMF, NeuMF, and CDAE backbones, demonstrate that\nCrossDenoise consistently and significantly outperforms state-of-the-art\nbaselines. For instance, it achieves up to 27.01% NDCG@50 gain on Yelp with\nNeuMF, while incurring negligible computational and memory overhead. Our\nanalysis confirms that CrossDenoise effectively separates clean from noisy\nsamples and remains robust under varied hyperparameter settings. It offers a\npractical and scalable solution for denoising implicit feedback.", "AI": {"tldr": "CrossDenoise\u662f\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u9690\u5f0f\u53cd\u9988\u53bb\u566a\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u53ca\u53c2\u6570\u8c03\u8282\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e25\u91cd\u4f9d\u8d56\u9690\u5f0f\u53cd\u9988\uff0c\u4f46\u8fd9\u79cd\u53cd\u9988\u7531\u4e8e\u5b58\u5728\u5927\u91cf\u9519\u8bef\u6b63\u8d1f\u6837\u672c\u800c\u672c\u8d28\u4e0a\u5f88\u5608\u6742\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u63a8\u8350\u7684\u51c6\u786e\u6027\u3002\u73b0\u6709\u7684\u53bb\u566a\u7b56\u7565\u5f80\u5f80\u5ffd\u7565\u5b9e\u4f53\u611f\u77e5\u5efa\u6a21\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5f00\u9500\u6216\u9700\u8fc7\u591a\u7684\u8d85\u53c2\u6570\u8c03\u8282\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86CrossDenoise\uff0c\u4e00\u4e2a\u65b0\u9896\u4e14\u8f7b\u91cf\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u566a\u58f0\u4f30\u8ba1\u8fdb\u884c\u7528\u6237\u3001\u9879\u76ee\u548c\u4ea4\u4e92\u7279\u5b9a\u56e0\u7d20\u7684\u89e3\u8026\u6765\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\u3002\u6b64\u8bbe\u8ba1\u662f\u6a21\u578b\u4e0d\u53ef\u77e5\u7684\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\uff0c\u5e76\u4e14\u4ec5\u9700\u4e24\u4e2a\u76f4\u89c2\u8d85\u53c2\u6570\u3002", "result": "CrossDenoise\u5728ML-1M\u3001Yelp\u548cAmazon-book\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5728GMF\u3001NeuMF\u548cCDAE\u6a21\u578b\u4e0a\uff0c\u59cb\u7ec8\u663e\u8457\u5730\u4f18\u4e8e\u6700\u65b0\u7684\u57fa\u7ebf\u3002\u5b83\u5728Yelp\u4e0a\u4f7f\u7528NeuMF\u65f6\uff0c\u5728NDCG@50\u6307\u6807\u4e0a\u83b7\u5f97\u4e86\u6700\u9ad827.01%\u7684\u63d0\u5347\uff0c\u540c\u65f6\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "CrossDenoise\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u9645\u53ef\u884c\u4e14\u53ef\u6269\u5c55\u7684\u9690\u5f0f\u53cd\u9988\u53bb\u566a\u89e3\u51b3\u65b9\u6848\u3002\u5206\u6790\u8bc1\u660e\u5176\u6709\u6548\u5206\u79bb\u4e86\u6e05\u6670\u6837\u672c\u548c\u566a\u58f0\u6837\u672c\uff0c\u5e76\u5728\u5404\u79cd\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002"}}
