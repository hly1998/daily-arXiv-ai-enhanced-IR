{"id": "2509.14355", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.14355", "abs": "https://arxiv.org/abs/2509.14355", "authors": ["Dawn Lawrie", "Sean MacAvaney", "James Mayfield", "Paul McNamee", "Douglas W. Oard", "Luca Soldaini", "Eugene Yang"], "title": "Overview of the TREC 2024 NeuCLIR Track", "comment": "28 pages, 13 figures", "summary": "The principal goal of the TREC Neural Cross-Language Information Retrieval\n(NeuCLIR) track is to study the effect of neural approaches on cross-language\ninformation access. The track has created test collections containing Chinese,\nPersian, and Russian news stories and Chinese academic abstracts. NeuCLIR\nincludes four task types: Cross-Language Information Retrieval (CLIR) from\nnews, Multilingual Information Retrieval (MLIR) from news, Report Generation\nfrom news, and CLIR from technical documents. A total of 274 runs were\nsubmitted by five participating teams (and as baselines by the track\ncoordinators) for eight tasks across these four task types. Task descriptions\nand the available results are presented."}
{"id": "2509.14436", "categories": ["cs.IR", "cs.AI", "H.3.3; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2509.14436", "abs": "https://arxiv.org/abs/2509.14436", "authors": ["Lijia Ma", "Juan Qin", "Xingchen Xu", "Yong Tan"], "title": "When Content is Goliath and Algorithm is David: The Style and Semantic Effects of Generative Search Engine", "comment": "59 pages, 6 figures, 20 tables", "summary": "Generative search engines (GEs) leverage large language models (LLMs) to\ndeliver AI-generated summaries with website citations, establishing novel\ntraffic acquisition channels while fundamentally altering the search engine\noptimization landscape. To investigate the distinctive characteristics of GEs,\nwe collect data through interactions with Google's generative and conventional\nsearch platforms, compiling a dataset of approximately ten thousand websites\nacross both channels. Our empirical analysis reveals that GEs exhibit\npreferences for citing content characterized by significantly higher\npredictability for underlying LLMs and greater semantic similarity among\nselected sources. Through controlled experiments utilizing retrieval augmented\ngeneration (RAG) APIs, we demonstrate that these citation preferences emerge\nfrom intrinsic LLM tendencies to favor content aligned with their generative\nexpression patterns. Motivated by applications of LLMs to optimize website\ncontent, we conduct additional experimentation to explore how LLM-based content\npolishing by website proprietors alters AI summaries, finding that such\npolishing paradoxically enhances information diversity within AI summaries.\nFinally, to assess the user-end impact of LLM-induced information increases, we\ndesign a generative search engine and recruit Prolific participants to conduct\na randomized controlled experiment involving an information-seeking and writing\ntask. We find that higher-educated users exhibit minimal changes in their final\noutputs' information diversity but demonstrate significantly reduced task\ncompletion time when original sites undergo polishing. Conversely,\nlower-educated users primarily benefit through enhanced information density in\ntheir task outputs while maintaining similar completion times across\nexperimental groups."}
{"id": "2509.14457", "categories": ["cs.IR", "cs.DB", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14457", "abs": "https://arxiv.org/abs/2509.14457", "authors": ["Lisa-Yao Gan", "Arunav Das", "Johanna Walker", "Elena Simperl"], "title": "Keywords are not always the key: A metadata field analysis for natural language search on open data portals", "comment": "Accepted to CHIRA 2025 as Full Paper", "summary": "Open data portals are essential for providing public access to open datasets.\nHowever, their search interfaces typically rely on keyword-based mechanisms and\na narrow set of metadata fields. This design makes it difficult for users to\nfind datasets using natural language queries. The problem is worsened by\nmetadata that is often incomplete or inconsistent, especially when users lack\nfamiliarity with domain-specific terminology. In this paper, we examine how\nindividual metadata fields affect the success of conversational dataset\nretrieval and whether LLMs can help bridge the gap between natural queries and\nstructured metadata. We conduct a controlled ablation study using simulated\nnatural language queries over real-world datasets to evaluate retrieval\nperformance under various metadata configurations. We also compare existing\ncontent of the metadata field 'description' with LLM-generated content,\nexploring how different prompting strategies influence quality and impact on\nsearch outcomes. Our findings suggest that dataset descriptions play a central\nrole in aligning with user intent, and that LLM-generated descriptions can\nsupport effective retrieval. These results highlight both the limitations of\ncurrent metadata practices and the potential of generative models to improve\ndataset discoverability in open data portals."}
{"id": "2509.14979", "categories": ["cs.IR", "H.3.3; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.14979", "abs": "https://arxiv.org/abs/2509.14979", "authors": ["Kainan Shi", "Peilin Zhou", "Ge Wang", "Han Ding", "Fei Wang"], "title": "What Matters in LLM-Based Feature Extractor for Recommender? A Systematic Analysis of Prompts, Models, and Adaptation", "comment": "9 pages. Keywords: Recommender Systems, Large Language Models,\n  Sequential Recommendation, Feature Extraction", "summary": "Using Large Language Models (LLMs) to generate semantic features has been\ndemonstrated as a powerful paradigm for enhancing Sequential Recommender\nSystems (SRS). This typically involves three stages: processing item text,\nextracting features with LLMs, and adapting them for downstream models.\nHowever, existing methods vary widely in prompting, architecture, and\nadaptation strategies, making it difficult to fairly compare design choices and\nidentify what truly drives performance. In this work, we propose RecXplore, a\nmodular analytical framework that decomposes the LLM-as-feature-extractor\npipeline into four modules: data processing, semantic feature extraction,\nfeature adaptation, and sequential modeling. Instead of proposing new\ntechniques, RecXplore revisits and organizes established methods, enabling\nsystematic exploration of each module in isolation. Experiments on four public\ndatasets show that simply combining the best designs from existing techniques\nwithout exhaustive search yields up to 18.7% relative improvement in NDCG@5 and\n12.7% in HR@5 over strong baselines. These results underscore the utility of\nmodular benchmarking for identifying effective design patterns and promoting\nstandardized research in LLM-enhanced recommendation."}
