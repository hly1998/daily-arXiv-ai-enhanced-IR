{"id": "2509.07133", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07133", "abs": "https://arxiv.org/abs/2509.07133", "authors": ["Fernando Spadea", "Oshani Seneviratne"], "title": "Avoiding Over-Personalization with Rule-Guided Knowledge Graph Adaptation for LLM Recommendations", "comment": "5 pages, 2 figures, ISWC", "summary": "We present a lightweight neuro-symbolic framework to mitigate\nover-personalization in LLM-based recommender systems by adapting user-side\nKnowledge Graphs (KGs) at inference time. Instead of retraining models or\nrelying on opaque heuristics, our method restructures a user's Personalized\nKnowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce\nPersonalized Information Environments (PIEs), i.e., algorithmically induced\nfilter bubbles that constrain content diversity. These adapted PKGs are used to\nconstruct structured prompts that steer the language model toward more diverse,\nOut-PIE recommendations while preserving topical relevance. We introduce a\nfamily of symbolic adaptation strategies, including soft reweighting, hard\ninversion, and targeted removal of biased triples, and a client-side learning\nalgorithm that optimizes their application per user. Experiments on a recipe\nrecommendation benchmark show that personalized PKG adaptations significantly\nincrease content novelty while maintaining recommendation quality,\noutperforming global adaptation and naive prompt-based methods."}
{"id": "2509.07163", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07163", "abs": "https://arxiv.org/abs/2509.07163", "authors": ["Haike Xu", "Tong Chen"], "title": "Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval", "comment": null, "summary": "The widely used retrieve-and-rerank pipeline faces two critical limitations:\nthey are constrained by the initial retrieval quality of the top-k documents,\nand the growing computational demands of LLM-based rerankers restrict the\nnumber of documents that can be effectively processed. We introduce\nReranker-Guided-Search (RGS), a novel approach that bypasses these limitations\nby directly retrieving documents according to reranker preferences rather than\nfollowing the traditional sequential reranking method. Our method uses a greedy\nsearch on proximity graphs generated by approximate nearest neighbor\nalgorithms, strategically prioritizing promising documents for reranking based\non document similarity. Experimental results demonstrate substantial\nperformance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9\non FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100\ndocuments. Our analysis suggests that, given a fixed pair of embedding and\nreranker models, strategically selecting documents to rerank can significantly\nimprove retrieval accuracy under limited reranker budget."}
{"id": "2509.07253", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07253", "abs": "https://arxiv.org/abs/2509.07253", "authors": ["Julian Killingback", "Hamed Zamani"], "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks", "comment": null, "summary": "Large language models (LLMs) are incredible and versatile tools for\ntext-based tasks that have enabled countless, previously unimaginable,\napplications. Retrieval models, in contrast, have not yet seen such capable\ngeneral-purpose models emerge. To achieve this goal, retrieval models must be\nable to perform complex retrieval tasks, where queries contain multiple parts,\nconstraints, or requirements in natural language. These tasks represent a\nnatural progression from the simple, single-aspect queries that are used in the\nvast majority of existing, commonly used evaluation sets. Complex queries\nnaturally arise as people expect search systems to handle more specific and\noften ambitious information requests, as is demonstrated by how people use\nLLM-based information systems. Despite the growing desire for retrieval models\nto expand their capabilities in complex retrieval tasks, there exist limited\nresources to assess the ability of retrieval models on a comprehensive set of\ndiverse complex tasks. The few resources that do exist feature a limited scope\nand often lack realistic settings making it hard to know the true capabilities\nof retrieval models on complex real-world retrieval tasks. To address this\nshortcoming and spur innovation in next-generation retrieval models, we\nconstruct a diverse and realistic set of complex retrieval tasks and benchmark\na representative set of state-of-the-art retrieval models. Additionally, we\nexplore the impact of LLM-based query expansion and rewriting on retrieval\nquality. Our results show that even the best models struggle to produce\nhigh-quality retrieval results with the highest average nDCG@10 of only 0.346\nand R@100 of only 0.587 across all tasks. Although LLM augmentation can help\nweaker models, the strongest model has decreased performance across all metrics\nwith all rewriting techniques."}
{"id": "2509.07269", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07269", "abs": "https://arxiv.org/abs/2509.07269", "authors": ["Amelia Kovacs", "Jerry Chee", "Kimia Kazemian", "Sarah Dean"], "title": "Datasets for Navigating Sensitive Topics in Recommendation Systems", "comment": "Companion Proceedings of the ACM on Web Conference 2025, 2025", "summary": "Personalized AI systems, from recommendation systems to chatbots, are a\nprevalent method for distributing content to users based on their learned\npreferences. However, there is growing concern about the adverse effects of\nthese systems, including their potential tendency to expose users to sensitive\nor harmful material, negatively impacting overall well-being. To address this\nconcern quantitatively, it is necessary to create datasets with relevant\nsensitivity labels for content, enabling researchers to evaluate personalized\nsystems beyond mere engagement metrics. To this end, we introduce two novel\ndatasets that include a taxonomy of sensitivity labels alongside user-content\nratings: one that integrates MovieLens rating data with content warnings from\nthe Does the Dog Die? community ratings website, and another that combines\nfan-fiction interaction data and user-generated warnings from Archive of Our\nOwn."}
{"id": "2509.07319", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07319", "abs": "https://arxiv.org/abs/2509.07319", "authors": ["Yunxiao Shi", "Shuo Yang", "Haimin Zhang", "Li Wang", "Yongze Wang", "Qiang Wu", "Min Xu"], "title": "MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for Neural Recommendation Models", "comment": "Accepted by Data Mining and Knowledge Discovery (DMKD) in Sep 2025", "summary": "Neural Collaborative Filtering models are widely used in recommender systems\nbut are typically trained under static settings, assuming fixed data\ndistributions. This limits their applicability in dynamic environments where\nuser preferences evolve. Incremental learning offers a promising solution, yet\nconventional methods from computer vision or NLP face challenges in\nrecommendation tasks due to data sparsity and distinct task paradigms. Existing\napproaches for neural recommenders remain limited and often lack\ngeneralizability. To address this, we propose MEGG, Replay Samples with\nMaximally Extreme GGscore, an experience replay based incremental learning\nframework. MEGG introduces GGscore, a novel metric that quantifies sample\ninfluence, enabling the selective replay of highly influential samples to\nmitigate catastrophic forgetting. Being model-agnostic, MEGG integrates\nseamlessly across architectures and frameworks. Experiments on three neural\nmodels and four benchmark datasets show superior performance over\nstate-of-the-art baselines, with strong scalability, efficiency, and\nrobustness. Implementation will be released publicly upon acceptance."}
{"id": "2509.07485", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07485", "abs": "https://arxiv.org/abs/2509.07485", "authors": ["Jeongwoo Na", "Jun Kwon", "Eunseong Choi", "Jongwuk Lee"], "title": "Multi-view-guided Passage Reranking with Large Language Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have shown impressive\nperformance in passage reranking tasks. Despite their success, LLM-based\nmethods still face challenges in efficiency and sensitivity to external biases.\n(1) Existing models rely mostly on autoregressive generation and sliding window\nstrategies to rank passages, which incur heavy computational overhead as the\nnumber of passages increases. (2) External biases, such as position or\nselection bias, hinder the model's ability to accurately represent passages and\nincrease input-order sensitivity. To address these limitations, we introduce a\nnovel passage reranking model, called Multi-View-guided Passage Reranking\n(MVP). MVP is a non-generative LLM-based reranking method that encodes\nquery-passage information into diverse view embeddings without being influenced\nby external biases. For each view, it combines query-aware passage embeddings\nto produce a distinct anchor vector, which is then used to directly compute\nrelevance scores in a single decoding step. In addition, it employs an\northogonal loss to make the views more distinctive. Extensive experiments\ndemonstrate that MVP, with just 220M parameters, matches the performance of\nmuch larger 7B-scale fine-tuned models while achieving a 100x reduction in\ninference latency. Notably, the 3B-parameter variant of MVP achieves\nstate-of-the-art performance on both in-domain and out-of-domain benchmarks.\nThe source code is available at: https://github.com/bulbna/MVP"}
{"id": "2509.07531", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07531", "abs": "https://arxiv.org/abs/2509.07531", "authors": ["Zheng Dou", "Deqing Wang", "Fuzhen Zhuang", "Jian Ren", "Yanlin Hu"], "title": "FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents", "comment": "Accepted by DASFAA2025", "summary": "Scientific document representation learning provides powerful embeddings for\nvarious tasks, while current methods face challenges across three approaches.\n1) Contrastive training with citation-structural signals underutilizes citation\ninformation and still generates single-vector representations. 2) Fine-grained\nrepresentation learning, which generates multiple vectors at the sentence or\naspect level, requires costly integration and lacks domain generalization. 3)\nTask-aware learning depends on manually predefined task categorization,\noverlooking nuanced task distinctions and requiring extra training data for\ntask-specific modules. To address these problems, we propose a new method that\nunifies the three approaches for better representations, namely FLeW.\nSpecifically, we introduce a novel triplet sampling method that leverages\ncitation intent and frequency to enhance citation-structural signals for\ntraining. Citation intents (background, method, result), aligned with the\ngeneral structure of scientific writing, facilitate a domain-generalized facet\npartition for fine-grained representation learning. Then, we adopt a simple\nweight search to adaptively integrate three facet-level embeddings into a\ntask-specific document embedding without task-aware fine-tuning. Experiments\nshow the applicability and robustness of FLeW across multiple scientific tasks\nand fields, compared to prior models."}
{"id": "2509.07594", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07594", "abs": "https://arxiv.org/abs/2509.07594", "authors": ["Rui Dong", "Wentao Ouyang", "Xiangzheng Liu"], "title": "ELEC: Efficient Large Language Model-Empowered Click-Through Rate Prediction", "comment": "SIGIR 2025", "summary": "Click-through rate (CTR) prediction plays an important role in online\nadvertising systems. On the one hand, traditional CTR prediction models capture\nthe collaborative signals in tabular data via feature interaction modeling, but\nthey lose semantics in text. On the other hand, Large Language Models (LLMs)\nexcel in understanding the context and meaning behind text, but they face\nchallenges in capturing collaborative signals and they have long inference\nlatency. In this paper, we aim to leverage the benefits of both types of models\nand pursue collaboration, semantics and efficiency. We present ELEC, which is\nan Efficient LLM-Empowered CTR prediction framework. We first adapt an LLM for\nthe CTR prediction task. In order to leverage the ability of the LLM but\nsimultaneously keep efficiency, we utilize the pseudo-siamese network which\ncontains a gain network and a vanilla network. We inject the high-level\nrepresentation vector generated by the LLM into a collaborative CTR model to\nform the gain network such that it can take advantage of both tabular modeling\nand textual modeling. However, its reliance on the LLM limits its efficiency.\nWe then distill the knowledge from the gain network to the vanilla network on\nboth the score level and the representation level, such that the vanilla\nnetwork takes only tabular data as input, but can still generate comparable\nperformance as the gain network. Our approach is model-agnostic. It allows for\nthe integration with various existing LLMs and collaborative CTR models.\nExperiments on real-world datasets demonstrate the effectiveness and efficiency\nof ELEC for CTR prediction."}
{"id": "2509.07620", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07620", "abs": "https://arxiv.org/abs/2509.07620", "authors": ["Viju Sudhi", "Sinchana Ramakanth Bhat", "Max Rudat", "Roman Teucher", "Nicolas Flores-Herr"], "title": "Towards End-to-End Model-Agnostic Explanations for RAG Systems", "comment": "Accepted to Workshop on Explainability in Information Retrieval\n  (WExIR), SIGIR 2025 - July 17, 2025", "summary": "Retrieval Augmented Generation (RAG) systems, despite their growing\npopularity for enhancing model response reliability, often struggle with\ntrustworthiness and explainability. In this work, we present a novel, holistic,\nmodel-agnostic, post-hoc explanation framework leveraging perturbation-based\ntechniques to explain the retrieval and generation processes in a RAG system.\nWe propose different strategies to evaluate these explanations and discuss the\nsufficiency of model-agnostic explanations in RAG systems. With this work, we\nfurther aim to catalyze a collaborative effort to build reliable and\nexplainable RAG systems."}
{"id": "2509.07759", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07759", "abs": "https://arxiv.org/abs/2509.07759", "authors": ["Minghan Li", "Miyang Luo", "Tianrui Lv", "Yishuai Zhang", "Siqi Zhao", "Ercong Nie", "Guodong Zhou"], "title": "A Survey of Long-Document Retrieval in the PLM and LLM Era", "comment": "33 pages, 6 figures", "summary": "The proliferation of long-form documents presents a fundamental challenge to\ninformation retrieval (IR), as their length, dispersed evidence, and complex\nstructures demand specialized methods beyond standard passage-level techniques.\nThis survey provides the first comprehensive treatment of long-document\nretrieval (LDR), consolidating methods, challenges, and applications across\nthree major eras. We systematize the evolution from classical lexical and early\nneural models to modern pre-trained (PLM) and large language models (LLMs),\ncovering key paradigms like passage aggregation, hierarchical encoding,\nefficient attention, and the latest LLM-driven re-ranking and retrieval\ntechniques. Beyond the models, we review domain-specific applications,\nspecialized evaluation resources, and outline critical open challenges such as\nefficiency trade-offs, multimodal alignment, and faithfulness. This survey aims\nto provide both a consolidated reference and a forward-looking agenda for\nadvancing long-document retrieval in the era of foundation models."}
{"id": "2509.07794", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07794", "abs": "https://arxiv.org/abs/2509.07794", "authors": ["Minghan Li", "Xinxuan Lv", "Junjie Zou", "Tongna Chen", "Chao Zhang", "Suchao An", "Ercong Nie", "Guodong Zhou"], "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A Comprehensive Survey", "comment": "38 pages,3 figures", "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and\never more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key\nmechanism for mitigating vocabulary mismatch, but the design space has shifted\nmarkedly with pre-trained language models (PLMs) and large language models\n(LLMs). This survey synthesizes the field from three angles: (i) a\nfour-dimensional framework of query expansion - from the point of injection\n(explicit vs. implicit QE), through grounding and interaction (knowledge bases,\nmodel-internal capabilities, multi-turn retrieval) and learning alignment, to\nknowledge graph-based argumentation; (ii) a model-centric taxonomy spanning\nencoder-only, encoder-decoder, decoder-only, instruction-tuned, and\ndomain/multilingual variants, highlighting their characteristic affordances for\nQE (contextual disambiguation, controllable generation, zero-/few-shot\nreasoning); and (iii) practice-oriented guidance on where and how neural QE\nhelps in first-stage retrieval, multi-query fusion, re-ranking, and\nretrieval-augmented generation (RAG). We compare traditional query expansion\nwith PLM/LLM-based methods across seven key aspects, and we map applications\nacross web search, biomedicine, e-commerce, open-domain QA/RAG, conversational\nand code search, and cross-lingual settings. The review distills design\ngrounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG\nconstraints - as robust remedies to topic drift and hallucination. We conclude\nwith an agenda on quality control, cost-aware invocation, domain/temporal\nadaptation, evaluation beyond end-task metrics, and fairness/privacy.\nCollectively, these insights provide a principled blueprint for selecting and\ncombining QE techniques under real-world constraints."}
{"id": "2509.07860", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07860", "abs": "https://arxiv.org/abs/2509.07860", "authors": ["Guanzhi Deng", "Yi Xie", "Yu-Keung Ng", "Mingyang Liu", "Peijun Zheng", "Jie Liu", "Dapeng Wu", "Yinqiao Li", "Linqi Song"], "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis", "comment": null, "summary": "Effectively managing intellectual property is a significant challenge.\nTraditional methods for patent analysis depend on labor-intensive manual\nsearches and rigid keyword matching. These approaches are often inefficient and\nstruggle to reveal the complex relationships hidden within large patent\ndatasets, hindering strategic decision-making. To overcome these limitations,\nwe introduce KLIPA, a novel framework that leverages a knowledge graph and a\nlarge language model (LLM) to significantly advance patent analysis. Our\napproach integrates three key components: a structured knowledge graph to map\nexplicit relationships between patents, a retrieval-augmented generation(RAG)\nsystem to uncover contextual connections, and an intelligent agent that\ndynamically determines the optimal strategy for resolving user queries. We\nvalidated KLIPA on a comprehensive, real-world patent database, where it\ndemonstrated substantial improvements in knowledge extraction, discovery of\nnovel connections, and overall operational efficiency. This combination of\ntechnologies enhances retrieval accuracy, reduces reliance on domain experts,\nand provides a scalable, automated solution for any organization managing\nintellectual property, including technology corporations and legal firms,\nallowing them to better navigate the complexities of strategic innovation and\ncompetitive intelligence."}
