<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 16]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: 该论文介绍了db3团队在KDD Cup'25 Meta CRAG-MM挑战赛中获胜的解决方案，特别是在第一人称视角问题处理方面表现出色，赢得了总奖。


<details>
  <summary>Details</summary>
Motivation: 为了在Meta CRAG-MM Challenge 2025中应对独特的多模态、多轮次问题回答基准（CRAG-MM），开发出一个全面的框架。

Method: 综合框架集成了针对不同任务的检索流程以及统一的LLM调优方法，用于控制幻觉；并采用SFT、DPO和RL进行高级拒绝训练。

Result: 获得任务1的第二名，任务2的第二名以及任务3的第一名，最终赢得了以一流处理第一人称视角问题为特点的大奖。

Conclusion: 通过结合为不同任务量身定制的检索管道与统一的LLM调优方法，成功控制了幻觉问题，并在比赛中取得了骄人的成绩。

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [2] [Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs](https://arxiv.org/abs/2509.09682)
*Maxim Zhelnin,Dmitry Redko,Volkov Daniil,Anna Volodkevich,Petr Sokerin,Valeriy Shevchenko,Egor Shvetsov,Alexey Vasilev,Darya Denisova,Ruslan Izmailov,Alexey Zaytsev*

Main category: cs.IR

TL;DR: 提出CCE-方法，通过优化交叉熵损失和负采样，显著降低GPU内存消耗并提高训练速度和模型性能。


<details>
  <summary>Details</summary>
Motivation: 训练基于Transformer的顺序推荐模型的计算成本高，尤其是在处理超过数千个商品的目录时。为了减少内存消耗，通常会结合负采样来使用交叉熵损失，但这可能会损害性能。

Method: 引入CCE-方法，实现了一种GPU高效的交叉熵损失与负采样结合的方法，可以加快训练速度并显著减少内存消耗。

Result: CCE-方法能够将训练加速至两倍，同时内存消耗减少十倍以上，可在大规模商品目录上提高模型的准确性。

Conclusion: 在大规模商品目录的数据集上，使用CCE-方法进行模型训练可以比原始的PyTorch实现的损失函数更具准确性。

Abstract: Sequential recommendations (SR) with transformer-based architectures are
widely adopted in real-world applications, where SR models require frequent
retraining to adapt to ever-changing user preferences. However, training
transformer-based SR models often encounters a high computational cost
associated with scoring extensive item catalogs, often exceeding thousands of
items. This occurs mainly due to the use of cross-entropy loss, where peak
memory scales proportionally to catalog size, batch size, and sequence length.
Recognizing this, practitioners in the field of recommendation systems
typically address memory consumption by integrating the cross-entropy (CE) loss
with negative sampling, thereby reducing the explicit memory demands of the
final layer. However, a small number of negative samples would degrade model
performance, and as we demonstrate in our work, increasing the number of
negative samples and the batch size further improves the model's performance,
but rapidly starts to exceed industrial GPUs' size (~40Gb).
  In this work, we introduce the CCE- method, which offers a GPU-efficient
implementation of the CE loss with negative sampling. Our method accelerates
training by up to two times while reducing memory consumption by more than 10
times. Leveraging the memory savings afforded by using CCE- for model training,
it becomes feasible to enhance its accuracy on datasets with a large item
catalog compared to those trained with original PyTorch-implemented loss
functions. Finally, we perform an analysis of key memory-related
hyperparameters and highlight the necessity of a delicate balance among these
factors. We demonstrate that scaling both the number of negative samples and
batch size leads to better results rather than maximizing only one of them. To
facilitate further adoption of CCE-, we release a Triton kernel that
efficiently implements the proposed method.

</details>


### [3] [Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs](https://arxiv.org/abs/2509.09683)
*Briti Gangopadhyay,Zhao Wang,Shingo Takamatsu*

Main category: cs.IR

TL;DR: 本文提出了结合点击数据和关键词文本更新的多模态预测框架，使用强化学习提高文本信息理解和信息融合。实验结果表明该方法在准确性和推理质量上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型仅使用数值数据，忽略了关键词更新等文本信息的潜在价值，因此提出了结合文本和数值数据的多模态框架。

Method: 采用多模态预测框架，结合点击数据和文本日志信息，并通过强化学习优化文本信息理解及模态融合。

Result: 我们的多模态预测框架结合了点击数据和广告活动的文本日志，生成了易于人类解释的预测结果，并在数值预测中加入了解释功能。

Conclusion: 该多模态框架通过结合点击数据和文本信息显著提高了广告点击量预测的准确性和解释性。

Abstract: Forecasting click volume is a key task in digital advertising, influencing
both revenue and campaign strategy. Traditional time series models rely solely
on numerical data, often overlooking rich contextual information embedded in
textual elements, such as keyword updates. We present a multimodal forecasting
framework that combines click data with textual logs from real-world ad
campaigns and generates human-interpretable explanations alongside numeric
predictions. Reinforcement learning is used to improve comprehension of textual
information and enhance fusion of modalities. Experiments on a large-scale
industry dataset show that our method outperforms baselines in both accuracy
and reasoning quality.

</details>


### [4] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: 介绍了一个组用于过程挖掘领域的text-to-SQL任务的双语（葡萄牙语-英语）基准数据集。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是增强非SQL专家用户的数据库查询能力，提高SQL专家的生产力。

Method: 该论文的方法包括：专家手动策划、专业翻译以及详细注释过程，以实现任务复杂性的细致分析。此外，使用GPT-3.5 Turbo进行基线研究，展示数据集在text-to-SQL应用中的可行性和实用性。

Result: 结果表明，text-2-SQL-4-PM数据集支持text-to-SQL实现的评估，并为语义解析和其他自然语言处理任务提供了更广泛的应用。

Conclusion: 论文的结论是，text-2-SQL-4-PM数据集增强了非SQL专家用户的数据库查询能力，提高了SQL专家的生产力，同时展示了数据集在text-to-SQL实现评估中的支持能力。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [5] [TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation](https://arxiv.org/abs/2509.09685)
*Keunwoo Choi,Seungheon Doh,Juhan Nam*

Main category: cs.IR

TL;DR: TalkPlayData 2是一个生成多模态对话音乐推荐数据的合成数据集，并且证明其性能良好。


<details>
  <summary>Details</summary>
Motivation: 为了提升多模态对话音乐推荐系统的性能，研究者创造了TalkPlayData 2，一个由代理数据管道生成的合成数据集。

Method: 采用多模态大型语言模型（LLM）代理，根据不同角色使用专业化提示和有选择性的信息访问，通过记录听众LLM和推荐系统LLM之间的对话获取聊天数据，采用调优的会话目标指导听众LLM，从而覆盖多种会话场景，模拟多模态推荐和对话。

Result: TalkPlayData 2在LLM作为评判者和主观评价实验中在相关音乐推荐生成模型训练方面达到了预期目标。

Conclusion: TalkPlayData 2有效生成了用于音乐推荐的多模态对话数据，并开源其生成代码，促进相关领域的发展。

Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational
music recommendation generated by an agentic data pipeline. In TalkPlayData 2
pipeline, multiple large language model (LLM) agents are created under various
roles with specialized prompts and access to different parts of information,
and the chat data is acquired by logging the conversation between the Listener
LLM and the Recsys LLM. To cover various conversation scenarios, for each
conversation, the Listener LLM is conditioned on a finetuned conversation goal.
Finally, all the LLMs are multimodal with audio and images, allowing a
simulation of multimodal recommendation and conversation. In the LLM-as-a-judge
and subjective evaluation experiments, TalkPlayData 2 achieved the proposed
goal in various aspects related to training a generative recommendation model
for music. TalkPlayData 2 and its generation code are open-sourced at
https://talkpl.ai/talkplaydata2.html.

</details>


### [6] [GeoGPT.RAG Technical Report](https://arxiv.org/abs/2509.09686)
*Fei Huang,Fan Wu,Zeqing Zhang,Qihao Wang,Long Zhang,Grant Michael Boquet,Hongyang Chen*

Main category: cs.IR

TL;DR: GeoGPT通过RAG技术增强模型输出，提供精准地学回答。开放GeoEmbedding和GeoReranker组件以支持全球研究人员。


<details>
  <summary>Details</summary>
Motivation: 提升地球科学领域的研究能力。通过GeoGPT系统提供精准、可信的答案，推动开放科学。

Method: 集成了检索增强生成（RAG），通过GeoGPT Library 提供上下文相关的答案，并允许用户创建个性化知识库；提升了嵌入模型与排序模型以改善检索质量。

Result: GeoGPT成功优化了检索增强生成（RAG）技术，显著提升了地球科学应用的回答准确性和可信度。

Conclusion: GeoGPT展示了开放科学的承诺，通过开放关键技术组件，支持全球地球科学研究。

Abstract: GeoGPT is an open large language model system built to advance research in
the geosciences. To enhance its domain-specific capabilities, we integrated
Retrieval Augmented Generation(RAG), which augments model outputs with relevant
information retrieved from an external knowledge source. GeoGPT uses RAG to
draw from the GeoGPT Library, a specialized corpus curated for geoscientific
content, enabling it to generate accurate, context-specific answers. Users can
also create personalized knowledge bases by uploading their own publication
lists, allowing GeoGPT to retrieve and respond using user-provided materials.
To further improve retrieval quality and domain alignment, we fine-tuned both
the embedding model and a ranking model that scores retrieved passages by
relevance to the query. These enhancements optimize RAG for geoscience
applications and significantly improve the system's ability to deliver precise
and trustworthy outputs. GeoGPT reflects a strong commitment to open science
through its emphasis on collaboration, transparency, and community driven
development. As part of this commitment, we have open-sourced two core RAG
components-GeoEmbedding and GeoReranker-to support geoscientists, researchers,
and professionals worldwide with powerful, accessible AI tools.

</details>


### [7] [Demonstrating Narrative Pattern Discovery from Biomedical Literature](https://arxiv.org/abs/2509.09687)
*Hermann Kroll,Pascal Sackhoff,Bill Matthias Thang,Christin Katharina Kreutz,Wolf-Tilo Balke*

Main category: cs.IR

TL;DR: 提出了一种新的搜索功能“叙事模式挖掘”，以帮助用户在数字图书馆中探索相关实体及其交互，并通过访谈验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 数字图书馆需要为用户提供有效的检索途径，例如PubPharm提供并开发获取生物医学文献集合的路径，而传统的关键词搜索不能满足当前用户需求，故提出更先进的搜索方法。

Method: 引入“叙事模式挖掘”功能，通过采访五位领域专家以验证其原型的实用性。

Result: 新型搜索功能“叙事模式挖掘”可以有效地探索上下文相关实体和实体交互。

Conclusion: 此研究展示了在数字图书馆中引入新型搜索功能“叙事模式挖掘”是有效的，经过与领域专家的访谈验证其原型的有用性。

Abstract: Digital libraries maintain extensive collections of knowledge and need to
provide effective access paths for their users. For instance, PubPharm, the
specialized information service for Pharmacy in Germany, provides and develops
access paths to their underlying biomedical document collection. In brief,
PubPharm supports traditional keyword-based search, search for chemical
structures, as well as novel graph-based discovery workflows, e.g., listing or
searching for interactions between different pharmaceutical entities. This
paper introduces a new search functionality, called narrative pattern mining,
allowing users to explore context-relevant entities and entity interactions. We
performed interviews with five domain experts to verify the usefulness of our
prototype.

</details>


### [8] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: 介绍RHIC数据和分析保存计划，通过AI助手增强数据可重复性和可发现性，提高科学遗产数据的可用性。


<details>
  <summary>Details</summary>
Motivation: RHIC数据和科学知识保存的必要性，以及支持实验数据的重复性、教育和未来发现的需求。

Method: 使用大型语言模型结合检索增强生成和模型上下文协议构建AI助手系统，以支持领域适应性互动。

Result: 成功部署了AI助手系统，并实现多实验整合，改善了科学遗产数据的可用性和易发现性。系统具备可持续且可解释的长效AI访问架构。

Conclusion: 现代AI/ML工具可以显著提高科学遗产数据的可用性和发现性。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [9] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 利用大语言模型提取用户表示，微调小型语言模型模拟用户行为，提升推荐系统表现。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型难以准确模拟用户行为，主因是用户互动行为复杂且随机。

Method: 使用冻结的大型语言模型（LLM）提取用户的文本表示，并利用经过微调的小型语言模型（SLM）模拟用户代理。此外，还开发了多种低阶适配器以优化用户组的表现。

Result: 实验结果表明，使用该方法开发的用户代理能够有效提升推荐系统的离线指标与实际表现之间的对接能力。

Conclusion: 展示了一种能够在规模与性能之间取得平衡的用户行为模拟方法，有助于改进推荐系统的实际表现。

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


### [10] [Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems](https://arxiv.org/abs/2509.09690)
*Ping Liu,Jianqiang Shen,Qianqi Shen,Chunnan Yao,Kevin Kao,Dan Xu,Rajat Arora,Baofen Zheng,Caleb Johnson,Liangjie Hong,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 引入一个利用LLM的统一查询理解框架，提升查询推荐质量，降低系统复杂性。


<details>
  <summary>Details</summary>
Motivation: 现代相关性系统需要理解用户的短查询，这些查询常常模糊且高度依赖上下文，传统方法维护成本高且适应性差。

Method: 通过联合建模用户查询和上下文信号（如个人资料属性）以生成结构化解释，并应用于个性化推荐。

Result: 框架在在线A/B测试中改善了相关性质量，同时显著减少了系统的复杂性和运营开销。

Conclusion: 提出了一种用于查询理解的统一框架，该框架利用大型语言模型（LLM）去应对传统方法的限制。

Abstract: Query understanding is essential in modern relevance systems, where user
queries are often short, ambiguous, and highly context-dependent. Traditional
approaches often rely on multiple task-specific Named Entity Recognition models
to extract structured facets as seen in job search applications. However, this
fragmented architecture is brittle, expensive to maintain, and slow to adapt to
evolving taxonomies and language patterns. In this paper, we introduce a
unified query understanding framework powered by a Large Language Model (LLM),
designed to address these limitations. Our approach jointly models the user
query and contextual signals such as profile attributes to generate structured
interpretations that drive more accurate and personalized recommendations. The
framework improves relevance quality in online A/B testing while significantly
reducing system complexity and operational overhead. The results demonstrate
that our solution provides a scalable and adaptable foundation for query
understanding in dynamic web applications.

</details>


### [11] [Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores](https://arxiv.org/abs/2509.09691)
*Aleksandr Listopad*

Main category: cs.IR

TL;DR: The paper proposes a wave-based semantic memory system that improves semantic similarity by preserving amplitude and phase, showing higher discriminative power than traditional vector-based systems.


<details>
  <summary>Details</summary>
Motivation: Conventional vector-based systems are phase-insensitive and limited in capturing resonance phenomena, which are crucial for meaning representation.

Method: The paper introduces a framework called Wave-Based Semantic Memory that models knowledge as wave patterns and retrieves it through resonance-based interference.

Result: Resonance-based retrieval achieves higher discriminative power, especially in cases involving phase shifts, negations, and compositional queries, with the implementation 'ResonanceDB' showing scalability and low latency.

Conclusion: Wave-based semantic memory provides a more expressive and robust method for semantic similarity by preserving both amplitude and phase information, offering higher discriminative power than conventional vector-based memory systems.

Abstract: Conventional vector-based memory systems rely on cosine or inner product
similarity within real-valued embedding spaces. While computationally
efficient, such approaches are inherently phase-insensitive and limited in
their ability to capture resonance phenomena crucial for meaning
representation. We propose Wave-Based Semantic Memory, a novel framework that
models knowledge as wave patterns $\psi(x) = A(x) e^{i\phi(x)}$ and retrieves
it through resonance-based interference. This approach preserves both amplitude
and phase information, enabling more expressive and robust semantic similarity.
We demonstrate that resonance-based retrieval achieves higher discriminative
power in cases where vector methods fail, including phase shifts, negations,
and compositional queries. Our implementation, ResonanceDB, shows scalability
to millions of patterns with millisecond latency, positioning wave-based memory
as a viable alternative to vector stores for AGI-oriented reasoning and
knowledge representation.

</details>


### [12] [A Research Vision for Web Search on Emerging Topics](https://arxiv.org/abs/2509.10212)
*Alisa Rieger,Stefan Dietze,Ran Yu*

Main category: cs.IR

TL;DR: 论文提出了一种研究愿景，以改进搜索系统，使其能有效处理新兴主题的信息获取问题。


<details>
  <summary>Details</summary>
Motivation: 网络搜索在获取新兴主题信息时的重要性，以及这些主题信息的不确定性、质量和可信度问题。

Method: 通过提出研究问题并分析相关文献，探讨如何建立与研究愿景一致的搜索系统。

Result: 提出了三项研究问题，以深入了解现状、确定与愿景一致的系统要求，并构建这些系统。

Conclusion: 该论文为提升搜索系统处理新兴主题信息的能力提供了研究愿景和解决挑战的方法，旨在促进有效的知识获取和负责任的观点形成。

Abstract: We regularly encounter information on novel, emerging topics for which the
body of knowledge is still evolving, which can be linked, for instance, to
current events. A primary way to learn more about such topics is through web
search. However, information on emerging topics is sparse and evolves
dynamically as knowledge grows, making it uncertain and variable in quality and
trustworthiness and prone to deliberate or accidental manipulation,
misinformation, and bias. In this paper, we outline a research vision towards
search systems and interfaces that support effective knowledge acquisition,
awareness of the dynamic nature of topics, and responsible opinion formation
among people searching the web for information on emerging topics. To realize
this vision, we propose three overarching research questions, aimed at
understanding the status quo, determining requirements of systems aligned with
our vision, and building these systems. For each of the three questions, we
highlight relevant literature, including pointers on how they could be
addressed. Lastly, we discuss the challenges that will potentially arise in
pursuing the proposed vision.

</details>


### [13] [Model-agnostic post-hoc explainability for recommender systems](https://arxiv.org/abs/2509.10245)
*Irina Arévalo,Jose L Salmeron*

Main category: cs.IR

TL;DR: 本文通过比较含有和不含特定用户或项目的模型性能，来量化该观察如何影响推荐系统。


<details>
  <summary>Details</summary>
Motivation: 复杂特征嵌入和深度学习在推荐系统中增强用户体验，但降低了系统的可解释性。

Method: 删除诊断方法通过将含有特定用户或项目的模型与未包含这些元素的类似模型进行比较，来量化其影响。

Result: 实验表明该方法适用于不同的推荐范式，包括神经协同过滤和奇异值分解，在MovieLens和Amazon Reviews数据集上验证了其适用性。

Conclusion: 所提出的方法是模型无关的，能够提升推荐系统的解释性和透明度。

Abstract: Recommender systems often benefit from complex feature embeddings and deep
learning algorithms, which deliver sophisticated recommendations that enhance
user experience, engagement, and revenue. However, these methods frequently
reduce the interpretability and transparency of the system. In this research,
we develop a systematic application, adaptation, and evaluation of deletion
diagnostics in the recommender setting. The method compares the performance of
a model to that of a similar model trained without a specific user or item,
allowing us to quantify how that observation influences the recommender, either
positively or negatively. To demonstrate its model-agnostic nature, the
proposal is applied to both Neural Collaborative Filtering (NCF), a widely used
deep learning-based recommender, and Singular Value Decomposition (SVD), a
classical collaborative filtering technique. Experiments on the MovieLens and
Amazon Reviews datasets provide insights into model behavior and highlight the
generality of the approach across different recommendation paradigms.

</details>


### [14] [Diversified recommendations of cultural activities with personalized determinantal point processes](https://arxiv.org/abs/2509.10392)
*Carole Ibrahim,Hiba Bederina,Daniel Cuesta,Laurent Montier,Cyrille Delabre,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 该研究实现了个性化DPP采样以优化推荐系统的多样性，同时不影响核心业务指标。


<details>
  <summary>Details</summary>
Motivation: 优化推荐系统以增加用户参与是一个成熟的实践，但在不影响核心业务指标的情况下有效地多样化推荐仍然是行业中的一个重要挑战。

Method: 使用个性化的行列式点过程（DPPs）来采样多样化和相关的推荐，基于已知的质量-多样性分解相似性内核赋予用户偏好更多权重。

Result: 实施并评估了个性化DPP采样，在离线和在线指标中研究了相关性和多样性之间的权衡，并发布了代码以便于实验的重现。

Conclusion: 个人化的DPP采样可以用来提供多样化且相关的推荐，方法在生产环境中具有实用性，并提供了可用于重现的代码。

Abstract: While optimizing recommendation systems for user engagement is a
well-established practice, effectively diversifying recommendations without
negatively impacting core business metrics remains a significant industry
challenge. In line with our initiative to broaden our audience's cultural
practices, this study investigates using personalized Determinantal Point
Processes (DPPs) to sample diverse and relevant recommendations. We rely on a
well-known quality-diversity decomposition of the similarity kernel to give
more weight to user preferences. In this paper, we present our implementations
of the personalized DPP sampling, evaluate the trade-offs between relevance and
diversity through both offline and online metrics, and give insights for
practitioners on their use in a production environment. For the sake of
reproducibility, we release the full code for our platform and experiments on
GitHub.

</details>


### [15] [RecoWorld: Building Simulated Environments for Agentic Recommender Systems](https://arxiv.org/abs/2509.10397)
*Fei Liu,Xinyu Lin,Hanchao Yu,Mingyuan Wu,Jianyu Wang,Qiang Zhang,Zhuokai Zhao,Yinglong Xia,Yao Zhang,Weiwei Li,Mingze Gao,Qifan Wang,Lizhu Zhang,Benyu Zhang,Xiangjun Fan*

Main category: cs.IR

TL;DR: RecoWorld是一个为推荐系统开发的模拟环境，通过用户和代理系统的互动优化用户保留和参与。


<details>
  <summary>Details</summary>
Motivation: RecoWorld的提出旨在为个性化推荐系统提供一个模拟环境，使得推荐系统能够在不影响真实用户的情况下进行训练和错误学习。

Method: RecoWorld采用双视角架构：模拟用户和代理推荐系统进行多轮互动，以最大化用户保留率。用户模拟器评估推荐内容，更新心态，并在检测到用户可能流失时生成反思指示，代理推荐系统则根据这些指示和推理记录调整推荐内容。

Result: RecoWorld提供了一个与用户进行多轮互动的训练场，使推荐系统能够通过反复试验不断优化推荐策略。

Conclusion: RecoWorld是推荐系统向个性化信息流协作塑造的一次重要进步，促进了用户和系统之间新互动模式的发展。

Abstract: We present RecoWorld, a blueprint for building simulated environments
tailored to agentic recommender systems. Such environments give agents a proper
training space where they can learn from errors without impacting real users.
RecoWorld distinguishes itself with a dual-view architecture: a simulated user
and an agentic recommender engage in multi-turn interactions aimed at
maximizing user retention. The user simulator reviews recommended items,
updates its mindset, and when sensing potential user disengagement, generates
reflective instructions. The agentic recommender adapts its recommendations by
incorporating these user instructions and reasoning traces, creating a dynamic
feedback loop that actively engages users. This process leverages the
exceptional reasoning capabilities of modern LLMs. We explore diverse content
representations within the simulator, including text-based, multimodal, and
semantic ID modeling, and discuss how multi-turn RL enables the recommender to
refine its strategies through iterative interactions. RecoWorld also supports
multi-agent simulations, allowing creators to simulate the responses of
targeted user populations. It marks an important first step toward recommender
systems where users and agents collaboratively shape personalized information
streams. We envision new interaction paradigms where "user instructs,
recommender responds," jointly optimizing user retention and engagement.

</details>


### [16] [MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables](https://arxiv.org/abs/2509.10448)
*Kausik Hira,Mohd Zaki,Mausam,N. M. Anoop Krishnan*

Main category: cs.IR

TL;DR: 本文介绍了MatSKRAFT，这是一种自动从材料科学领域的表格数据中提取和整合知识的计算框架。该框架利用约束驱动的图神经网络将表格数据转化为图形表示，编码科学原理。与大型语言模型相比，MatSKRAFT在属性提取和成分提取方面表现更佳，处理速度更快。通过该方法，我们构建了一个包含超过53.5万条记录的大型数据库。


<details>
  <summary>Details</summary>
Motivation: 科学研究的进展越来越依赖于从广泛的文献中综合知识。然而，大多数实验数据仍然以半结构化格式存在，难以进行系统的提取和分析。因此，需要一种有效的方法来处理这些数据。

Method: MatSKRAFT框架采用约束驱动的图神经网络，将表格数据转化为图形表示，直接将科学原理编码到模型结构中。然后自动提取和整合材料科学知识。

Result: MatSKRAFT在属性提取和成分提取方面的F1得分分别达到88.68和71.35，比最慢和最快模型处理数据的速度高19至496倍，且硬件需求适中。应用于来自47,000多篇研究论文的近69,000个表格中，构建了一个包含超过535,000条记录的数据库，包括104,000个成分记录，覆盖范围超越现有数据库。

Conclusion: MatSKRAFT提供了一种系统的方法，可以比现有大型语言模型更高效地从材料科学文献中提取知识，形成广泛的材料数据库。这种方法可以揭示以前被忽视的材料，以及这些材料与属性的关系，推进科学发现。

Abstract: Scientific progress increasingly depends on synthesizing knowledge across
vast literature, yet most experimental data remains trapped in semi-structured
formats that resist systematic extraction and analysis. Here, we present
MatSKRAFT, a computational framework that automatically extracts and integrates
materials science knowledge from tabular data at unprecedented scale. Our
approach transforms tables into graph-based representations processed by
constraint-driven GNNs that encode scientific principles directly into model
architecture. MatSKRAFT significantly outperforms state-of-the-art large
language models, achieving F1 scores of 88.68 for property extraction and 71.35
for composition extraction, while processing data $19$-$496\times$ faster than
them (compared to the slowest and the fastest models, respectively) with modest
hardware requirements. Applied to nearly 69,000 tables from more than 47,000
research publications, we construct a comprehensive database containing over
535,000 entries, including 104,000 compositions that expand coverage beyond
major existing databases, pending manual validation. This systematic approach
reveals previously overlooked materials with distinct property combinations and
enables data-driven discovery of composition-property relationships forming the
cornerstone of materials and scientific discovery.

</details>
