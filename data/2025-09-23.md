<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 21]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction](https://arxiv.org/abs/2509.16369)
*Akshay Govind Srinivasan,Ryan Jacob George,Jayden Koshy Joe,Hrushikesh Kant,Harshith M R,Sachin Sundar,Sudharshan Suresh,Rahul Vimalkanth,Vijayavallabh*

Main category: cs.IR

TL;DR: 本文介绍了一种用于金融领域的增强型知识检索生成框架，显著提高了金融问答的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 金融问答需要精准的知识检索，以应对不断更新的数据源和复杂的高风险环境。传统检索系统难以处理复杂的监管文件、市场分析和多年报告等繁琐的数据。

Method: 利用代理型AI和Multi-HyDE系统，生成多个不等价的查询以增加检索的效果和覆盖率，优化了标记效率和多步骤金融推理。

Result: 与标准金融问答基准进行对比测试，表明将域特定的检索机制与强大的工具集合（如关键词和基于表格的检索）结合，准确性提升了11.2%，幻觉现象减少了15%。

Conclusion: 本文提出的RAG框架和Multi-HyDE系统，通过生成多个不等价的查询，优化检索效果，实现了知识检索的准确性和可靠性提升。在金融问答基准测试中显著增强了答案的准确性和可靠性。

Abstract: Accurate and reliable knowledge retrieval is vital for financial
question-answering, where continually updated data sources and complex,
high-stakes contexts demand precision. Traditional retrieval systems rely on a
single database and retriever, but financial applications require more
sophisticated approaches to handle intricate regulatory filings, market
analyses, and extensive multi-year reports. We introduce a framework for
financial Retrieval Augmented Generation (RAG) that leverages agentic AI and
the Multi-HyDE system, an approach that generates multiple, nonequivalent
queries to boost the effectiveness and coverage of retrieval from large,
structured financial corpora. Our pipeline is optimized for token efficiency
and multi-step financial reasoning, and we demonstrate that their combination
improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is
evaluated on standard financial QA benchmarks, showing that integrating
domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets,
including keyword and table-based retrieval, significantly enhances both the
accuracy and reliability of answers. This research not only delivers a modular,
adaptable retrieval framework for finance but also highlights the importance of
structured agent workflows and multi-perspective retrieval for trustworthy
deployment of AI in high-stakes financial applications.

</details>


### [2] [Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe](https://arxiv.org/abs/2509.16411)
*Chong You,Rajesh Jayaram,Ananda Theertha Suresh,Robin Nittka,Felix Yu,Sanjiv Kumar*

Main category: cs.IR

TL;DR: 本文探讨双编码器模型在层次化检索中的表达能力限制，并通过预训练-微调策略提升远距离文档检索性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入空间的欧几里得几何限制了双编码器(DE)模型的表现力，可能影响其质量。

Method: 引入一种预训练-微调策略，显著提高远距离检索的效果，同时不损失近距离文档的性能。

Result: 在WordNet的层次结构上进行实验，发现预训练-微调策略将远距离文档配对的召回率从19%提升到76%。

Conclusion: 本文的方法有效提高了层次检索中远距离文档配对的准确率，同时也适用于购物查询数据集上相关产品的检索。

Abstract: Dual encoder (DE) models, where a pair of matching query and document are
embedded into similar vector representations, are widely used in information
retrieval due to their simplicity and scalability. However, the Euclidean
geometry of the embedding space limits the expressive power of DEs, which may
compromise their quality. This paper investigates such limitations in the
context of hierarchical retrieval (HR), where the document set has a
hierarchical structure and the matching documents for a query are all of its
ancestors. We first prove that DEs are feasible for HR as long as the embedding
dimension is linear in the depth of the hierarchy and logarithmic in the number
of documents. Then we study the problem of learning such embeddings in a
standard retrieval setup where DEs are trained on samples of matching query and
document pairs. Our experiments reveal a lost-in-the-long-distance phenomenon,
where retrieval accuracy degrades for documents further away in the hierarchy.
To address this, we introduce a pretrain-finetune recipe that significantly
improves long-distance retrieval without sacrificing performance on closer
documents. We experiment on a realistic hierarchy from WordNet for retrieving
documents at various levels of abstraction, and show that pretrain-finetune
boosts the recall on long-distance pairs from 19% to 76%. Finally, we
demonstrate that our method improves retrieval of relevant products on a
shopping queries dataset.

</details>


### [3] [Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval](https://arxiv.org/abs/2509.16442)
*Pranjal A. Chitale,Bishal Santra,Yashoteja Prabhu,Amit Sharma*

Main category: cs.IR

TL;DR: 研究探讨了LLM数据增强对紧凑型双编码器模型检索性能提升的效果，发现增强有效但在特定规模后收益减少，小型LLM增强可竞争，未预训练模型获益最大。


<details>
  <summary>Details</summary>
Motivation: 紧凑型双编码器模型因其效率和可扩展性被广泛用于检索，但其性能往往低于基于大型语言模型（LLM）的检索模型，可能是由于其有限的世界知识。LLM数据增强被提出作为桥接性能差距的一种策略，但对其有效性和可扩展性缺乏足够了解。

Method: 进行了一项全面研究，涵盖了检索模型、增强模型和增强策略的100多个不同实验设置，以评估LLM增强对检索的有效性。

Result: 增强确实提升了检索性能，但在某一增强规模之后，其益处开始减少，即使在采用多样化增强策略的情况下也是如此。令人惊讶的是，使用较小的LLM进行增强可以达到与较大增强模型竞争的性能。增强对未经良好预训练的模型提供最大好处。

Conclusion: 研究结果揭示了LLM增强的效果，强调了在特定条件下成本效益的增强策略的重要性，同时代码和增强数据集可公开获取。

Abstract: Compact dual-encoder models are widely used for retrieval owing to their
efficiency and scalability. However, such models often underperform compared to
their Large Language Model (LLM)-based retrieval counterparts, likely due to
their limited world knowledge. While LLM-based data augmentation has been
proposed as a strategy to bridge this performance gap, there is insufficient
understanding of its effectiveness and scalability to real-world retrieval
problems. Existing research does not systematically explore key factors such as
the optimal augmentation scale, the necessity of using large augmentation
models, and whether diverse augmentations improve generalization, particularly
in out-of-distribution (OOD) settings. This work presents a comprehensive study
of the effectiveness of LLM augmentation for retrieval, comprising over 100
distinct experimental settings of retrieval models, augmentation models and
augmentation strategies. We find that, while augmentation enhances retrieval
performance, its benefits diminish beyond a certain augmentation scale, even
with diverse augmentation strategies. Surprisingly, we observe that
augmentation with smaller LLMs can achieve performance competitive with larger
augmentation models. Moreover, we examine how augmentation effectiveness varies
with retrieval model pre-training, revealing that augmentation provides the
most benefit to models which are not well pre-trained. Our insights pave the
way for more judicious and efficient augmentation strategies, thus enabling
informed decisions and maximizing retrieval performance while being more
cost-effective. Code and augmented datasets accompanying this work are publicly
available at https://aka.ms/DAGR.

</details>


### [4] [Purely Semantic Indexing for LLM-based Generative Recommendation and Retrieval](https://arxiv.org/abs/2509.16446)
*Ruohan Zhang,Jiacheng Li,Julian McAuley,Yupeng Hou*

Main category: cs.IR

TL;DR: 提出了纯语义索引方法，避免语义ID的冲突，提升推荐和检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在为语言模型适配生成推荐和检索时，常常出现语义ID冲突的问题，即语义相似的文档被分配了相同的ID。为了避免冲突，通常采用附加非语义标记来区分，这样会引入随机性并扩大搜索空间，从而影响性能。

Method: 提出两种模型无关的算法：穷举候选匹配（ECM）和递归残差搜索（RRS），放宽严格的最近中心选择以生成唯一ID。

Result: 提出了纯语义索引方法，通过放宽严格的最近中心选择生成唯一且保持语义的ID，而无需附加非语义标记。提供了两种与模型无关的算法：穷举候选匹配（ECM）和递归残差搜索（RRS），实验表明这些方法能改善整体和冷启动性能。

Conclusion: 通过确保ID的唯一性有效提升模型在推荐和检索任务中的表现。

Abstract: Semantic identifiers (IDs) have proven effective in adapting large language
models for generative recommendation and retrieval. However, existing methods
often suffer from semantic ID conflicts, where semantically similar documents
(or items) are assigned identical IDs. A common strategy to avoid conflicts is
to append a non-semantic token to distinguish them, which introduces randomness
and expands the search space, therefore hurting performance. In this paper, we
propose purely semantic indexing to generate unique, semantic-preserving IDs
without appending non-semantic tokens. We enable unique ID assignment by
relaxing the strict nearest-centroid selection and introduce two model-agnostic
algorithms: exhaustive candidate matching (ECM) and recursive residual
searching (RRS). Extensive experiments on sequential recommendation, product
search, and document retrieval tasks demonstrate that our methods improve both
overall and cold-start performance, highlighting the effectiveness of ensuring
ID uniqueness.

</details>


### [5] [Long document summarization using page specific target text alignment and distilling page importance](https://arxiv.org/abs/2509.16539)
*Pushpa Devi,Ayush Agrawal,Ashutosh Dubey,C. Ravindranath Chowdary*

Main category: cs.IR

TL;DR: 文章提出了PTS和PTSPI模型来改善长文档的抽象摘要，PTSPI在ROUGE分数上超过现有技术6%以上。


<details>
  <summary>Details</summary>
Motivation: 面对新闻、法律、医学和科学领域日益增长的文本数据，需要有效的长文档摘要方法以提高信息提取效率。

Method: 提出了PTS和PTSPI两种模型，分别采用页面划分和页面重要性权重的方法，以改进长文档的抽象摘要性能。

Result: PTSPI在基准数据集上表现优异，ROUGE-1提升6.32%，ROUGE-2提升8.08%。

Conclusion: PTSPI模型显著提高了长文档摘要的性能，在ROUGE-1和ROUGE-2评分中超越了现有的最先进技术。

Abstract: The rapid growth of textual data across news, legal, medical, and scientific
domains is becoming a challenge for efficiently accessing and understanding
large volumes of content. It is increasingly complex for users to consume and
extract meaningful information efficiently. Thus, raising the need for
summarization. Unlike short document summarization, long document abstractive
summarization is resource-intensive, and very little literature is present in
this direction. BART is a widely used efficient sequence-to-sequence
(seq-to-seq) model. However, when it comes to summarizing long documents, the
length of the context window limits its capabilities. We proposed a model
called PTS (Page-specific Target-text alignment Summarization) that extends the
seq-to-seq method for abstractive summarization by dividing the source document
into several pages. PTS aligns each page with the relevant part of the target
summary for better supervision. Partial summaries are generated for each page
of the document. We proposed another model called PTSPI (Page-specific
Target-text alignment Summarization with Page Importance), an extension to PTS
where an additional layer is placed before merging the partial summaries into
the final summary. This layer provides dynamic page weightage and explicit
supervision to focus on the most informative pages. We performed experiments on
the benchmark dataset and found that PTSPI outperformed the SOTA by 6.32\% in
ROUGE-1 and 8.08\% in ROUGE-2 scores.

</details>


### [6] [The Role of Vocabularies in Learning Sparse Representations for Ranking](https://arxiv.org/abs/2509.16621)
*Hiun Kim,Tae Kwan Lee,Taeryun Won*

Main category: cs.IR

TL;DR: 本文研究了词汇规模和预训练权重在SPLADE模型中的作用，构建了两种BERT模型并经过微调，结果显示ESPLADE模型比随机词汇模型效果更佳，并揭示了词汇配置对检索效率和效果的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究SPLADE模型中的词汇作用及其与检索效率和效果之间的关系。

Method: 构建具有10万个输出词汇的BERT模型，其中一个使用ESPLADE预训练方法初始化，一个随机初始化。经过在真实世界搜索点击日志上的微调后，应用基于logit分数的查询和文档剪枝以达到最大尺寸，从而进一步平衡效率。

Result: 实验结果表明，应用剪枝后，与具有32K词汇的普通SPLADE模型相比，两个模型在BM25计算预算下都表现有效。同时，ESPLADE模型比随机词汇模型更有效，检索成本相近。

Conclusion: 研究结果表明，输出词汇的规模和预训练权重在构建查询、文档及其在检索引擎中的交互表示方面起着重要作用，超越了它们在自然语言处理中的原始意义和用途。这些发现为学习稀疏检索（LSR）的改进提供了新的空间，通过识别词汇配置中的表示规格的重要性以提高检索效率和效果。

Abstract: Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for
effective semantic 1st stage matching while enjoying the efficiency of inverted
indices. A recent work on learning SPLADE models with expanded vocabularies
(ESPLADE) was proposed to represent queries and documents into a sparse space
of custom vocabulary which have different levels of vocabularic granularity.
Within this effort, however, there have not been many studies on the role of
vocabulary in SPLADE models and their relationship to retrieval efficiency and
effectiveness.
  To study this, we construct BERT models with 100K-sized output vocabularies,
one initialized with the ESPLADE pretraining method and one initialized
randomly. After finetune on real-world search click logs, we applied logit
score-based queries and documents pruning to max size for further balancing
efficiency. The experimental result in our evaluation set shows that, when
pruning is applied, the two models are effective compared to the 32K-sized
normal SPLADE model in the computational budget under the BM25. And the ESPLADE
models are more effective than the random vocab model, while having a similar
retrieval cost.
  The result indicates that the size and pretrained weight of output
vocabularies play the role of configuring the representational specification
for queries, documents, and their interactions in the retrieval engine, beyond
their original meaning and purposes in NLP. These findings can provide a new
room for improvement for LSR by identifying the importance of representational
specification from vocabulary configuration for efficient and effective
retrieval.

</details>


### [7] [Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook](https://arxiv.org/abs/2509.16780)
*Eason Chen,Chuangji Li,Shizhuo Li,Conrad Borchers,Zimo Xiao,Chloe Qianhui Zhao,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.IR

TL;DR: 这项研究探讨了使用RAG和GraphRAG进行大学数学课本页面级别的问题回答，发现RAG在检索准确性和生成答案质量方面表现更好。使用LLM重新排序页面时，结果混合，存在性能下降和幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是鉴于LLMs在学习中的信息检索辅助角色，尤其是在如何提高针对特定课程材料的领域知识对齐方面进行探索。

Method: 研究比较了标准的基于嵌入的RAG方法与GraphRAG，评估检索准确性和生成答案质量。

Result: 嵌入式RAG在检索准确性和F1得分方面优于GraphRAG；GraphRAG因其实体结构，存在检索过多和不相关内容的问题。与LLM的页面重新排序产生混合结果，有时可能导致性能下降和幻觉。

Conclusion: 这项研究表明，在教育上下文中进行页面级检索系统存在许多挑战，需要更精细的方法来构建可靠的AI辅导解决方案。

Abstract: Technology-enhanced learning environments often help students retrieve
relevant learning content for questions arising during self-paced study. Large
language models (LLMs) have emerged as novel aids for information retrieval
during learning. While LLMs are effective for general-purpose
question-answering, they typically lack alignment with the domain knowledge of
specific course materials such as textbooks and slides. We investigate
Retrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced
RAG approach, for page-level question answering in an undergraduate mathematics
textbook. While RAG has been effective for retrieving discrete, contextually
relevant passages, GraphRAG may excel in modeling interconnected concepts and
hierarchical knowledge structures. We curate a dataset of 477 question-answer
pairs, each tied to a distinct textbook page. We then compare the standard
embedding-based RAG methods to GraphRAG for evaluating both retrieval
accuracy-whether the correct page is retrieved-and generated answer quality via
F1 scores. Our findings show that embedding-based RAG achieves higher retrieval
accuracy and better F1 scores compared to GraphRAG, which tends to retrieve
excessive and sometimes irrelevant content due to its entity-based structure.
We also explored re-ranking the retrieved pages with LLM and observed mixed
results, including performance drop and hallucinations when dealing with larger
context windows. Overall, this study highlights both the promises and
challenges of page-level retrieval systems in educational contexts, emphasizing
the need for more refined retrieval methods to build reliable AI tutoring
solutions in providing reference page numbers.

</details>


### [8] [Temporal-Aware User Behaviour Simulation with Large Language Models for Recommender Systems](https://arxiv.org/abs/2509.16895)
*Xinye Wanyan,Danula Hettiachchi,Chenglong Ma,Ziqi Xu,Jeffrey Chan*

Main category: cs.IR

TL;DR: DyTA4Rec是一个针对推荐系统的动态时间感知代理模拟器，能够有效建模和利用用户行为的演变，显著提高模拟效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在模拟人类反馈方面的能力有限，因为它们依赖于静态的用户画像，忽视了用户兴趣的时间变动性和动态特征。

Method: DyTA4Rec采用动态更新机制、时间增强提示、以及自适应聚合方法来反映用户的行为演变并提供一致的反馈。

Result: DyTA4Rec显著提高了模拟用户行为与实际用户行为的对齐程度，它通过动态特征模型和增强时间意识来改善LLM基础代理。

Conclusion: DyTA4Rec利用动态更新和时间增强机制改善推荐系统中用户行为模拟的准确性和实时性。

Abstract: Large Language Models (LLMs) demonstrate human-like capabilities in language
understanding, reasoning, and generation, driving interest in using LLM-based
agents to simulate human feedback in recommender systems. However, most
existing approaches rely on static user profiling, neglecting the temporal and
dynamic nature of user interests. This limitation stems from a disconnect
between language modelling and behaviour modelling, which constrains the
capacity of agents to represent sequential patterns. To address this challenge,
we propose a Dynamic Temporal-aware Agent-based simulator for Recommender
Systems, DyTA4Rec, which enables agents to model and utilise evolving user
behaviour based on historical interactions. DyTA4Rec features a dynamic updater
for real-time profile refinement, temporal-enhanced prompting for sequential
context, and self-adaptive aggregation for coherent feedback. Experimental
results at group and individual levels show that DyTA4Rec significantly
improves the alignment between simulated and actual user behaviour by modelling
dynamic characteristics and enhancing temporal awareness in LLM-based agents.

</details>


### [9] [Equip Pre-ranking with Target Attention by Residual Quantization](https://arxiv.org/abs/2509.16931)
*Yutong Li,Yu Zhu,Yichen Qiao,Ziyu Guan,Lv Shao,Tong Liu,Bo Zheng*

Main category: cs.IR

TL;DR: 提出TARQ框架，使用Residual Quantization在预排序阶段应用复杂模型，显著提升推荐系统效果并实现生产部署。


<details>
  <summary>Details</summary>
Motivation: 当前的工业推荐系统在预排序阶段面临效率和效果之间的严重矛盾，复杂的模型如Target Attention由于计算开销巨大无法应用于预排序。

Method: 提出TARQ框架，通过Residual Quantization的方式，使预排序阶段具备类似于Target Attention的模型能力。

Result: TARQ在离线实验和大规模在线A/B测试中表现出显著的排序性能提升，并在淘宝的生产环境中全面部署。

Conclusion: TARQ模型通过在预排序环节加入类似于TA的架构，解决了工业推荐系统中的效率与效果之间的冲突，显著提升了推荐效果并被全面部署。

Abstract: The pre-ranking stage in industrial recommendation systems faces a
fundamental conflict between efficiency and effectiveness. While powerful
models like Target Attention (TA) excel at capturing complex feature
interactions in the ranking stage, their high computational cost makes them
infeasible for pre-ranking, which often relies on simplistic vector-product
models. This disparity creates a significant performance bottleneck for the
entire system. To bridge this gap, we propose TARQ, a novel pre-ranking
framework. Inspired by generative models, TARQ's key innovation is to equip
pre-ranking with an architecture approximate to TA by Residual Quantization.
This allows us to bring the modeling power of TA into the latency-critical
pre-ranking stage for the first time, establishing a new state-of-the-art
trade-off between accuracy and efficiency. Extensive offline experiments and
large-scale online A/B tests at Taobao demonstrate TARQ's significant
improvements in ranking performance. Consequently, our model has been fully
deployed in production, serving tens of millions of daily active users and
yielding substantial business improvements.

</details>


### [10] [Identifying and Upweighting Power-Niche Users to Mitigate Popularity Bias in Recommendations](https://arxiv.org/abs/2509.17265)
*David Liu,Erik Weis,Moritz Laber,Tina Eliassi-Rad,Brennan Klein*

Main category: cs.IR

TL;DR: 本研究探讨了推荐系统中的流行度偏差问题，发现“高活动、偏好小众商品”的用户数量大于预期，提出了一种基于用户活跃度和商品流行度的重加权框架以减少流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解推荐系统中与小众商品的交互，以减轻流行度偏差问题，尤其是关注偏好小众商品且活动水平高的用户。

Method: 提出了一种新的框架，通过调整贝叶斯个性化排名（BPR）损失函数的权重来考虑用户活动水平和商品流行度的交互，该框架具有两个参数用于解释用户活动水平的显著性和商品流行度的显著性。

Result: 实验表明，提高对“高活动、小众偏好”用户的权重可减少推荐系统的流行度偏差，并能提高整体性能，与以往单独考虑用户活动或商品流行度的方法相比，表现更为优越。

Conclusion: 提出了一种同时基于用户活动水平和商品流行度重加权的框架，以降低推荐系统的流行度偏差，提高整体性能。

Abstract: Recommender systems have been shown to exhibit popularity bias by
over-recommending popular items and under-recommending relevant niche items. We
seek to understand interactions with niche items in benchmark recommendation
datasets as a step toward mitigating popularity bias. We find that, compared to
mainstream users, niche-preferring users exhibit a longer-tailed activity-level
distribution, indicating the existence of users who both prefer niche items and
exhibit high activity levels. We partition users along two axes: (1) activity
level ("power" vs. "light") and (2) item-popularity preference ("mainstream"
vs. "niche"), and show that in several benchmark datasets, the number of
power-niche users (high activity and niche preference) is statistically
significantly larger than expected under a null configuration model. Motivated
by this observation, we propose a framework for reweighting the Bayesian
Personalized Ranking (BPR) loss that simultaneously reweights based on user
activity level and item popularity. Our method introduces two interpretable
parameters: one controlling the significance of user activity level, and the
other of item popularity. Experiments on benchmark datasets show that
upweighting power-niche users reduces popularity bias and can increase overall
performance. In contrast to previous work that only considers user activity
level or item popularity in isolation, our results suggest that considering
their interaction leads to Pareto-dominant performance.

</details>


### [11] [MLLM-Driven Semantic Identifier Generation for Generative Cross-Modal Retrieval](https://arxiv.org/abs/2509.17359)
*Tianyuan Li,Lei Wang,Ahtamjan Ahmat,Yating Yang,Bo Ma,Rui Dong,Bangju Han*

Main category: cs.IR

TL;DR: 跨模态检索中，通过生成结构化语义标识符提高语义对齐和可扩展性，并引入理由指导以改善训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态检索方法存在语义对齐和可扩展性的挑战。

Method: 提出一种词汇高效的标识生成框架和理由指导监督策略，用于从图像和标题对生成结构化语义标识符。

Result: 改进了模型的语义基础和减少了训练期间的幻觉。

Conclusion: 通过生成结构化语义标识符和提供一条语句解释，提高了跨模态检索的效率和效果。

Abstract: Generative cross-modal retrieval, which treats retrieval as a generation
task, has emerged as a promising direction with the rise of Multimodal Large
Language Models (MLLMs). In this setting, the model responds to a text query by
generating an identifier corresponding to the target image. However, existing
methods typically rely on manually crafted string IDs, clustering-based labels,
or atomic identifiers requiring vocabulary expansion, all of which face
challenges in semantic alignment or scalability.To address these limitations,
we propose a vocabulary-efficient identifier generation framework that prompts
MLLMs to generate Structured Semantic Identifiers from image-caption pairs.
These identifiers are composed of concept-level tokens such as objects and
actions, naturally aligning with the model's generation space without modifying
the tokenizer. Additionally, we introduce a Rationale-Guided Supervision
Strategy, prompting the model to produce a one-sentence explanation alongside
each identifier serves as an auxiliary supervision signal that improves
semantic grounding and reduces hallucinations during training.

</details>


### [12] [SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing](https://arxiv.org/abs/2509.17361)
*Ruihan Luo,Xuanjing Chen,Ziyang Ding*

Main category: cs.IR

TL;DR: SeqUDA-Rec框架通过整合用户行为序列和无监督的数据增强，显著提高了推荐精度和鲁棒性。实验结果表明其在个性化广告和智慧内容推荐中具有很大优势。


<details>
  <summary>Details</summary>
Motivation: 个性化内容营销已成为数字平台的关键策略，但传统推荐系统面临两大局限：对不充分监督信号的依赖和对噪声或无意行为的脆弱性。为解决这些问题，研究提出了一种新的深度学习框架SeqUDA-Rec。

Method: 构建全球用户-项目交互图（GUIG），使用图对比学习模块生成鲁棒嵌入，采用基于Transformer的编码器模拟用户的动态偏好，并通过GAN生成可行的交互模式来增强多样性。

Result: SeqUDA-Rec在两个真实的营销数据集（Amazon Ads和TikTok Ad Clicks）上进行了实验，表现超越了最先进的基准方法如SASRec、BERT4Rec和GCL4SR。模型在NDCG@10上提高了6.7%，在HR@10上提高了11.3%。

Conclusion: SeqUDA-Rec模型有效提高个性化广告推荐系统的精度和鲁棒性，为智慧内容推荐提供了新的方法。

Abstract: Personalized content marketing has become a crucial strategy for digital
platforms, aiming to deliver tailored advertisements and recommendations that
match user preferences. Traditional recommendation systems often suffer from
two limitations: (1) reliance on limited supervised signals derived from
explicit user feedback, and (2) vulnerability to noisy or unintentional
interactions. To address these challenges, we propose SeqUDA-Rec, a novel deep
learning framework that integrates user behavior sequences with global
unsupervised data augmentation to enhance recommendation accuracy and
robustness. Our approach first constructs a Global User-Item Interaction Graph
(GUIG) from all user behavior sequences, capturing both local and global item
associations. Then, a graph contrastive learning module is applied to generate
robust embeddings, while a sequential Transformer-based encoder models users'
evolving preferences. To further enhance diversity and counteract sparse
supervised labels, we employ a GAN-based augmentation strategy, generating
plausible interaction patterns and supplementing training data. Extensive
experiments on two real-world marketing datasets (Amazon Ads and TikTok Ad
Clicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art
baselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%
improvement in NDCG@10 and 11.3% improvement in HR@10, proving its
effectiveness in personalized advertising and intelligent content
recommendation.

</details>


### [13] [Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion and Document Boosting](https://arxiv.org/abs/2509.17440)
*Jüri Keller,Maik Fröbe,Gijs Hendriksen,Daria Alexander,Martin Potthast,Philipp Schaer*

Main category: cs.IR

TL;DR: 提出ir_datasets的扩展来支持纵向评估，简化代码结构，提升实验可重复性。


<details>
  <summary>Details</summary>
Motivation: 传统的Cranfield风格的检索评估只包含静态的查询和文档集，无法体现时间作为评估维度的影响。因此，需要进行纵向评估来捕获信息需求和文档随时间的演变。

Method: 提出了一种ir_datasets的自定义扩展，用于纵向检索实验。这个扩展允许通过声明性的方法，而不是命令式的方法，描述纵向检索实验的重要方面。

Result: 通过将2024年LongEval的提交重新实现为使用新的ir_datasets扩展，发现声明式访问可以减少代码的复杂性。

Conclusion: 增加的自定义逻辑复杂了研究软件，可能降低实验的可重复性和可扩展性。通过新的扩展，能够更好地支持纵向检索实验，简化代码结构。

Abstract: The longitudinal evaluation of retrieval systems aims to capture how
information needs and documents evolve over time. However, classical
Cranfield-style retrieval evaluations only consist of a static set of queries
and documents and thereby miss time as an evaluation dimension. Therefore,
longitudinal evaluations need to complement retrieval toolkits with custom
logic. This custom logic increases the complexity of research software, which
might reduce the reproducibility and extensibility of experiments. Based on our
submissions to the 2024 edition of LongEval, we propose a custom extension of
ir_datasets for longitudinal retrieval experiments. This extension allows for
declaratively, instead of imperatively, describing important aspects of
longitudinal retrieval experiments, e.g., which queries, documents, and/or
relevance feedback are available at which point in time. We reimplement our
submissions to LongEval 2024 against our new ir_datasets extension, and find
that the declarative access can reduce the complexity of the code.

</details>


### [14] [WildClaims: Information Access Conversations in the Wild(Chat)](https://arxiv.org/abs/2509.17442)
*Hideaki Joko,Shakiba Amirshahi,Charles L. A. Clarke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: 该研究表明在与ChatGPT的对话中，用户往往通过隐式方式获取需核查的信息，远超传统明确的信息访问方式


<details>
  <summary>Details</summary>
Motivation: 探讨用户在现实世界中如何通过对话获取信息，尤其是隐式的信息获取方式

Method: 进行观察性研究并创建和分析WildClaims数据集

Result: 大规模用户聊天数据分析表明，18%至51%的对话中存在需核查的断言，高达76%的对话可能含有此类断言，强调隐式信息获取的重要性

Conclusion: 研究展示了现实世界用户系统对话中，隐式信息获取的重要性，并为相关研究提供了新数据集

Abstract: The rapid advancement of Large Language Models (LLMs) has transformed
conversational systems into practical tools used by millions. However, the
nature and necessity of information retrieval in real-world conversations
remain largely unexplored, as research has focused predominantly on
traditional, explicit information access conversations. The central question
is: What do real-world information access conversations look like? To this end,
we first conduct an observational study on the WildChat dataset, large-scale
user-ChatGPT conversations, finding that users' access to information occurs
implicitly as check-worthy factual assertions made by the system, even when the
conversation's primary intent is non-informational, such as creative writing.
To enable the systematic study of this phenomenon, we release the WildClaims
dataset, a novel resource consisting of 121,905 extracted factual claims from
7,587 utterances in 3,000 WildChat conversations, each annotated for
check-worthiness. Our preliminary analysis of this resource reveals that
conservatively 18% to 51% of conversations contain check-worthy assertions,
depending on the methods employed, and less conservatively, as many as 76% may
contain such assertions. This high prevalence underscores the importance of
moving beyond the traditional understanding of explicit information access, to
address the implicit information access that arises in real-world user-system
conversations.

</details>


### [15] [LongEval at CLEF 2025: Longitudinal Evaluation of IR Systems on Web and Scientific Data](https://arxiv.org/abs/2509.17469)
*Matteo Cancellieri,Alaa El-Ebshihy,Tobias Fink,Maik Fröbe,Petra Galuščáková,Gabriela Gonzalez-Saez,Lorraine Goeuriot,David Iommi,Jüri Keller,Petr Knoth,Philippe Mulhem,Florina Piroi,David Pride,Philipp Schaer*

Main category: cs.IR

TL;DR: 分析信息检索系统在动态环境中随时间变化的检索效果。


<details>
  <summary>Details</summary>
Motivation: 评估信息检索系统随时间变化的效果，提供捕捉动态搜索场景的数据集。

Method: 提供两个数据集，用于捕捉不断变化的搜索情景，并通过nDCG和其他评估方法量化检索效果变化。

Result: 参与评估的19个团队的系统在动态环境中的检索效果得到分析。

Conclusion: 展示了不同系统在动态场景中的检索性能，并提出了不同量化检索效果的方法。

Abstract: The LongEval lab focuses on the evaluation of information retrieval systems
over time. Two datasets are provided that capture evolving search scenarios
with changing documents, queries, and relevance assessments. Systems are
assessed from a temporal perspective-that is, evaluating retrieval
effectiveness as the data they operate on changes. In its third edition,
LongEval featured two retrieval tasks: one in the area of ad-hoc web retrieval,
and another focusing on scientific article retrieval. We present an overview of
this year's tasks and datasets, as well as the participating systems. A total
of 19 teams submitted their approaches, which we evaluated using nDCG and a
variety of measures that quantify changes in retrieval effectiveness over time.

</details>


### [16] [Human vs. Agent in Task-Oriented Conversations](https://arxiv.org/abs/2509.17619)
*Zhefan Wang,Ning Geng,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.IR

TL;DR: 研究首次系统比较了LLM模拟用户与人类用户在个性化任务导向对话中的行为差异，揭示了显著的行为差异，并提供关键视角来推进LLM用户模拟。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决任务导向的对话系统中数据获取困难的问题，通过比较大型语言模型生成的数据与真实用户数据，来评估其可替代性。

Method: 提出了一个全面的分析框架，包括三个关键方面（对话策略、互动风格和对话评估）和十个不同维度，用于评估用户行为。通过四种代表性场景下的平行对话数据集比较人类用户与LLM代理用户的行为差异。

Result: 分析显示人类用户与LLM代理用户在解决问题的方法、问题的宽度、用户参与度、上下文依赖性、反馈极性和承诺、语言风格以及幻觉意识方面有显著差异。

Conclusion: 构建了一个可扩展的框架，提供了对用户行为模式分析的洞察，有助于反思在未来对话系统中如何使用用户模拟。

Abstract: Task-oriented conversational systems are essential for efficiently addressing
diverse user needs, yet their development requires substantial amounts of
high-quality conversational data that is challenging and costly to obtain.
While large language models (LLMs) have demonstrated potential in generating
synthetic conversations, the extent to which these agent-generated interactions
can effectively substitute real human conversations remains unclear. This work
presents the first systematic comparison between LLM-simulated users and human
users in personalized task-oriented conversations. We propose a comprehensive
analytical framework encompassing three key aspects (conversation strategy,
interaction style, and conversation evaluation) and ten distinct dimensions for
evaluating user behaviors, and collect parallel conversational datasets from
both human users and LLM agent users across four representative scenarios under
identical conditions. Our analysis reveals significant behavioral differences
between the two user types in problem-solving approaches, question broadness,
user engagement, context dependency, feedback polarity and promise, language
style, and hallucination awareness. We found consistency in the agent users and
human users across the depth-first or breadth-first dimensions, as well as the
usefulness dimensions. These findings provide critical insights for advancing
LLM-based user simulation. Our multi-dimensional taxonomy constructed a
generalizable framework for analyzing user behavior patterns, offering insights
from LLM agent users and human users. By this work, we provide perspectives on
rethinking how to use user simulation in conversational systems in the future.

</details>


### [17] [A Generative Framework for Personalized Sticker Retrieval](https://arxiv.org/abs/2509.17749)
*Changjiang Zhou,Ruqing Zhang,Jiafeng Guo,Yu-An Liu,Fan Zhang,Ganyuan Luo,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出了一个个性化贴纸检索的新框架PEARL，通过编码用户特定的贴纸偏好和用户的查询意图，生成符合用户需求的贴纸。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式检索方法通常缺乏个性化，导致用户期望和检索结果不匹配的问题。

Method: 设计一个表示学习模型来编码用户特定的贴纸偏好，并提出一个新的意图感知学习目标，以优先生成与用户查询意图更匹配的贴纸。

Result: PEARL显著优于当前最先进的方法。

Conclusion: PEARL在个性化贴纸检索方面展示了优异效果，提供了一种解决个性化挑战的新方法。

Abstract: Formulating information retrieval as a variant of generative modeling,
specifically using autoregressive models to generate relevant identifiers for a
given query, has recently attracted considerable attention. However, its
application to personalized sticker retrieval remains largely unexplored and
presents unique challenges: existing relevance-based generative retrieval
methods typically lack personalization, leading to a mismatch between diverse
user expectations and the retrieved results. To address this gap, we propose
PEARL, a novel generative framework for personalized sticker retrieval, and
make two key contributions: (i) To encode user-specific sticker preferences, we
design a representation learning model to learn discriminative user
representations. It is trained on three prediction tasks that leverage personal
information and click history; and (ii) To generate stickers aligned with a
user's query intent, we propose a novel intent-aware learning objective that
prioritizes stickers associated with higher-ranked intents. Empirical results
from both offline evaluations and online tests demonstrate that PEARL
significantly outperforms state-of-the-art methods.

</details>


### [18] [Shilling Recommender Systems by Generating Side-feature-aware Fake User Profiles](https://arxiv.org/abs/2509.17918)
*Yuanrong Wang,Yingpeng Du*

Main category: cs.IR

TL;DR: 本文提出了一种改进的生成器架构，用于创建带有侧面特征的假用户档案，并提升攻击性能。



<details>
  <summary>Details</summary>
Motivation: 现有的恶意行为缺乏应对含侧面特征场景的全面解决方案。


Method: 扩展Leg-UP框架，通过改进生成器架构，以包含侧面特征。


Result: 在标杆测试中，该方法在保持隐蔽性的同时表现出了强大的攻击性能。

Conclusion: 提高推荐系统在面对含有侧面特征的恶意攻击时的稳健性。


Abstract: Recommender systems (RS) greatly influence users' consumption decisions,
making them attractive targets for malicious shilling attacks that inject fake
user profiles to manipulate recommendations. Existing shilling methods can
generate effective and stealthy fake profiles when training data only contain
rating matrix, but they lack comprehensive solutions for scenarios where side
features are present and utilized by the recommender. To address this gap, we
extend the Leg-UP framework by enhancing the generator architecture to
incorporate side features, enabling the generation of side-feature-aware fake
user profiles. Experiments on benchmarks show that our method achieves strong
attack performance while maintaining stealthiness.

</details>


### [19] [A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem](https://arxiv.org/abs/2509.18054)
*Nikhil N S,Amol Dilip Joshi,Bilal Muhammed,Soban Babu*

Main category: cs.IR

TL;DR: 通过知识图谱和大型语言模型进行设施布局问题算法推荐的新方法，验证其在多测试案例中优于市场产品。


<details>
  <summary>Details</summary>
Motivation: 解决设施布局问题（FLP）的算法选择复杂，涉及多目标的权衡，需要深厚的专家知识。由于算法的性能依赖于问题的具体特征，如规模、目标和约束，因此需要一种数据驱动的方法来指导自动化设计系统中的算法选择。

Method: 构建领域特定知识图谱，并采用图搜索、向量搜索和聚类搜索三种方法从中检索证据，结合大型语言模型生成推荐。与商业LLM聊天机器人相比，在推荐准确性和推理能力方面表现显著更好。

Result: 提出了一种基于知识图谱增强生成（KG RAG）框架的新推荐方法，用于方便获取专家知识。通过构建领域特定知识图谱，并使用多重检索机制从中收集相关证据，以推荐适合的算法。这种新方法在多样的现实FLP测试案例中表现优于商业LLM聊天机器人。

Conclusion: 基于知识图谱的检索增强生成方法在设施布局问题的算法选择中取得了显著优势，为设计系统中的自动化算法选择提供了一种数据驱动的解决方案。

Abstract: Selecting a solution algorithm for the Facility Layout Problem (FLP), an
NP-hard optimization problem with a multiobjective trade-off, is a complex task
that requires deep expert knowledge. The performance of a given algorithm
depends on specific problem characteristics such as its scale, objectives, and
constraints. This creates a need for a data-driven recommendation method to
guide algorithm selection in automated design systems. This paper introduces a
new recommendation method to make such expertise accessible, based on a
Knowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To
address this, a domain-specific knowledge graph is constructed from published
literature. The method then employs a multi-faceted retrieval mechanism to
gather relevant evidence from this knowledge graph using three distinct
approaches, which include a precise graph-based search, flexible vector-based
search, and high-level cluster-based search. The retrieved evidence is utilized
by a Large Language Model (LLM) to generate algorithm recommendations with
data-driven reasoning. The proposed KG-RAG method is compared against a
commercial LLM chatbot with access to the knowledge base as a table, across a
series of diverse, real-world FLP test cases. Based on recommendation accuracy
and reasoning capability, the proposed method performed significantly better
than the commercial LLM chatbot.

</details>


### [20] [OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System](https://arxiv.org/abs/2509.18091)
*Sunhao Dai,Jiakai Tang,Jiahua Wu,Kun Wang,Yuxuan Zhu,Bingjun Chen,Bangyang Hong,Yu Zhao,Cong Fu,Kangle Wu,Yabo Ni,Anxiang Zeng,Wenjie Wang,Xu Chen,Jun Xu,See-Kiong Ng*

Main category: cs.IR

TL;DR: 现有工业系统在复制LLMs成功时仅限于移植Transformer架构，改进有限。本文提出OnePiece框架，通过上下文工程和推理机制，提升工业排名系统。这一框架在Shopee上取得了显著商业成果。


<details>
  <summary>Details</summary>
Motivation: LLMs在工业搜索和推荐系统中的成功复制受到广泛关注，但大多数现有工业努力仅限于移植Transformer架构，只能对强大的DLRM模型带来增量改进。本文旨在突破LLMs的架构，通过上下文工程和多步推理机制解锁工业排名系统的显著进步潜力。

Method: 提出了一个名为OnePiece的统一框架，该框架将LLM风格的上下文工程和推理无缝集成到工业级流水线的检索和排名模型中。OnePiece基于纯Transformer架构，并引入三项关键创新：结构化上下文工程、块式潜在推理、渐进式多任务训练。

Result: OnePiece已经在Shopee的主要个性化搜索场景中部署，并在不同关键业务指标上实现了持续的在线增长，包括GMV/UU提高超过2%，广告收入增长2.90%。

Conclusion: OnePiece框架有效整合了上下文工程和推理机制，显著提高了工业系统的检索和排名能力，证明了其在商业应用中的潜力和价值。

Abstract: Despite the growing interest in replicating the scaled success of large
language models (LLMs) in industrial search and recommender systems, most
existing industrial efforts remain limited to transplanting Transformer
architectures, which bring only incremental improvements over strong Deep
Learning Recommendation Models (DLRMs). From a first principle perspective, the
breakthroughs of LLMs stem not only from their architectures but also from two
complementary mechanisms: context engineering, which enriches raw input queries
with contextual cues to better elicit model capabilities, and multi-step
reasoning, which iteratively refines model outputs through intermediate
reasoning paths. However, these two mechanisms and their potential to unlock
substantial improvements remain largely underexplored in industrial ranking
systems.
  In this paper, we propose OnePiece, a unified framework that seamlessly
integrates LLM-style context engineering and reasoning into both retrieval and
ranking models of industrial cascaded pipelines. OnePiece is built on a pure
Transformer backbone and further introduces three key innovations: (1)
structured context engineering, which augments interaction history with
preference and scenario signals and unifies them into a structured tokenized
input sequence for both retrieval and ranking; (2) block-wise latent reasoning,
which equips the model with multi-step refinement of representations and scales
reasoning bandwidth via block size; (3) progressive multi-task training, which
leverages user feedback chains to effectively supervise reasoning steps during
training. OnePiece has been deployed in the main personalized search scenario
of Shopee and achieves consistent online gains across different key business
metrics, including over $+2\%$ GMV/UU and a $+2.90\%$ increase in advertising
revenue.

</details>


### [21] [MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction](https://arxiv.org/abs/2509.18095)
*Zilin Xiao,Qi Ma,Mengting Gu,Chun-cheng Jason Chen,Xintao Chen,Vicente Ordonez,Vijai Mohan*

Main category: cs.IR

TL;DR: 引入了一种新的叫做MetaEmbed的框架来改进多模态检索，通过固定数量的可学习Meta Tokens来生成紧凑且表达丰富的多向量嵌入，从而在保证检索质量的同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法存在只用单个向量限制表达能力或使用太多向量导致检索开销过大的问题，本文提出MetaEmbed以解决这些问题。

Method: 在训练中，加入固定数量的可学习Meta Tokens到输入序列中，然后使用其最后一层的上下文表示作为多向量嵌入。通过Matryoshka Multi-Vector Retrieval训练，MetaEmbed学会跨多个向量按粒度组织信息。

Result: MetaEmbed在多个基准测试中达到了最先进的检索性能，同时能有效扩展到具有32B参数的模型。

Conclusion: MetaEmbed框架通过改进多模态嵌入的构建和交互方式，在提高检索性能的同时解决了效率问题，实现了检索质量和检索效率的平衡。

Abstract: Universal multimodal embedding models have achieved great success in
capturing semantic relevance between queries and candidates. However, current
methods either condense queries and candidates into a single vector,
potentially limiting the expressiveness for fine-grained information, or
produce too many vectors that are prohibitively expensive for multi-vector
retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal
retrieval that rethinks how multimodal embeddings are constructed and
interacted with at scale. During training, a fixed number of learnable Meta
Tokens are appended to the input sequence. At test-time, their last-layer
contextualized representations serve as compact yet expressive multi-vector
embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,
MetaEmbed learns to organize information by granularity across multiple
vectors. As a result, we enable test-time scaling in multimodal retrieval,
where users can balance retrieval quality against efficiency demands by
selecting the number of tokens used for indexing and retrieval interactions.
Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and
the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed
achieves state-of-the-art retrieval performance while scaling robustly to
models with 32B parameters.

</details>
