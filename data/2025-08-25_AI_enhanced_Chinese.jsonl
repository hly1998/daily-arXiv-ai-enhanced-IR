{"id": "2508.16046", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16046", "abs": "https://arxiv.org/abs/2508.16046", "authors": ["Shadikur Rahman", "Umme Ayman Koana", "Aras M. Ismael", "Karmand Hussein Abdalla"], "title": "Estimating the Effective Topics of Articles and journals Abstract Using LDA And K-Means Clustering Algorithm", "comment": null, "summary": "Analyzing journals and articles abstract text or documents using topic\nmodelling and text clustering has become a modern solution for the increasing\nnumber of text documents. Topic modelling and text clustering are both\nintensely involved tasks that can benefit one another. Text clustering and\ntopic modelling algorithms are used to maintain massive amounts of text\ndocuments. In this study, we have used LDA, K-Means cluster and also lexical\ndatabase WordNet for keyphrases extraction in our text documents. K-Means\ncluster and LDA algorithms achieve the most reliable performance for keyphrase\nextraction in our text documents. This study will help the researcher to make a\nsearch string based on journals and articles by avoiding misunderstandings.", "AI": {"tldr": "\u672c\u6587\u5229\u7528LDA\u3001K-Means\u548cWordNet\u8fdb\u884c\u6587\u672c\u5206\u6790\uff0c\u6210\u529f\u63d0\u53d6\u5173\u952e\u77ed\u8bed\uff0c\u63d0\u9ad8\u6587\u732e\u68c0\u7d22\u7cbe\u51c6\u5ea6\u3002", "motivation": "\u968f\u7740\u6587\u672c\u6587\u4ef6\u6570\u91cf\u7684\u589e\u52a0\uff0c\u7814\u7a76\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7ef4\u62a4\u548c\u5904\u7406\u5927\u91cf\u7684\u6587\u672c\u6587\u4ef6\uff0c\u4ece\u800c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u9ad8\u6548\u68c0\u7d22\u548c\u7406\u89e3\u6587\u732e\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86LDA\uff08\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d\uff09\u6a21\u578b\u548cK-Means\u805a\u7c7b\u7b97\u6cd5\uff0c\u4ee5\u53caWordNet\u8bcd\u6c47\u6570\u636e\u5e93\uff0c\u8fdb\u884c\u6587\u672c\u7684\u4e3b\u9898\u5efa\u6a21\u548c\u5173\u952e\u77ed\u8bed\u63d0\u53d6\u3002", "result": "\u901a\u8fc7LDA\u548cK-Means\u805a\u7c7b\u7b97\u6cd5\uff0c\u4ee5\u53caWordNet\u8bcd\u5e93\uff0c\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5173\u952e\u77ed\u8bed\u63d0\u53d6\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u4ece\u671f\u520a\u548c\u6587\u7ae0\u4e2d\u6784\u5efa\u641c\u7d22\u5b57\u7b26\u4e32\uff0c\u51cf\u5c11\u8bef\u89e3\u3002", "conclusion": "\u5bf9\u6587\u672c\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\u548c\u805a\u7c7b\u662f\u5904\u7406\u5927\u91cf\u6587\u672c\u7684\u73b0\u4ee3\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u91c7\u7528LDA\u3001K-Means\u805a\u7c7b\u4ee5\u53caWordNet\u8bcd\u6c47\u6570\u636e\u5e93\u6765\u8fdb\u884c\u5173\u952e\u77ed\u8bed\u63d0\u53d6\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u5904\u7406\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.16106", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16106", "abs": "https://arxiv.org/abs/2508.16106", "authors": ["Yongzhi Jin", "Kazushi Okamoto", "Kei Harada", "Atsushi Shibata", "Koki Karube"], "title": "Similarity-Based Supervised User Session Segmentation Method for Behavior Logs", "comment": "Submitted to Journal of Advanced Computational Intelligence and\n  Intelligent Informatics", "summary": "In information recommendation, a session refers to a sequence of user actions\nwithin a specific time frame. Session-based recommender systems aim to capture\nshort-term preferences and generate relevant recommendations. However, user\ninterests may shift even within a session, making appropriate segmentation\nessential for modeling dynamic behaviors. In this study, we propose a\nsupervised session segmentation method based on similarity features derived\nfrom action embeddings and attributes. We compute the similarity scores between\nitems within a fixed-size window around each candidate segmentation point,\nusing four types of features: item co-occurrence embeddings, text embeddings of\ntitles and brands, and price. These features are used to train supervised\nclassifiers (LightGBM, XGBoost, CatBoost, support vector machine, and logistic\nregression) to predict the session boundaries. We construct a manually\nannotated dataset from real user browsing histories and evaluate the\nsegmentation performance using F1-score, area under the precision-recall curve\n(PR-AUC), and area under the receiver operating characteristic curve. The\nLightGBM model achieves the best performance, with an F1-score of 0.806 and a\nPR-AUC of 0.831. These results demonstrate the effectiveness of the proposed\nmethod for session segmentation and its potential to capture dynamic user\nbehaviors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u4f1a\u8bdd\u5206\u5272\u65b9\u6cd5\uff0c\u4f7f\u7528\u76f8\u4f3c\u6027\u7279\u5f81\u6709\u6548\u6355\u6349\u52a8\u6001\u7528\u6237\u884c\u4e3a\uff0cLightGBM\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7528\u6237\u5174\u8da3\u53ef\u80fd\u5728\u4f1a\u8bdd\u4e2d\u8fc5\u901f\u53d8\u5316\uff0c\u56e0\u6b64\u9002\u5f53\u7684\u4f1a\u8bdd\u5206\u5272\u5bf9\u4e8e\u5efa\u6a21\u52a8\u6001\u884c\u4e3a\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u7279\u5f81\u7684\u76d1\u7763\u4f1a\u8bdd\u5206\u5272\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u52a8\u4f5c\u5d4c\u5165\u548c\u5c5e\u6027\u751f\u6210\u76f8\u4f3c\u6027\u7279\u5f81\u3002\u4e3b\u8981\u4f7f\u7528\u56db\u79cd\u7c7b\u578b\u7684\u7279\u5f81\uff1a\u9879\u5171\u73b0\u5d4c\u5165\u3001\u6587\u672c\u5d4c\u5165\u3001\u6807\u9898\u548c\u54c1\u724c\u4ee5\u53ca\u4ef7\u683c\u3002\u901a\u8fc7\u8fd9\u4e9b\u7279\u5f81\u8bad\u7ec3\u76d1\u7763\u5206\u7c7b\u5668\uff08LightGBM\u3001XGBoost\u3001CatBoost\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u903b\u8f91\u56de\u5f52\uff09\u6765\u9884\u6d4b\u4f1a\u8bdd\u8fb9\u754c\u3002", "result": "LightGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u4e3a0.806\uff0cPR-AUC\u4e3a0.831\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f1a\u8bdd\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u52a8\u6001\u7528\u6237\u884c\u4e3a\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.16126", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16126", "abs": "https://arxiv.org/abs/2508.16126", "authors": ["Haitao Lin", "Zhen Yang", "Jiawei Xue", "Ziji Zhang", "Luzhu Wang", "Yikun Gu", "Yao Xu", "Xin Li"], "title": "Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation", "comment": null, "summary": "Building upon the strong sequence modeling capability, Generative\nRecommendation (GR) has gradually assumed a dominant position in the\napplication of recommendation tasks (e.g., video and product recommendation).\nHowever, the application of Generative Recommendation in Point-of-Interest\n(POI) recommendation, where user preferences are significantly affected by\nspatiotemporal variations, remains a challenging open problem. In this paper,\nwe propose Spacetime-GR, the first spacetime-aware generative model for\nlarge-scale online POI recommendation. It extends the strong sequence modeling\nability of generative models by incorporating flexible spatiotemporal\ninformation encoding. Specifically, we first introduce a geographic-aware\nhierarchical POI indexing strategy to address the challenge of large vocabulary\nmodeling. Subsequently, a novel spatiotemporal encoding module is introduced to\nseamlessly incorporate spatiotemporal context into user action sequences,\nthereby enhancing the model's sensitivity to spatiotemporal variations.\nFurthermore, we incorporate multimodal POI embeddings to enrich the semantic\nunderstanding of each POI. Finally, to facilitate practical deployment, we\ndevelop a set of post-training adaptation strategies after sufficient\npre-training on action sequences. These strategies enable Spacetime-GR to\ngenerate outputs in multiple formats (i.e., embeddings, ranking scores and POI\ncandidates) and support a wide range of downstream application scenarios (i.e.,\nranking and end-to-end recommendation). We evaluate the proposed model on both\npublic benchmark datasets and large-scale industrial datasets, demonstrating\nits superior performance over existing methods in terms of POI recommendation\naccuracy and ranking quality. Furthermore, the model is the first generative\nmodel deployed in online POI recommendation services that scale to hundreds of\nmillions of POIs and users.", "AI": {"tldr": "Spacetime-GR\u6a21\u578b\u6269\u5c55\u751f\u6210\u6a21\u578b\u4ee5\u65f6\u7a7a\u611f\u77e5\u6765\u5904\u7406POI\u63a8\u8350\u7684\u6311\u6218\uff0c\u5728\u51c6\u786e\u5ea6\u548c\u6392\u540d\u8d28\u91cf\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u90e8\u7f72\u4e8e\u5728\u7ebf\u670d\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u63a8\u8350\u65b9\u6cd5\u5728\u89c6\u9891\u548c\u4ea7\u54c1\u63a8\u8350\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u70b9\u5bf9\u70b9\u5174\u8da3\u63a8\u8350\uff08POI\u63a8\u8350\uff09\u4e2d\uff0c\u7531\u4e8e\u7528\u6237\u504f\u597d\u53d7\u65f6\u7a7a\u53d8\u5316\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u672c\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65f6\u7a7a\u611f\u77e5\u751f\u6210\u6a21\u578b\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u5728\u7ebfPOI\u63a8\u8350\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "\u63d0\u51faSpacetime-GR\u6a21\u578b\uff0c\u91c7\u7528\u5730\u7406\u611f\u77e5\u5206\u5c42POI\u7d22\u5f15\u7b56\u7565\uff0c\u6bcf\u4e2aPOI\u5d4c\u5165\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u65f6\u7a7a\u7f16\u7801\u6a21\u5757\uff0c\u5c06\u65f6\u7a7a\u4e0a\u4e0b\u6587\u6574\u5408\u5230\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u4e2d\u3002\u7ecf\u8fc7\u5145\u5206\u9884\u8bad\u7ec3\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u5957\u540e\u8bad\u7ec3\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u652f\u6301\u591a\u79cd\u8f93\u51fa\u683c\u5f0f\u548c\u5e94\u7528\u573a\u666f\u3002", "result": "\u901a\u8fc7\u5728\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cSpacetime-GR\u5728POI\u63a8\u8350\u51c6\u786e\u5ea6\u548c\u6392\u540d\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u9996\u6b21\u5e94\u7528\u4e8e\u5728\u7ebfPOI\u63a8\u8350\u670d\u52a1\u3002", "conclusion": "Spacetime-GR\u6210\u529f\u6269\u5c55\u4e86\u751f\u6210\u6a21\u578b\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u5728\u7ebfPOI\u63a8\u8350\u4efb\u52a1\u4e2d\u7528\u6237\u504f\u597d\u7684\u65f6\u7a7a\u53d8\u5316\u3002"}}
{"id": "2508.16147", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16147", "abs": "https://arxiv.org/abs/2508.16147", "authors": ["Ao Zhou", "Mingsheng Tu", "Luping Wang", "Tenghao Sun", "Zifeng Cheng", "Yafeng Yin", "Zhiwei Jiang", "Qing Gu"], "title": "Cross-Modal Prototype Augmentation and Dual-Grained Prompt Learning for Social Media Popularity Prediction", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Social Media Popularity Prediction is a complex multimodal task that requires\neffective integration of images, text, and structured information. However,\ncurrent approaches suffer from inadequate visual-textual alignment and fail to\ncapture the inherent cross-content correlations and hierarchical patterns in\nsocial media data. To overcome these limitations, we establish a multi-class\nframework , introducing hierarchical prototypes for structural enhancement and\ncontrastive learning for improved vision-text alignment. Furthermore, we\npropose a feature-enhanced framework integrating dual-grained prompt learning\nand cross-modal attention mechanisms, achieving precise multimodal\nrepresentation through fine-grained category modeling. Experimental results\ndemonstrate state-of-the-art performance on benchmark metrics, establishing new\nreference standards for multimodal social media analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7c7b\u6846\u67b6\uff0c\u5f15\u5165\u5c42\u6b21\u539f\u578b\u8fdb\u884c\u7ed3\u6784\u589e\u5f3a\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u6539\u5584\u89c6\u89c9\u548c\u6587\u672c\u7684\u5bf9\u9f50\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5730\u8fdb\u884c\u89c6\u89c9\u548c\u6587\u672c\u7684\u5bf9\u9f50\uff0c\u4e5f\u672a\u80fd\u6355\u6349\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u7684\u8de8\u5185\u5bb9\u5173\u8054\u548c\u5c42\u6b21\u6a21\u5f0f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u5efa\u7acb\u4e86\u591a\u7c7b\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86\u5c42\u6b21\u539f\u578b\u8fdb\u884c\u7ed3\u6784\u589e\u5f3a\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u6539\u5584\u89c6\u89c9\u548c\u6587\u672c\u7684\u5bf9\u9f50\u3002\u8fd8\u63d0\u51fa\u4e86\u5305\u542b\u53cc\u7c92\u5ea6\u63d0\u793a\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u673a\u5236\u7684\u7279\u5f81\u589e\u5f3a\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u591a\u6a21\u6001\u8868\u793a\u548c\u7ec6\u7c92\u5ea6\u7c7b\u522b\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u8bbe\u7acb\u4e86\u65b0\u7684\u53c2\u8003\u6807\u51c6\u3002"}}
{"id": "2508.16170", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16170", "abs": "https://arxiv.org/abs/2508.16170", "authors": ["Xiaoxiong Zhang", "Xin Zhou", "Zhiwei Zeng", "Yongjie Wang", "Dusit Niyato", "Zhiqi Shen"], "title": "EGRA:Toward Enhanced Behavior Graphs and Representation Alignment for Multimodal Recommendation", "comment": null, "summary": "MultiModal Recommendation (MMR) systems have emerged as a promising solution\nfor improving recommendation quality by leveraging rich item-side modality\ninformation, prompting a surge of diverse methods. Despite these advances,\nexisting methods still face two critical limitations. First, they use raw\nmodality features to construct item-item links for enriching the behavior\ngraph, while giving limited attention to balancing collaborative and\nmodality-aware semantics or mitigating modality noise in the process. Second,\nthey use a uniform alignment weight across all entities and also maintain a\nfixed alignment strength throughout training, limiting the effectiveness of\nmodality-behavior alignment. To address these challenges, we propose EGRA.\nFirst, instead of relying on raw modality features, it alleviates sparsity by\nincorporating into the behavior graph an item-item graph built from\nrepresentations generated by a pretrained MMR model. This enables the graph to\ncapture both collaborative patterns and modality aware similarities with\nenhanced robustness against modality noise. Moreover, it introduces a novel\nbi-level dynamic alignment weighting mechanism to improve modality-behavior\nrepresentation alignment, which dynamically assigns alignment strength across\nentities according to their alignment degree, while gradually increasing the\noverall alignment intensity throughout training. Extensive experiments on five\ndatasets show that EGRA significantly outperforms recent methods, confirming\nits effectiveness.", "AI": {"tldr": "EGRA\u65b9\u6cd5\u901a\u8fc7\u6539\u5584\u6a21\u6001\u884c\u4e3a\u5bf9\u9f50\u548c\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u63a8\u8350\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4ecd\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u7684\u6311\u6218\uff1a\u4e00\u662f\u4f7f\u7528\u539f\u59cb\u6a21\u6001\u7279\u5f81\u6784\u5efa\u7269\u54c1\u95f4\u94fe\u63a5\u65f6\uff0c\u672a\u80fd\u6709\u6548\u5e73\u8861\u534f\u540c\u8bed\u4e49\u4e0e\u6a21\u6001\u611f\u77e5\u8bed\u4e49\uff0c\u4e14\u5728\u8fc7\u7a0b\u4e2d\u672a\u80fd\u7f13\u89e3\u6a21\u6001\u566a\u58f0\uff1b\u4e8c\u662f\u5bf9\u6240\u6709\u5b9e\u4f53\u4f7f\u7528\u7edf\u4e00\u7684\u5bf9\u9f50\u6743\u91cd\uff0c\u5e76\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u56fa\u5b9a\u7684\u5bf9\u9f50\u5f3a\u5ea6\uff0c\u9650\u5236\u4e86\u6a21\u6001\u884c\u4e3a\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5EGRA\u3002\u9996\u5148\uff0c\u901a\u8fc7\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u63a8\u8350\u6a21\u578b\u751f\u6210\u7684\u8868\u793a\u6784\u5efa\u7269\u54c1\u95f4\u56fe\uff0c\u4ece\u800c\u5c06\u5176\u7eb3\u5165\u884c\u4e3a\u56fe\u3002\u8fd9\u4f7f\u5f97\u56fe\u53ef\u4ee5\u6355\u6349\u534f\u540c\u6a21\u5f0f\u548c\u6a21\u6001\u611f\u77e5\u76f8\u4f3c\u6027\uff0c\u5e76\u589e\u5f3a\u5bf9\u6a21\u6001\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5c42\u52a8\u6001\u5bf9\u9f50\u52a0\u6743\u673a\u5236\uff0c\u4ee5\u6539\u5584\u6a21\u6001\u884c\u4e3a\u8868\u793a\u5bf9\u9f50\uff0c\u5b83\u6839\u636e\u5b9e\u4f53\u7684\u5bf9\u9f50\u7a0b\u5ea6\u52a8\u6001\u5206\u914d\u5bf9\u9f50\u5f3a\u5ea6\uff0c\u540c\u65f6\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6e10\u589e\u52a0\u6574\u4f53\u5bf9\u9f50\u5f3a\u5ea6\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEGRA\u663e\u8457\u4f18\u4e8e\u8fd1\u671f\u7684\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "EGRA\u5728\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u901a\u8fc7\u6539\u8fdb\u6a21\u6001\u884c\u4e3a\u5bf9\u9f50\u548c\u589e\u5f3a\u5bf9\u6a21\u6001\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\u3002"}}
{"id": "2508.16210", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16210", "abs": "https://arxiv.org/abs/2508.16210", "authors": ["Ziyin Xiao", "Toyotaro Suzumura"], "title": "Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings", "comment": null, "summary": "Cross-Domain Recommender (CDR) systems aim to transfer knowledge from dense\nto sparse domains, alleviating data sparsity and cold-start issues in\nsingle-domain recommendation. While many methods assume overlapping users or\nitems to connect domains, this is often unrealistic in real-world settings.\nThus, non-overlapping CDR systems, which require no shared users or items, are\nneeded.\n  However, non-overlapping CDR is challenging due to: (1) the absence of\noverlap preventing direct bridges between domains, and (2) large distributional\ndiscrepancies degrading transfer performance. Moreover, most recommenders\nrepresent user preferences as discrete vectors, failing to capture their\nfine-grained, multi-faceted nature.\n  We propose DUP-OT (Distributional User Preferences with Optimal Transport), a\nframework for non-overlapping CDR. DUP-OT has three stages: (1) Shared\nPreprocessing, where review-based embeddings and an autoencoder encode users\nand items from both domains; (2) User GMM Weight Learning, which models user\npreferences as Gaussian mixtures with learned weights; and (3) Cross-domain\nRating Prediction, where optimal transport aligns Gaussian components across\ndomains, enabling preference transfer from source to target.\n  Experiments on Amazon review datasets show that DUP-OT effectively mitigates\ndomain discrepancy and outperforms state-of-the-art baselines under the\nnon-overlapping CDR setting.", "AI": {"tldr": "DUP-OT\u6846\u67b6\u901a\u8fc7\u5c06\u7528\u6237\u504f\u597d\u6a21\u578b\u4e3a\u9ad8\u65af\u6df7\u5408\u5e76\u4f7f\u7528\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u65e0\u91cd\u53e0\u7684\u8de8\u9886\u57df\u63a8\u8350\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8de8\u9886\u57df\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u7a00\u758f\u6027\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9\u65e0\u91cd\u53e0\u7528\u6237\u6216\u9879\u76ee\u7684\u5b9e\u9645\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86DUP-OT\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u9884\u5904\u7406\u3001\u7528\u6237GMM\u6743\u91cd\u5b66\u4e60\u548c\u8de8\u57df\u8bc4\u5206\u9884\u6d4b\u4e09\u4e2a\u9636\u6bb5\u5b9e\u73b0\u65e0\u91cd\u53e0\u7684\u8de8\u57df\u63a8\u8350\u3002", "result": "\u5728Amazon\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDUP-OT\u6709\u6548\u7f13\u89e3\u4e86\u9886\u57df\u5dee\u5f02\uff0c\u5e76\u5728\u65e0\u91cd\u53e0CDR\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "DUP-OT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u91cd\u53e0\u8de8\u9886\u57df\u63a8\u8350\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2508.16438", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16438", "abs": "https://arxiv.org/abs/2508.16438", "authors": ["Yu Liu", "Yanbing Liu", "Fangfang Yuan", "Cong Cao", "Youbang Sun", "Kun Peng", "WeiZhuo Chen", "Jianjun Li", "Zhiyuan Ma"], "title": "OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval", "comment": null, "summary": "Recent advances in large language models (LLMs) and dense retrievers have\ndriven significant progress in retrieval-augmented generation (RAG). However,\nexisting approaches face significant challenges in complex reasoning-oriented\nmulti-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior\nmethods struggle to generate robust multi-step plans for complex queries, as\nrule-based decomposers perform poorly on out-of-template questions. 2)\nSuboptimal reasoning-driven retrieval: Related methods employ limited query\nreformulation, leading to iterative retrieval loops that often fail to locate\ngolden documents. 3) Insufficient reasoning-guided filtering: Prevailing\nmethods lack the fine-grained reasoning to effectively filter salient\ninformation from noisy results, hindering utilization of retrieved knowledge.\nFundamentally, these limitations all stem from the weak coupling between\nretrieval and reasoning in current RAG architectures. We introduce the\nOrchestrated Planner-Executor Reasoning Architecture (OPERA), a novel\nreasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)\ndecomposes questions into sub-goals, which are executed by a Reason-Execute\nModule (REM) with specialized components for precise reasoning and effective\nretrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative\nPolicy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex\nmulti-hop benchmarks show OPERA's superior performance, validating both the\nMAPGRPO method and OPERA's design. Code is available at\nhttps://github.com/Ameame1/OPERA.", "AI": {"tldr": "OPERA\u6846\u67b6\u901a\u8fc7\u6539\u5584\u68c0\u7d22\u4e0e\u63a8\u7406\u7684\u8026\u5408\u5ea6\uff0c\u63d0\u9ad8\u4e86\u591a\u8df3\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u591a\u8df3\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b58\u5728\u89c4\u5212\u3001\u68c0\u7d22\u548c\u4fe1\u606f\u8fc7\u6ee4\u4e0d\u4f73\u7684\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165OPERA\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u89c4\u5212\u6a21\u5757\u548c\u63a8\u7406\u6267\u884c\u6a21\u5757\u8fdb\u884c\u63a8\u7406\u9a71\u52a8\u7684\u68c0\u7d22\uff0c\u4f7f\u7528MAPGRPO\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u590d\u6742\u591a\u8df3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOPERA\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86MAPGRPO\u65b9\u6cd5\u548cOPERA\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "OPERA\u663e\u8457\u6539\u5584\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u591a\u8df3\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2508.16516", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16516", "abs": "https://arxiv.org/abs/2508.16516", "authors": ["Lin Li", "Chunyang Li", "Yu Yin", "Xiaohui Tao", "Jianwei Zhang"], "title": "A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering", "comment": null, "summary": "In the realm of collaborative filtering recommendation systems, Graph Neural\nNetworks (GNNs) have demonstrated remarkable performance but face significant\nchallenges in deployment on resource-constrained edge devices due to their high\nembedding parameter requirements and computational costs. Using common\nquantization method directly on node embeddings may overlooks their graph based\nstructure, causing error accumulation during message passing and degrading the\nquality of quantized embeddings.To address this, we propose Graph based\nNode-Aware Dynamic Quantization training for collaborative filtering (GNAQ), a\nnovel quantization approach that leverages graph structural information to\nenhance the balance between efficiency and accuracy of GNNs for Top-K\nrecommendation. GNAQ introduces a node-aware dynamic quantization strategy that\nadapts quantization scales to individual node embeddings by incorporating graph\ninteraction relationships. Specifically, it initializes quantization intervals\nbased on node-wise feature distributions and dynamically refines them through\nmessage passing in GNN layers. This approach mitigates information loss caused\nby fixed quantization scales and captures hierarchical semantic features in\nuser-item interaction graphs. Additionally, GNAQ employs graph relation-aware\ngradient estimation to replace traditional straight-through estimators,\nensuring more accurate gradient propagation during training. Extensive\nexperiments on four real-world datasets demonstrate that GNAQ outperforms\nstate-of-the-art quantization methods, including BiGeaR and N2UQ, by achieving\naverage improvement in 27.8\\% Recall@10 and 17.6\\% NDCG@10 under 2-bit\nquantization. In particular, GNAQ is capable of maintaining the performance of\nfull-precision models while reducing their model sizes by 8 to 12 times; in\naddition, the training time is twice as fast compared to quantization baseline\nmethods.", "AI": {"tldr": "GNAQ, a novel dynamic quantization method, enhances GNN efficiency and accuracy in collaborative filtering, outperforming existing methods with improved Recall@10 and NDCG@10 in a reduced model size and training time.", "motivation": "To overcome the high computational and embedding parameter costs of GNNs on edge devices and improve the quality of quantized embeddings in collaborative filtering systems.", "method": "GNAQ introduces node-aware dynamic quantization that adjusts quantization scales based on graph interactions and uses graph relation-aware gradient estimation.", "result": "GNAQ outperforms existing quantization methods like BiGeaR and N2UQ by 27.8% in Recall@10 and 17.6% in NDCG@10 under 2-bit quantization, maintaining full-precision performance with significantly reduced model sizes and faster training times.", "conclusion": "GNAQ significantly improves the performance of GNNs for collaborative filtering in resource-constrained environments by leveraging graph structural information for dynamic quantization."}}
{"id": "2508.16550", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16550", "abs": "https://arxiv.org/abs/2508.16550", "authors": ["Nirmal Gaud", "Prasad Krishna Murthy", "Mostaque Md. Morshedur Hassan", "Abhijit Ganguly", "Vinay Mali", "Ms Lalita Bhagwat Randive", "Abhaypratap Singh"], "title": "Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis", "comment": "7 pages, 1 figure, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:2508.04293", "summary": "This study introduces the Enhanced NIRMAL (Novel Integrated Robust\nMulti-Adaptation Learning with Damped Nesterov Acceleration) optimizer, an\nimproved version of the original NIRMAL optimizer. By incorporating an\n$(\\alpha, r)$-damped Nesterov acceleration mechanism, Enhanced NIRMAL improves\nconvergence stability while retaining chess-inspired strategies of gradient\ndescent, momentum, stochastic perturbations, adaptive learning rates, and\nnon-linear transformations.\n  We evaluate Enhanced NIRMAL against Adam, SGD with Momentum, Nesterov, and\nthe original NIRMAL on four benchmark image classification datasets: MNIST,\nFashionMNIST, CIFAR-10, and CIFAR-100, using tailored convolutional neural\nnetwork (CNN) architectures.\n  Enhanced NIRMAL achieves a test accuracy of 46.06\\% and the lowest test loss\n(1.960435) on CIFAR-100, surpassing the original NIRMAL (44.34\\% accuracy) and\nclosely rivaling SGD with Momentum (46.43\\% accuracy). These results underscore\nEnhanced NIRMAL's superior generalization and stability, particularly on\ncomplex datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684NIRMAL\u4f18\u5316\u5668\uff0c\u589e\u5f3a\u7248NIRMAL\uff0c\u901a\u8fc7\u5f15\u5165\u963b\u5c3cNesterov\u52a0\u901f\u673a\u5236\u63d0\u9ad8\u4e86\u6536\u655b\u7a33\u5b9a\u6027\uff0c\u5e76\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u4f18\u5316\u5668\u7684\u6536\u655b\u7a33\u5b9a\u6027\u5e76\u83b7\u5f97\u66f4\u597d\u7684\u666e\u9002\u6027\uff0c\u63d0\u51fa\u4e86\u589e\u5f3a\u7248NIRMAL\u4f18\u5316\u5668\u3002", "method": "\u91c7\u7528\u589e\u5f3a\u7248NIRMAL\u4f18\u5316\u5668\uff0c\u8be5\u4f18\u5316\u5668\u7ed3\u5408\u4e86(\u03b1, r)\u963b\u5c3cNesterov\u52a0\u901f\u673a\u5236\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u68cb\u76d8\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u5982\u68af\u5ea6\u4e0b\u964d\u3001\u52a8\u91cf\u3001\u968f\u673a\u6270\u52a8\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u548c\u975e\u7ebf\u6027\u53d8\u6362\u3002", "result": "\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u589e\u5f3a\u7248NIRMAL\u4f18\u5316\u5668\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u4e3a46.06%\uff0c\u6d4b\u8bd5\u635f\u5931\u4e3a1.960435\uff0c\u8d85\u8d8a\u4e86\u539f\u59cb\u7248NIRMAL\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0844.34%\uff09\uff0c\u5e76\u63a5\u8fd1SGD\u52a8\u91cf\u4f18\u5316\u5668\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0846.43%\uff09\u3002", "conclusion": "\u589e\u5f3a\u7248NIRMAL\u4f18\u5316\u5668\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u7684\u666e\u9002\u6027\u548c\u7a33\u5b9a\u6027\u4f18\u4e8e\u539f\u59cb\u7248NIRMAL\u3002"}}
{"id": "2508.16573", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16573", "abs": "https://arxiv.org/abs/2508.16573", "authors": ["Huishi Luo", "Fuzhen Zhuang", "Yongchun Zhu", "Yiqing Wu", "Bo Kang", "Ruobing Xie", "Feng Xia", "Deqing Wang", "Jin Dong"], "title": "ORCA: Mitigating Over-Reliance for Multi-Task Dwell Time Prediction with Causal Decoupling", "comment": "Accepted as a short paper at CIKM 2025", "summary": "Dwell time (DT) is a critical post-click metric for evaluating user\npreference in recommender systems, complementing the traditional click-through\nrate (CTR). Although multi-task learning is widely adopted to jointly optimize\nDT and CTR, we observe that multi-task models systematically collapse their DT\npredictions to the shortest and longest bins, under-predicting the moderate\ndurations. We attribute this moderate-duration bin under-representation to\nover-reliance on the CTR-DT spurious correlation, and propose ORCA to address\nit with causal-decoupling. Specifically, ORCA explicitly models and subtracts\nCTR's negative transfer while preserving its positive transfer. We further\nintroduce (i) feature-level counterfactual intervention, and (ii) a\ntask-interaction module with instance inverse-weighting, weakening CTR-mediated\neffect and restoring direct DT semantics. ORCA is model-agnostic and easy to\ndeploy. Experiments show an average 10.6% lift in DT metrics without harming\nCTR. Code is available at\nhttps://github.com/Chrissie-Law/ORCA-Mitigating-Over-Reliance-for-Multi-Task-Dwell-Time-Prediction-with-Causal-Decoupling.", "AI": {"tldr": "ORCA\u901a\u8fc7\u56e0\u679c\u53bb\u8026\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u505c\u7559\u65f6\u95f4\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u5bf9\u70b9\u51fb\u7387\u7684\u8fc7\u5ea6\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u5728\u4f18\u5316DT\u548cCTR\u65f6\u5b58\u5728\u5bf9\u4e2d\u7b49\u65f6\u957f\u7684DT\u9884\u6d4b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5bf9CTR-DT\u865a\u5047\u76f8\u5173\u6027\u7684\u8fc7\u5ea6\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aORCA\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u56e0\u679c\u53bb\u8026\u7b56\u7565\uff0c\u7ed3\u5408\u7279\u5f81\u7ea7\u53cd\u4e8b\u5b9e\u5e72\u9884\u548c\u4efb\u52a1\u4ea4\u4e92\u6a21\u5757\uff0c\u4ee5\u51cf\u5f31CTR\u7684\u5f71\u54cd\u5e76\u6062\u590dDT\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cORCA\u5728DT\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u5347\u4e8610.6%\u4e14\u4e0d\u5f71\u54cdCTR\u3002", "conclusion": "ORCA\u663e\u8457\u63d0\u9ad8\u4e86DT\u6307\u6807\uff0c\u800c\u5bf9CTR\u6ca1\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u5176\u4ee3\u7801\u5df2\u5f00\u653e\u83b7\u53d6\u3002"}}
