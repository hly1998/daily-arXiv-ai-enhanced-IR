{"id": "2508.14052", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14052", "abs": "https://arxiv.org/abs/2508.14052", "authors": ["Chanyeol Choi", "Jihoon Kwon", "Alejandro Lopez-Lira", "Chaewoon Kim", "Minjae Kim", "Juneha Hwang", "Jaeseon Ha", "Hojun Choi", "Suyeol Yun", "Yongjin Kim", "Yongjae Lee"], "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering", "comment": "6 pages", "summary": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods-whether sparse or dense-often fall short in\nretrieval accuracy, as it requires not only capturing semantic similarity but\nalso performing fine-grained reasoning over document structure and\ndomain-specific knowledge. Recent advances in large language models (LLMs) have\nopened up new opportunities for retrieval with multi-step reasoning, where the\nmodel ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms\nand assesses whether LLM agents can (1) identify the most relevant document\ntype among candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance. We will release the dataset\npublicly upon acceptance of the paper and plan to expand and share dataset for\nthe full S&P 500 and beyond.", "AI": {"tldr": "The paper introduces FinAgentBench, a benchmark for evaluating multi-step reasoning retrieval in finance, achieves improvements with fine-tuned LLMs, and plans further expansions post-acceptance.", "motivation": "To address the lack of benchmarks for evaluating multi-step reasoning retrieval capabilities of LLMs in the financial domain.", "method": "The paper introduces a large-scale benchmark named FinAgentBench to evaluate the multi-step reasoning retrieval capabilities of large language models in the financial domain.", "result": "The benchmark contains 3,429 expert-annotated examples focusing on S&P-100 firms, demonstrating that state-of-the-art models can improve retrieval performance through fine-tuning. The framework achieves separation of reasoning steps to handle context limitations.", "conclusion": "The paper concludes that with the introduction of FinAgentBench, substantial improvements in agentic retrieval can be achieved through targeted fine-tuning of LLMs in the financial domain."}}
{"id": "2508.14058", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14058", "abs": "https://arxiv.org/abs/2508.14058", "authors": ["Jingmao Zhang", "Zhiting Zhao", "Yunqi Lin", "Jianghong Ma", "Tianjun Wei", "Haijun Zhang", "Xiaofeng Zhang"], "title": "Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks", "comment": "Accepted for publication at ACM Multimedia (ACM MM) 2025. 10 pages, 5\n  figures. Code and dataset: https://github.com/zqxwcevrtyui/DP2Rec", "summary": "The explosive growth of the video game industry has created an urgent need\nfor recommendation systems that can scale with expanding catalogs and maintain\nuser engagement. While prior work has explored accuracy and diversity in\nrecommendations, existing models underutilize playtime, a rich behavioral\nsignal unique to gaming platforms, and overlook the potential of multimodal\ninformation to enhance diversity. In this paper, we propose DP2Rec, a novel\nDual-Phase Playtime-guided Recommendation model designed to jointly optimize\naccuracy and diversity. First, we introduce a playtime-guided interest\nintensity exploration module that separates strong and weak preferences via\ndual-beta modeling, enabling fine-grained user profiling and more accurate\nrecommendations. Second, we present a playtime-guided multimodal random walks\nmodule that simulates player exploration using transitions guided by both\nplaytime-derived interest similarity and multimodal semantic similarity. This\nmechanism preserves core preferences while promoting cross-category discovery\nthrough latent semantic associations and adaptive category balancing. Extensive\nexperiments on a real-world game dataset show that DP2Rec outperforms existing\nmethods in both recommendation accuracy and diversity.", "AI": {"tldr": "DP2Rec\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u6e38\u620f\u65f6\u95f4\u548c\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89c6\u9891\u6e38\u620f\u884c\u4e1a\u7684\u5feb\u901f\u589e\u957f\u9700\u8981\u63a8\u8350\u7cfb\u7edf\u80fd\u9002\u5e94\u4e0d\u65ad\u6269\u5927\u7684\u76ee\u5f55\u5e76\u4fdd\u6301\u7528\u6237\u53c2\u4e0e\u3002\u76ee\u524d\u7684\u6a21\u578b\u672a\u5145\u5206\u5229\u7528\u72ec\u7279\u7684\u884c\u4e3a\u4fe1\u53f7\u2014\u2014\u6e38\u620f\u65f6\u95f4\uff0c\u4e5f\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u9ad8\u591a\u6837\u6027\u7684\u6f5c\u529b\u3002", "method": "\u8be5\u6a21\u578b\u5f15\u5165\u4e86\u4e24\u4e2a\u6a21\u5757\uff1a\u4e00\u4e2a\u662f\u57fa\u4e8e\u6e38\u620f\u65f6\u95f4\u6307\u5bfc\u7684\u5174\u8da3\u5f3a\u5ea6\u63a2\u7d22\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u03b2\u5efa\u6a21\u6765\u5206\u79bb\u5f3a\u5f31\u504f\u597d\uff1b\u53e6\u4e00\u4e2a\u662f\u57fa\u4e8e\u6e38\u620f\u65f6\u95f4\u6307\u5bfc\u7684\u591a\u6a21\u6001\u968f\u673a\u6e38\u8d70\u6a21\u5757\uff0c\u7ed3\u5408\u6e38\u620f\u65f6\u95f4\u548c\u591a\u6a21\u6001\u8bed\u4e49\u76f8\u4f3c\u6027\u6765\u6a21\u62df\u73a9\u5bb6\u7684\u63a2\u7d22\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDP2Rec\u5728\u5b9e\u9645\u6e38\u620f\u6570\u636e\u96c6\u4e0a\u7684\u63a8\u8350\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DP2Rec\u6a21\u578b\uff0c\u5728\u6e38\u620f\u63a8\u8350\u7cfb\u7edf\u4e2d\u540c\u65f6\u4f18\u5316\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.14059", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14059", "abs": "https://arxiv.org/abs/2508.14059", "authors": ["Mengyang Cao", "Frank F. Yang", "Yi Jin", "Yijun Yan"], "title": "Graph Neural Network for Product Recommendation on the Amazon Co-purchase Graph", "comment": "15 pages, 5 figures, preprint", "summary": "Identifying relevant information among massive volumes of data is a challenge\nfor modern recommendation systems. Graph Neural Networks (GNNs) have\ndemonstrated significant potential by utilizing structural and semantic\nrelationships through graph-based learning. This study assessed the abilities\nof four GNN architectures, LightGCN, GraphSAGE, GAT, and PinSAGE, on the Amazon\nProduct Co-purchase Network under link prediction settings. We examined\npractical trade-offs between architectures, model performance, scalability,\ntraining complexity and generalization. The outcomes demonstrated each model's\nperformance characteristics for deploying GNN in real-world recommendation\nscenarios.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cdGNN\u67b6\u6784\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u94fe\u63a5\u9884\u6d4b\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u5728\u6d77\u91cf\u6570\u636e\u4e2d\u8bc6\u522b\u76f8\u5173\u4fe1\u606f\uff0cGNN\u901a\u8fc7\u56fe\u5b66\u4e60\u5229\u7528\u7ed3\u6784\u548c\u8bed\u4e49\u5173\u7cfb\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u56db\u79cdGNN\u67b6\u6784\u8fdb\u884c\u94fe\u63a5\u9884\u6d4b\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728Amazon\u4ea7\u54c1\u5171\u8d2d\u7f51\u7edc\u4e0a\u7684\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e86\u4e0d\u540c\u67b6\u6784\u4e4b\u95f4\u7684\u5b9e\u7528\u6027\u6743\u8861\uff0c\u5305\u62ec\u6a21\u578b\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u3001\u8bad\u7ec3\u590d\u6742\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6bcf\u79cdGNN\u6a21\u578b\u5728\u5b9e\u9645\u63a8\u8350\u573a\u666f\u4e2d\u7684\u6027\u80fd\u7279\u5f81\u5404\u5f02\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2508.14061", "categories": ["cs.IR", "68P30, 68T50"], "pdf": "https://arxiv.org/pdf/2508.14061", "abs": "https://arxiv.org/abs/2508.14061", "authors": ["Anurag Kumar Ojha"], "title": "GPT-2 as a Compression Preprocessor: Improving Gzip for Structured Text Domains", "comment": null, "summary": "In the modern era, large volumes of data are being produced continuously,\nespecially in domain-specific fields such as medical records and clinical\nfiles, defence logs and HTML-based web traffic. Data with such volume and\ncomplexity needs to be compressed before storing and transmitting efficiently.\nData compression has gained significant attention from modern researchers,\nresulting in the development of fast and efficient compression algorithms such\nas Gzip. However, since gzip works on the principle of repetition of binary\npatterns, one of the limitations of gzip is that domain-specific formats like\nJSON, XML, HTML, and log files, while structured, may have semantic repetition\nbut not syntactic repetition, which gzip finds difficult to compress. In this\narticle, we propose a GPT-based preprocessor for such domain-specific files. We\npropose a pipeline made up of GPT-2 taking domain-specific files as input,\nwhich pattern-based compressors like gzip find difficult to work on. The\npreprocessor results are output in a file that is designed for compressors like\ngzip. After preprocessing, the gzip works on the other end of the pipeline and\ncompresses the data as usual. We used different types of both real-world and\nsynthetically generated data, such as logs and HTML files, for the experiment\nof the proposed model. We found promising results and an improvement of the\nDefence logs by 0.34 per cent and HTML files by 5.8 per cent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPT\u7684\u9884\u5904\u7406\u5668\uff0c\u7528\u4e8e\u4f18\u5316\u57df\u7279\u5b9a\u6587\u4ef6\u7684\u538b\u7f29\u6548\u679c\u3002", "motivation": "\u9488\u5bf9gzip\u5728\u538b\u7f29\u57df\u7279\u5b9a\u683c\u5f0f\u6587\u4ef6\u65f6\u5b58\u5728\u7684\u8bed\u4e49\u91cd\u590d\u800c\u975e\u53e5\u6cd5\u91cd\u590d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528GPT-2\u4f5c\u4e3a\u9884\u5904\u7406\u5668\uff0c\u5c06\u57df\u7279\u5b9a\u6587\u4ef6\u8f6c\u6362\u4e3a\u9002\u5408gzip\u538b\u7f29\u7684\u683c\u5f0f\uff0c\u7136\u540e\u8fdb\u884c\u540e\u7eed\u7684\u538b\u7f29\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u56fd\u9632\u65e5\u5fd7\u538b\u7f29\u6548\u679c\u63d0\u9ad80.34%\uff0cHTML\u6587\u4ef6\u538b\u7f29\u6548\u679c\u63d0\u9ad85.8%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u65f6\u8868\u73b0\u51fa\u6539\u5584\u6548\u679c\u3002"}}
{"id": "2508.14063", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14063", "abs": "https://arxiv.org/abs/2508.14063", "authors": ["Moran Sorka", "Alon Gorenshtein", "Dvir Aran", "Shahar Shelly"], "title": "A Multi-Agent Approach to Neurological Clinical Reasoning", "comment": null, "summary": "Large language models (LLMs) have shown promise in medical domains, but their\nability to handle specialized neurological reasoning requires systematic\nevaluation. We developed a comprehensive benchmark using 305 questions from\nIsraeli Board Certification Exams in Neurology, classified along three\ncomplexity dimensions: factual knowledge depth, clinical concept integration,\nand reasoning complexity. We evaluated ten LLMs using base models,\nretrieval-augmented generation (RAG), and a novel multi-agent system. Results\nshowed significant performance variation. OpenAI-o1 achieved the highest base\nperformance (90.9% accuracy), while specialized medical models performed poorly\n(52.9% for Meditron-70B). RAG provided modest benefits but limited\neffectiveness on complex reasoning questions. In contrast, our multi-agent\nframework, decomposing neurological reasoning into specialized cognitive\nfunctions including question analysis, knowledge retrieval, answer synthesis,\nand validation, achieved dramatic improvements, especially for mid-range\nmodels. The LLaMA 3.3-70B-based agentic system reached 89.2% accuracy versus\n69.5% for its base model, with substantial gains on level 3 complexity\nquestions. The multi-agent approach transformed inconsistent subspecialty\nperformance into uniform excellence, addressing neurological reasoning\nchallenges that persisted with RAG enhancement. We validated our approach using\nan independent dataset of 155 neurological cases from MedQA. Results confirm\nthat structured multi-agent approaches designed to emulate specialized\ncognitive processes significantly enhance complex medical reasoning, offering\npromising directions for AI assistance in challenging clinical contexts.", "AI": {"tldr": "\u591a\u4ee3\u7406\u7cfb\u7edf\u901a\u8fc7\u6a21\u62df\u4e13\u95e8\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u795e\u7ecf\u9886\u57df\u590d\u6742\u533b\u7597\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u795e\u7ecf\u9886\u57df\u4e13\u95e8\u5316\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u4fbf\u63a2\u7d22\u63d0\u9ad8\u5176\u5728\u590d\u6742\u533b\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528305\u9053\u4ee5\u8272\u5217\u795e\u7ecf\u75c5\u5b66\u8ba4\u8bc1\u8003\u8bd5\u9898\u76ee\u521b\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5341\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u521b\u65b0\u65b9\u6cd5\u3002", "result": "\u6539\u8fdb\u540e\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u4e2d\u7b49\u590d\u6742\u5ea6\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u663e\u8457\u63d0\u9ad8\uff0cLLaMA 3.3-70B\u57fa\u7840\u7cfb\u7edf\u7684\u51c6\u786e\u7387\u4ece69.5%\u63d0\u5347\u81f389.2%\u3002", "conclusion": "\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u533b\u5b66\u63a8\u7406\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u6216\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u751f\u6210\u65b9\u5f0f\u3002"}}
{"id": "2508.14064", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14064", "abs": "https://arxiv.org/abs/2508.14064", "authors": ["Yao Ding", "Yuqing Wu", "Ziyang Ding"], "title": "An automatic patent literature retrieval system based on LLM-RAG", "comment": null, "summary": "With the acceleration of technological innovation efficient retrieval and\nclassification of patent literature have become essential for intellectual\nproperty management and enterprise RD Traditional keyword and rulebased\nretrieval methods often fail to address complex query intents or capture\nsemantic associations across technical domains resulting in incomplete and\nlowrelevance results This study presents an automated patent retrieval\nframework integrating Large Language Models LLMs with RetrievalAugmented\nGeneration RAG technology The system comprises three components: 1) a\npreprocessing module for patent data standardization, 2) a highefficiency\nvector retrieval engine leveraging LLMgenerated embeddings, and 3) a\nRAGenhanced query module that combines external document retrieval with\ncontextaware response generation Evaluations were conducted on the Google\nPatents dataset 20062024 containing millions of global patent records with\nmetadata such as filing date domain and status The proposed gpt35turbo0125RAG\nconfiguration achieved 805 semantic matching accuracy and 92.1% recall\nsurpassing baseline LLM methods by 28 percentage points The framework also\ndemonstrated strong generalization in crossdomain classification and semantic\nclustering tasks These results validate the effectiveness of LLMRAG integration\nfor intelligent patent retrieval providing a foundation for nextgeneration\nAIdriven intellectual property analysis platforms", "AI": {"tldr": "\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u63d0\u9ad8\u4e13\u5229\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u968f\u7740\u6280\u672f\u521b\u65b0\u52a0\u901f\uff0c\u4e13\u5229\u6587\u732e\u7684\u9ad8\u6548\u68c0\u7d22\u548c\u5206\u7c7b\u6210\u4e3a\u77e5\u8bc6\u4ea7\u6743\u7ba1\u7406\u548c\u4f01\u4e1a\u7814\u53d1\u7684\u91cd\u8981\u9700\u6c42\u3002\u4f20\u7edf\u7684\u5173\u952e\u8bcd\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u67e5\u8be2\u610f\u56fe\u6216\u8de8\u6280\u672f\u9886\u57df\u7684\u8bed\u4e49\u5173\u8054\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u5b8c\u6574\u4e14\u76f8\u5173\u6027\u4f4e\u3002", "method": "\u672c\u7814\u7a76\u7684\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u4e13\u5229\u6570\u636e\u6807\u51c6\u5316\u7684\u9884\u5904\u7406\u6a21\u5757\uff1b\u5229\u7528LLM\u751f\u6210\u5d4c\u5165\u8fdb\u884c\u9ad8\u6548\u5411\u91cf\u68c0\u7d22\u7684\u5f15\u64ce\uff1b\u7ed3\u5408\u5916\u90e8\u6587\u6863\u68c0\u7d22\u4e0e\u4e0a\u4e0b\u6587\u54cd\u5e94\u751f\u6210\u7684RAG\u589e\u5f3a\u67e5\u8be2\u6a21\u5757\u3002", "result": "\u5728\u8c37\u6b4c\u4e13\u5229\u6570\u636e\u96c6\uff082006-2024\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684gpt-3.5-turbo-0.125-RAG\u914d\u7f6e\u8fbe\u5230\u4e8680.5%\u7684\u8bed\u4e49\u5339\u914d\u51c6\u786e\u7387\u548c92.1%\u7684\u53ec\u56de\u7387\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebfLLM\u65b9\u6cd528\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u4e13\u5229\u68c0\u7d22\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u4e49\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u68c0\u7d22\u53ec\u56de\u7387\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u9a71\u52a8\u7684\u77e5\u8bc6\u4ea7\u6743\u5206\u6790\u5e73\u53f0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.14065", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14065", "abs": "https://arxiv.org/abs/2508.14065", "authors": ["Madiraju Srilakshmi", "Kartavya Kothari", "Kamlesh Marathe", "Vedavyas Chigurupati", "Hitesh Kapoor"], "title": "Personalized Contest Recommendation in Fantasy Sports", "comment": null, "summary": "In daily fantasy sports, players enter into \"contests\" where they compete\nagainst each other by building teams of athletes that score fantasy points\nbased on what actually occurs in a real-life sports match. For any given sports\nmatch, there are a multitude of contests available to players, with substantial\nvariation across 3 main dimensions: entry fee, number of spots, and the prize\npool distribution. As player preferences are also quite heterogeneous, contest\npersonalization is an important tool to match players with contests. This paper\npresents a scalable contest recommendation system, powered by a Wide and Deep\nInteraction Ranker (WiDIR) at its core. We productionized this system at our\ncompany, one of the large fantasy sports platforms with millions of daily\ncontests and millions of players, where online experiments show a marked\nimprovement over other candidate models in terms of recall and other critical\nbusiness metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u5e7b\u60f3\u4f53\u80b2\u5e73\u53f0\u7684\u6bd4\u8d5b\u63a8\u8350\u7cfb\u7edf\uff0c\u57fa\u4e8eWiDIR\u6a21\u578b\uff0c\u5df2\u5728\u5927\u578b\u5e73\u53f0\u4e0a\u6210\u529f\u5b9e\u73b0\uff0c\u5e76\u8bc1\u660e\u5728\u53ec\u56de\u7387\u548c\u4e1a\u52a1\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u73a9\u5bb6\u504f\u597d\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u6bd4\u8d5b\u4e2a\u6027\u5316\u662f\u5c06\u73a9\u5bb6\u4e0e\u5408\u9002\u6bd4\u8d5b\u5339\u914d\u7684\u91cd\u8981\u5de5\u5177\u3002", "method": "\u8be5\u7cfb\u7edf\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u79f0\u4e3aWiDIR\uff08Wide and Deep Interaction Ranker\uff09\u7684\u6a21\u578b\u3002", "result": "\u5728\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u53ec\u56de\u7387\u548c\u5176\u4ed6\u5173\u952e\u4e1a\u52a1\u6307\u6807\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5019\u9009\u6a21\u578b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6bd4\u8d5b\u63a8\u8350\u7cfb\u7edf\uff0c\u5e76\u5728\u5927\u578b\u5e7b\u60f3\u4f53\u80b2\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u663e\u793a\u51fa\u5728\u53ec\u56de\u7387\u548c\u5176\u4ed6\u5173\u952e\u4e1a\u52a1\u6307\u6807\u4e0a\u7684\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2508.14066", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14066", "abs": "https://arxiv.org/abs/2508.14066", "authors": ["Lorenz Brehme", "Benedikt Dornauer", "Thomas Str\u00f6hle", "Maximilian Ehrhart", "Ruth Breu"], "title": "Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation", "comment": "This preprint was accepted for presentation at the 17th International\n  Conference on Knowledge Discovery and Information Retrieval (KDIR25)", "summary": "Retrieval-Augmented Generation (RAG) is a well-established and rapidly\nevolving field within AI that enhances the outputs of large language models by\nintegrating relevant information retrieved from external knowledge sources.\nWhile industry adoption of RAG is now beginning, there is a significant lack of\nresearch on its practical application in industrial contexts. To address this\ngap, we conducted a semistructured interview study with 13 industry\npractitioners to explore the current state of RAG adoption in real-world\nsettings. Our study investigates how companies apply RAG in practice, providing\n(1) an overview of industry use cases, (2) a consolidated list of system\nrequirements, (3) key challenges and lessons learned from practical\nexperiences, and (4) an analysis of current industry evaluation methods. Our\nmain findings show that current RAG applications are mostly limited to\ndomain-specific QA tasks, with systems still in prototype stages; industry\nrequirements focus primarily on data protection, security, and quality, while\nissues such as ethics, bias, and scalability receive less attention; data\npreprocessing remains a key challenge, and system evaluation is predominantly\nconducted by humans rather than automated methods.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86RAG\u5728\u5de5\u4e1a\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u53d1\u73b0\u5e94\u7528\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u4fdd\u62a4\u548c\u8d28\u91cf\uff0c\u8bc4\u4f30\u591a\u7531\u4eba\u5de5\u8fdb\u884c\u3002", "motivation": "\u5f25\u8865RAG\u5728\u5de5\u4e1a\u5e94\u7528\u7814\u7a76\u4e0a\u7684\u7f3a\u4e4f\uff0c\u63d0\u4f9b\u884c\u4e1a\u5e94\u7528\u73b0\u72b6\u7684\u6d1e\u5bdf\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u7814\u7a76\u5bf913\u4f4d\u884c\u4e1a\u5b9e\u8df5\u8005\u8fdb\u884c\u8c03\u67e5\u3002", "result": "\u53d1\u73b0\u884c\u4e1a\u9700\u6c42\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u4fdd\u62a4\u3001\u5b89\u5168\u548c\u8d28\u91cf\u65b9\u9762\uff1b\u6570\u636e\u9884\u5904\u7406\u662f\u4e3b\u8981\u6311\u6218\uff1b\u7cfb\u7edf\u8bc4\u4f30\u591a\u7531\u4eba\u5de5\u8fdb\u884c\u3002", "conclusion": "\u76ee\u524d\uff0cRAG\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u4e3b\u8981\u4efb\u52a1\u662f\u7279\u5b9a\u9886\u57df\u7684\u95ee\u7b54\uff0c\u7cfb\u7edf\u591a\u5904\u4e8e\u539f\u578b\u9636\u6bb5\u3002"}}
{"id": "2508.14180", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14180", "abs": "https://arxiv.org/abs/2508.14180", "authors": ["Gaurav Bhatt", "Kiran Koshy Thekumparampil", "Tanmay Gangwani", "Tesi Xiao", "Leonid Sigal"], "title": "RewardRank: Optimizing True Learning-to-Rank Utility", "comment": null, "summary": "Traditional ranking systems rely on proxy loss functions that assume\nsimplistic user behavior, such as users preferring a rank list where items are\nsorted by hand-crafted relevance. However, real-world user interactions are\ninfluenced by complex behavioral biases, including position bias, brand\naffinity, decoy effects, and similarity aversion, which these objectives fail\nto capture. As a result, models trained on such losses often misalign with\nactual user utility, such as the probability of any click or purchase across\nthe ranked list. In this work, we propose a data-driven framework for modeling\nuser behavior through counterfactual reward learning. Our method, RewardRank,\nfirst trains a deep utility model to estimate user engagement for entire item\npermutations using logged data. Then, a ranking policy is optimized to maximize\npredicted utility via differentiable soft permutation operators, enabling\nend-to-end training over the space of factual and counterfactual rankings. To\naddress the challenge of evaluation without ground-truth for unseen\npermutations, we introduce two automated protocols: (i) $\\textit{KD-Eval}$,\nusing a position-aware oracle for counterfactual reward estimation, and (ii)\n$\\textit{LLM-Eval}$, which simulates user preferences via large language\nmodels. Experiments on large-scale benchmarks, including Baidu-ULTR and the\nAmazon KDD Cup datasets, demonstrate that our approach consistently outperforms\nstrong baselines, highlighting the effectiveness of modeling user behavior\ndynamics for utility-optimized ranking. Our code is available at:\nhttps://github.com/GauravBh1010tt/RewardRank", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7RewardRank\u65b9\u6cd5\u5b9e\u73b0\u6548\u7528\u4f18\u5316\u6392\u540d\uff0c\u8d85\u8fc7\u4f20\u7edf\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u6392\u540d\u7cfb\u7edf\u91c7\u7528\u7684\u4ee3\u7406\u635f\u5931\u51fd\u6570\u672a\u80fd\u6709\u6548\u6355\u6349\u7528\u6237\u590d\u6742\u884c\u4e3a\u504f\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0e\u5b9e\u9645\u7528\u6237\u6548\u7528\u4e0d\u4e00\u81f4\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u4ee5\u4f18\u5316\u6392\u540d\u6548\u7528\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u6df1\u5ea6\u6548\u7528\u6a21\u578b\uff0c\u901a\u8fc7\u767b\u5f55\u6570\u636e\u8bc4\u4f30\u7528\u6237\u4e0e\u6574\u4e2a\u9879\u76ee\u6392\u5e8f\u7684\u4e92\u52a8\uff0c\u7136\u540e\u4f7f\u7528\u53ef\u5fae\u6392\u5e8f\u64cd\u4f5c\u4f18\u5316\u6392\u5e8f\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u9884\u6d4b\u6548\u7528\uff0c\u5e76\u5b9e\u73b0\u8fc7\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u6392\u540d\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u3002\u540c\u65f6\u5f15\u5165\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u5982KD-Eval\u548cLLM-Eval\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRewardRank\u5728\u5305\u62ecBaidu-ULTR\u548cAmazon KDD Cup\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u5728\u7528\u6237\u884c\u4e3a\u52a8\u6001\u5efa\u6a21\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "RewardRank\u5b9e\u73b0\u4e86\u5bf9\u7528\u6237\u884c\u4e3a\u52a8\u6001\u7684\u6709\u6548\u5efa\u6a21\uff0c\u5e76\u5728\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51f8\u663e\u4e86\u8be5\u9879\u7814\u7a76\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.14420", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.14420", "abs": "https://arxiv.org/abs/2508.14420", "authors": ["Shuli Wang", "Yinqiu Huang", "Changhao Li", "Yuan Zhou", "Yonggang Liu", "Yongqiang Zhang", "Yinhua Zhu", "Haitao Wang", "Xingxing Wang"], "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan", "comment": "Accepted by CIKM 2025", "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.", "AI": {"tldr": "YOLOR\u662f\u4e00\u79cd\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u7684\u4e00\u9636\u6bb5\u91cd\u6392\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ee5\u524d\u65b9\u6cd5\u7684\u6548\u7387\u4e0e\u6548\u679c\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u91cd\u6392\u5728\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aYOLOR\u7684\u4e00\u9636\u6bb5\u91cd\u6392\u65b9\u6cd5\uff0c\u91c7\u7528\u6811\u72b6\u4e0a\u4e0b\u6587\u63d0\u53d6\u6a21\u5757\u548c\u4e0a\u4e0b\u6587\u7f13\u5b58\u6a21\u5757\u3002", "result": "\u5728\u516c\u5171\u548c\u884c\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86YOLOR\u7684\u6027\u80fd\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u7f8e\u56e2\u5916\u5356\u5e73\u53f0\u3002", "conclusion": "YOLOR\u901a\u8fc7\u53bb\u9664\u7b80\u5355\u641c\u7d22\u5355\u5143\uff0c\u4ec5\u4fdd\u7559\u7cbe\u786e\u641c\u7d22\u5355\u5143\uff0c\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u6574\u4f53\u6709\u6548\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.14468", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.14468", "abs": "https://arxiv.org/abs/2508.14468", "authors": ["Yueqing Xuan", "Kacper Sokol", "Mark Sanderson", "Jeffrey Chan"], "title": "Diverse Negative Sampling for Implicit Collaborative Filtering", "comment": null, "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8d1f\u91c7\u6837\u65b9\u6cd5(DivNS)\uff0c\u901a\u8fc7\u589e\u52a0\u8d1f\u6837\u672c\u591a\u6837\u6027\u6765\u6539\u5584\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u5e76\u4e14\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u901a\u5e38\u4f1a\u5728\u5bc6\u96c6\u533a\u57df\u8fc7\u5ea6\u91c7\u6837\u8d1f\u6837\u672c\uff0c\u5bfc\u81f4\u8d1f\u6837\u672c\u540c\u8d28\u5316\u53ca\u6a21\u578b\u8868\u73b0\u80fd\u529b\u53d7\u9650\uff0c\u9700\u63d0\u9ad8\u8d1f\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u3002", "method": "DivNS\u9996\u5148\u901a\u8fc7\u7f13\u5b58\u7b56\u7565\u627e\u5230\u5177\u6709\u9ad8\u504f\u597d\u8bc4\u5206\u7684\u56f0\u96be\u8d1f\u6837\u672c\uff0c\u7136\u540e\u9009\u62e9\u591a\u6837\u5316\u7684\u8d1f\u6837\u672c\u5b50\u96c6\uff0c\u6700\u540e\u5229\u7528\u5408\u6210\u8d1f\u6837\u672c\u751f\u6210\u5668\u7ec4\u5408\u8fd9\u4e9b\u8d1f\u6837\u672c\u548c\u56f0\u96be\u8d1f\u6837\u672c\u4ee5\u5f62\u6210\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDivNS\u5728\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684DivNS\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u5408\u6210\u8d1f\u6837\u672c\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u63a8\u8350\u8d28\u91cf\u3002"}}
{"id": "2508.14485", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.14485", "abs": "https://arxiv.org/abs/2508.14485", "authors": ["Moyu Zhang", "Yongxiang Tang", "Yujun Jin", "Jinxin Hu", "Yu Zhang"], "title": "Distribution-Guided Auto-Encoder for User Multimodal Interest Cross Fusion", "comment": "Accepted by CIKM 2025, 11 pages, 4 figures, 4 tables", "summary": "Traditional recommendation methods rely on correlating the embedding vectors\nof item IDs to capture implicit collaborative filtering signals to model the\nuser's interest in the target item. Consequently, traditional ID-based methods\noften encounter data sparsity problems stemming from the sparse nature of ID\nfeatures. To alleviate the problem of item ID sparsity, recommendation models\nincorporate multimodal item information to enhance recommendation accuracy.\nHowever, existing multimodal recommendation methods typically employ early\nfusion approaches, which focus primarily on combining text and image features,\nwhile neglecting the contextual influence of user behavior sequences. This\noversight prevents dynamic adaptation of multimodal interest representations\nbased on behavioral patterns, consequently restricting the model's capacity to\neffectively capture user multimodal interests. Therefore, this paper proposes\nthe Distribution-Guided Multimodal-Interest Auto-Encoder (DMAE), which achieves\nthe cross fusion of user multimodal interest at the behavioral\nlevel.Ultimately, extensive experiments demonstrate the superiority of DMAE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aDMAE\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u884c\u4e3a\u5c42\u9762\u7684\u591a\u6a21\u6001\u5174\u8da3\u878d\u5408\uff0c\u6709\u6548\u63d0\u9ad8\u63a8\u8350\u7cbe\u5ea6\u5e76\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9879\u76eeID\u7684\u5d4c\u5165\u5411\u91cf\u6765\u6355\u6349\u9690\u5f0f\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\u4ee5\u5efa\u6a21\u7528\u6237\u7684\u5174\u8da3\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5e38\u5e38\u9762\u4e34\u6570\u636e\u7a00\u758f\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u9879\u76eeID\u7a00\u758f\u6027\u95ee\u9898\u5e76\u63d0\u9ad8\u63a8\u8350\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aDMAE\uff08Distribution-Guided Multimodal-Interest Auto-Encoder\uff09\u7684\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u884c\u4e3a\u5c42\u9762\u5b9e\u73b0\u7528\u6237\u591a\u6a21\u6001\u5174\u8da3\u7684\u8de8\u878d\u5408\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u591a\u6a21\u6001\u63a8\u8350\u65b9\u6cd5\u5ffd\u7565\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u4e0a\u4e0b\u6587\u5f71\u54cd\u7684\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDMAE\u5728\u6355\u6349\u7528\u6237\u591a\u6a21\u6001\u5174\u8da3\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "DMAE\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6a21\u578b\u6355\u6349\u7528\u6237\u591a\u6a21\u6001\u5174\u8da3\u7684\u80fd\u529b\uff0c\u4ece\u800c\u6539\u5584\u4e86\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2508.14493", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.14493", "abs": "https://arxiv.org/abs/2508.14493", "authors": ["Moyu Zhang", "Yujun Jin", "Jinxin Hu", "Yu Zhang"], "title": "Global-Distribution Aware Scenario-Specific Variational Representation Learning Framework", "comment": "Accepted by CIKM 2025, 6 pages, 1 figures, 5 tables", "summary": "With the emergence of e-commerce, the recommendations provided by commercial\nplatforms must adapt to diverse scenarios to accommodate users' varying\nshopping preferences. Current methods typically use a unified framework to\noffer personalized recommendations for different scenarios. However, they often\nemploy shared bottom representations, which partially hinders the model's\ncapacity to capture scenario uniqueness. Ideally, users and items should\nexhibit specific characteristics in different scenarios, prompting the need to\nlearn scenario-specific representations to differentiate scenarios. Yet,\nvariations in user and item interactions across scenarios lead to data sparsity\nissues, impeding the acquisition of scenario-specific representations. To learn\nrobust scenario-specific representations, we introduce a Global-Distribution\nAware Scenario-Specific Variational Representation Learning Framework (GSVR)\nthat can be directly applied to existing multi-scenario methods. Specifically,\nconsidering the uncertainty stemming from limited samples, our approach employs\na probabilistic model to generate scenario-specific distributions for each user\nand item in each scenario, estimated through variational inference (VI).\nAdditionally, we introduce the global knowledge-aware multinomial distributions\nas prior knowledge to regulate the learning of the posterior user and item\ndistributions, ensuring similarities among distributions for users with akin\ninterests and items with similar side information. This mitigates the risk of\nusers or items with fewer records being overwhelmed in sparse scenarios.\nExtensive experimental results affirm the efficacy of GSVR in assisting\nexisting multi-scenario recommendation methods in learning more robust\nrepresentations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6GSVR\uff0c\u901a\u8fc7\u4f30\u8ba1\u573a\u666f\u7279\u5b9a\u6982\u7387\u5206\u5e03\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u63d0\u5347\u591a\u573a\u666f\u63a8\u8350\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u7535\u5b50\u5546\u52a1\u7684\u5174\u8d77\uff0c\u5546\u4e1a\u5e73\u53f0\u5fc5\u987b\u6839\u636e\u7528\u6237\u7684\u8d2d\u7269\u504f\u597d\u8c03\u6574\u63a8\u8350\u7cfb\u7edf\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u573a\u666f\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u7edf\u4e00\u6846\u67b6\u63d0\u4f9b\u4e2a\u6027\u5316\u63a8\u8350\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u573a\u666f\u7684\u72ec\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u7403\u5206\u5e03\u611f\u77e5\u7684\u573a\u666f\u7279\u5b9a\u53d8\u5206\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff08GSVR\uff09\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6709\u591a\u573a\u666f\u65b9\u6cd5\u3002\u5229\u7528\u6982\u7387\u6a21\u578b\u751f\u6210\u6bcf\u4e2a\u573a\u666f\u4e2d\u7528\u6237\u548c\u5546\u54c1\u7684\u573a\u666f\u7279\u5b9a\u5206\u5e03\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u4f30\u8ba1\u3002\u540c\u65f6\u5f15\u5165\u5168\u7403\u77e5\u8bc6\u611f\u77e5\u7684\u591a\u9879\u5f0f\u5206\u5e03\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u6765\u8c03\u8282\u540e\u9a8c\u7528\u6237\u548c\u5546\u54c1\u5206\u5e03\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGSVR\u80fd\u591f\u5e2e\u52a9\u73b0\u6709\u7684\u591a\u573a\u666f\u63a8\u8350\u65b9\u6cd5\u5b66\u4e60\u66f4\u4e3a\u9c81\u68d2\u7684\u8868\u793a\u3002", "conclusion": "GSVR\u6846\u67b6\u5728\u6355\u6349\u573a\u666f\u7279\u5f02\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u758f\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2508.14500", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.14500", "abs": "https://arxiv.org/abs/2508.14500", "authors": ["Moyu Zhang", "Yun Chen", "Yujun Jin", "Jinxin Hu", "Yu Zhang"], "title": "DGenCTR: Towards a Universal Generative Paradigm for Click-Through Rate Prediction via Discrete Diffusion", "comment": "11 pages, 4 figures, 4 tables", "summary": "Recent advances in generative models have inspired the field of recommender\nsystems to explore generative approaches, but most existing research focuses on\nsequence generation, a paradigm ill-suited for click-through rate (CTR)\nprediction. CTR models critically depend on a large number of cross-features\nbetween the target item and the user to estimate the probability of clicking on\nthe item, and discarding these cross-features will significantly impair model\nperformance. Therefore, to harness the ability of generative models to\nunderstand data distributions and thereby alleviate the constraints of\ntraditional discriminative models in label-scarce space, diverging from the\nitem-generation paradigm of sequence generation methods, we propose a novel\nsample-level generation paradigm specifically designed for the CTR task: a\ntwo-stage Discrete Diffusion-Based Generative CTR training framework (DGenCTR).\nThis two-stage framework comprises a diffusion-based generative pre-training\nstage and a CTR-targeted supervised fine-tuning stage for CTR. Finally,\nextensive offline experiments and online A/B testing conclusively validate the\neffectiveness of our framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u751f\u6210\u8303\u5f0fDGenCTR\u4ee5\u63d0\u9ad8CTR\u9884\u6d4b\u6027\u80fd\uff0c\u5305\u542b\u751f\u6210\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u6709\u80fd\u529b\u7406\u89e3\u6570\u636e\u5206\u5e03\u4ee5\u51cf\u8f7b\u4f20\u7edf\u5224\u522b\u5f0f\u6a21\u578b\u5728\u6807\u7b7e\u7a00\u7f3a\u7a7a\u95f4\u4e2d\u7684\u9650\u5236\uff0c\u4e0e\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\u7684\u9879\u76ee\u751f\u6210\u8303\u5f0f\u76f8\u6096\uff0c\u63a2\u7d22\u9002\u5408CTR\u9884\u6d4b\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u4e24\u4e2a\u9636\u6bb5\u7684\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u9884\u8bad\u7ec3\u9636\u6bb5\u548c\u9488\u5bf9CTR\u7684\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u3002", "result": "\u5e7f\u6cdb\u7684\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9488\u5bf9CTR\u4efb\u52a1\u7684\u751f\u6210\u8303\u5f0f\uff0c\u5373\u4e00\u4e2a\u4e24\u9636\u6bb5\u79bb\u6563\u6269\u6563\u751f\u6210CTR\u8bad\u7ec3\u6846\u67b6\uff08DGenCTR\uff09\u3002"}}
{"id": "2508.14515", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14515", "abs": "https://arxiv.org/abs/2508.14515", "authors": ["Chengcheng Guo", "Junda She", "Kuo Cai", "Shiyao Wang", "Qigen Hu", "Qiang Luo", "Kun Gai", "Guorui Zhou"], "title": "MISS: Multi-Modal Tree Indexing and Searching with Lifelong Sequential Behavior for Retrieval Recommendation", "comment": "CIKM 2025", "summary": "Large-scale industrial recommendation systems typically employ a two-stage\nparadigm of retrieval and ranking to handle huge amounts of information. Recent\nresearch focuses on improving the performance of retrieval model. A promising\nway is to introduce extensive information about users and items. On one hand,\nlifelong sequential behavior is valuable. Existing lifelong behavior modeling\nmethods in ranking stage focus on the interaction of lifelong behavior and\ncandidate items from retrieval stage. In retrieval stage, it is difficult to\nutilize lifelong behavior because of a large corpus of candidate items. On the\nother hand, existing retrieval methods mostly relay on interaction information,\npotentially disregarding valuable multi-modal information. To solve these\nproblems, we represent the pioneering exploration of leveraging multi-modal\ninformation and lifelong sequence model within the advanced tree-based\nretrieval model. We propose Multi-modal Indexing and Searching with lifelong\nSequence (MISS), which contains a multi-modal index tree and a multi-modal\nlifelong sequence modeling module. Specifically, for better index structure, we\npropose multi-modal index tree, which is built using the multi-modal embedding\nto precisely represent item similarity. To precisely capture diverse user\ninterests in user lifelong sequence, we propose collaborative general search\nunit (Co-GSU) and multi-modal general search unit (MM-GSU) for\nmulti-perspective interests searching.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51faMISS\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7d22\u5f15\u548c\u7ec8\u8eab\u5e8f\u5217\u5efa\u6a21\u6539\u8fdb\u63a8\u8350\u7cfb\u7edf\u53ec\u56de\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u53ec\u56de\u9636\u6bb5\u7684\u6027\u80fd\uff0c\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5e9e\u5927\u5019\u9009\u9879\u5e93\u65f6\u96be\u4ee5\u5229\u7528\u7528\u6237\u7ec8\u8eab\u884c\u4e3a\u548c\u591a\u6a21\u6001\u4fe1\u606f\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e86\u591a\u6a21\u6001\u7d22\u5f15\u6811\u548c\u591a\u6a21\u6001\u7ec8\u8eab\u5e8f\u5217\u5efa\u6a21\u6a21\u5757\uff0c\u5305\u62ec\u534f\u540c\u901a\u7528\u641c\u7d22\u5355\u5143\uff08Co-GSU\uff09\u548c\u591a\u6a21\u6001\u901a\u7528\u641c\u7d22\u5355\u5143\uff08MM-GSU\uff09\u3002", "result": "\u901a\u8fc7\u591a\u6a21\u6001\u5d4c\u5165\u6784\u5efa\u7684\u591a\u6a21\u6001\u7d22\u5f15\u6811\u548c\u7528\u6237\u7ec8\u8eab\u5e8f\u5217\u7684\u591a\u89c6\u89d2\u5174\u8da3\u641c\u7d22\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u53ec\u56de\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u548c\u7ec8\u8eab\u5e8f\u5217\u5efa\u6a21\uff0c\u63d0\u5347\u4e86\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u7684\u53ec\u56de\u9636\u6bb5\u6027\u80fd\u3002"}}
{"id": "2508.14646", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14646", "abs": "https://arxiv.org/abs/2508.14646", "authors": ["Zhipeng Wei", "Kuo Cai", "Junda She", "Jie Chen", "Minghao Chen", "Yang Zeng", "Qiang Luo", "Wencong Zeng", "Ruiming Tang", "Kun Gai", "Guorui Zhou"], "title": "OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service", "comment": null, "summary": "Local life service is a vital scenario in Kuaishou App, where video\nrecommendation is intrinsically linked with store's location information. Thus,\nrecommendation in our scenario is challenging because we should take into\naccount user's interest and real-time location at the same time. In the face of\nsuch complex scenarios, end-to-end generative recommendation has emerged as a\nnew paradigm, such as OneRec in the short video scenario, OneSug in the search\nscenario, and EGA in the advertising scenario. However, in local life service,\nan end-to-end generative recommendation model has not yet been developed as\nthere are some key challenges to be solved. The first challenge is how to make\nfull use of geographic information. The second challenge is how to balance\nmultiple objectives, including user interests, the distance between user and\nstores, and some other business objectives. To address the challenges, we\npropose OneLoc. Specifically, we leverage geographic information from different\nperspectives: (1) geo-aware semantic ID incorporates both video and geographic\ninformation for tokenization, (2) geo-aware self-attention in the encoder\nleverages both video location similarity and user's real-time location, and (3)\nneighbor-aware prompt captures rich context information surrounding users for\ngeneration. To balance multiple objectives, we use reinforcement learning and\npropose two reward functions, i.e., geographic reward and GMV reward. With the\nabove design, OneLoc achieves outstanding offline and online performance. In\nfact, OneLoc has been deployed in local life service of Kuaishou App. It serves\n400 million active users daily, achieving 21.016% and 17.891% improvements in\nterms of gross merchandise value (GMV) and orders numbers.", "AI": {"tldr": "OneLoc\u901a\u8fc7\u5229\u7528\u5730\u7406\u4fe1\u606f\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u5feb\u624bApp\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u8350\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u7ed3\u5408\u7528\u6237\u5174\u8da3\u4e0e\u5b9e\u65f6\u5730\u7406\u4f4d\u7f6e\u8fdb\u884c\u63a8\u8350\u7684\u6311\u6218\uff0c\u540c\u65f6\u5904\u7406\u591a\u4e2a\u76ee\u6807\u3002", "method": "\u4f7f\u7528\u5730\u7406\u4fe1\u606f\u7684\u5404\u79cd\u65b9\u6cd5\uff0c\u5305\u62ecgeo-aware\u8bed\u4e49ID\u3001geo-aware\u81ea\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548cneighbor-aware\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5e73\u8861\u7528\u6237\u5174\u8da3\u548c\u5730\u7406\u5956\u52b1\u3002", "result": "OneLoc\u6a21\u578b\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86GMV\u63d0\u534721.016%\u548c\u8ba2\u5355\u6570\u91cf\u63d0\u534717.891%\u7684\u6548\u679c\u3002", "conclusion": "OneLoc\u6a21\u578b\u5728\u5feb\u624bApp\u7684\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86GMV\u548c\u8ba2\u5355\u6570\u91cf\u3002"}}
{"id": "2508.14786", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.14786", "abs": "https://arxiv.org/abs/2508.14786", "authors": ["Veronika Ivanova", "Evgeny Frolov", "Alexey Vasilev"], "title": "Benefiting from Negative yet Informative Feedback by Contrasting Opposing Sequential Patterns", "comment": null, "summary": "We consider the task of learning from both positive and negative feedback in\na sequential recommendation scenario, as both types of feedback are often\npresent in user interactions. Meanwhile, conventional sequential learning\nmodels usually focus on considering and predicting positive interactions,\nignoring that reducing items with negative feedback in recommendations improves\nuser satisfaction with the service. Moreover, the negative feedback can\npotentially provide a useful signal for more accurate identification of true\nuser interests. In this work, we propose to train two transformer encoders on\nseparate positive and negative interaction sequences. We incorporate both types\nof feedback into the training objective of the sequential recommender using a\ncomposite loss function that includes positive and negative cross-entropy as\nwell as a cleverly crafted contrastive term, that helps better modeling\nopposing patterns. We demonstrate the effectiveness of this approach in terms\nof increasing true-positive metrics compared to state-of-the-art sequential\nrecommendation methods while reducing the number of wrongly promoted negative\nitems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5e8f\u5217\u63a8\u8350\u573a\u666f\u4e2d\u540c\u65f6\u4ece\u6b63\u8d1f\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e24\u4e2a\u5206\u522b\u5904\u7406\u6b63\u8d1f\u4ea4\u4e92\u7684Transformer\u7f16\u7801\u5668\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u901a\u5e38\u53ea\u5173\u6ce8\u6b63\u53cd\u9988\uff0c\u800c\u5ffd\u7565\u4e86\u8d1f\u53cd\u9988\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u8d1f\u53cd\u9988\u53ef\u4ee5\u4e3a\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u7528\u6237\u771f\u5b9e\u5174\u8da3\u63d0\u4f9b\u6709\u7528\u4fe1\u53f7\uff0c\u5e76\u4e14\u51cf\u5c11\u8d1f\u53cd\u9988\u9879\u53ef\u4ee5\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u6b63\u8d1f\u53cd\u9988\u7684\u4fe1\u606f\u6765\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4f7f\u7528\u4e24\u4e2aTransformer\u7f16\u7801\u5668\u5206\u522b\u5904\u7406\u6b63\u8d1f\u4ea4\u4e92\u5e8f\u5217\uff0c\u5e76\u5f15\u5165\u7531\u6b63\u8d1f\u4ea4\u53c9\u71b5\u548c\u5bf9\u6bd4\u9879\u7ec4\u6210\u7684\u7efc\u5408\u635f\u5931\u51fd\u6570\u4f5c\u4e3a\u5e8f\u5217\u63a8\u8350\u5668\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u4ee5\u6b64\u66f4\u597d\u5730\u5efa\u6a21\u6b63\u8d1f\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u76f8\u6bd4\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5e8f\u5217\u63a8\u8350\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u6b63\u4f8b\u6307\u6807\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u9519\u8bef\u5730\u63a8\u9001\u8d1f\u53cd\u9988\u9879\u7684\u6570\u91cf\u3002", "conclusion": "\u672c\u6587\u7684\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u6b63\u8d1f\u53cd\u9988\uff0c\u901a\u8fc7\u7efc\u5408\u635f\u5931\u51fd\u6570\u548c\u5bf9\u6bd4\u9879\u8bad\u7ec3\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u7528\u6237\u5174\u8da3\uff0c\u5e76\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002"}}
